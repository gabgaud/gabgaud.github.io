import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 13

# Haute-Disponibilit√©

* * *

## Qu'est-ce que la haute-disponibilit√© ?

La haute-disponibilit√© est la capacit√© d'un syst√®me informatique √† rester op√©rationnel et accessible presque 100% du temps, visant √† r√©duire ou √©liminer les temps d'arr√™t. Dans le contexte de Proxmox, on cherche √† √©viter l'arr√™t des appareils virtuels.

Lors du dernier laboratoire, vous avez d√©j√† exp√©riment√© quelques param√®tres en lien avec la haute-disponibilit√© dans Proxmox. Aujourd'hui, nous nous attarderons √† certains √©l√©ments que nous n'avons toujours pas abord√©s.

## Les groupes HA üíì

En production, dans une entreprise, les noeuds Proxmox peuvent se multiplier rapidement. Certains de ces noeuds peuvent √™tre plus puissants, poss√©der plus ou moins de m√©moire vive ou poss√©der des caract√©ristiques pr√©cises qui les distinguent des autres. Dans un contexte de haute-disponibilit√©, o√π les machines virtuelles peuvent √™tre poss√©der diff√©rentes services et √™tre relocalis√©es rapidement, nous pourrions vouloir faire en sorte que certaines *VMs* demeurent sur des noeuds pr√©cis. C'est exactement √† ce besoin que viennent r√©pondre les groupes.

Dans le sch√©ma ci-dessous, √† titre d'exemple, trois groupes de haute-disponibilit√© ont √©t√© cr√©√©s:

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/GroupeHA_W.svg'),
            dark: useBaseUrl('/img/Virtu/GroupeHA_D.svg'),
        }}
    />
</div>

Les machines virtuelles et conteneurs peuvent √™tre attitr√©es √† ces groupes. Si une machine virtuelle s'ex√©cutant sur le **noeud C** doit √™tre relocalis√©, elle le sera sur le noeud A ou B seulement.

### Cr√©er des groupes

Pour cr√©er des groupes de haute-disponibilit√© dans Proxmox, il vous suffit de cliquer sur `Datacenter`, puis de cliquer sur `Groups` dans la section `HA`.

![GroupHA](../Assets/13/GroupHA.png)

## Fencing üöß

L'expression *fencing* r√©f√®re au mot cl√¥ture en anglais (*fence*). On pourrait donc traduire le terme *fencing* par le verbe cl√¥turer. √áa consiste √† quoi le *fencing* exactement ? Lorsqu'un noeud perd le quorum pendant 60 secondes ou plus, un *watchdog* (logiciel de surveillance) force le red√©marrage du noeud. Lorsqu'un noeud faisant parti d'un cluster red√©marre, la premi√®re chose qu'il tente d'effectuer, c'est de v√©rifier s'il a le quorum. Dans le cas o√π il ne l'aurait pas, aucune *VM* ou conteneur identifi√© **HA** ne sera d√©marr√©.

### Pourquoi c'est ainsi ?

On cherche √† tout prix d'√©viter de cr√©er un *split-brain*, c'est-√†-dire, des noeuds qui ne communiquent plus entre-eux et qui √©crivent tous les deux sur une seule machine virtuelle. Cela pourrait causer une corruption importante au niveau des donn√©es. On verra cependant, qu'on peut simplifier la chose, particuli√®rement avec du stockage partag√©.

>*Donc il n'y a pas vraiment de configuration √† mettre en place pour le fencing Gabriel ?*
>
>*-Les √©tudiants*

Non, pas vraiment. D√®s que vous cr√©ez un cluster Proxmox, le *fencing* logiciel s'enclanche automatiquement. Par contre, c'est ce que l'on nomme du *soft-fencing*.

### Soft ü™∂ & Hard üî® fencing

Nous pourrions cependant configurer ce que l'on nomme du **Hard-Fencing**. Cette technologie fonctionne avec le mat√©riel des serveurs et n√©cessite donc certains types de mat√©riel tr√®s pr√©cis. Le **Hard-Fencing** utilisera l'iLO, l'iDRAC ou l'interface de gestion √† distance du serveur sp√©cifique pour litt√©ralement l'arr√™ter. Ainsi, le serveur ne pourra pas corrompre les donn√©es des machines virtuelles ou des conteneurs pr√©sents dans son environnement.

## Migration 

√âvidemment, il ne faut pas obligatoirement une panne pour migrer une machine virtuelle ou un conteneur vers un autre noeud. On pourrait, pas exemple, vouloir transf√©rer les machines virtuelles et les conteneurs vers un autre noeud en vue d'effectuer une maintenance sur un noeud particulier. Proxmox offre diff√©rents types de migration auxquels nous allons nous attarder un peu.

### Hors ligne (*Offline*)

En mode *offline*, la machine virtuelle ou le conteneur est d'abord arr√™t√©. Si la machine virtuelle ou le conteneur poss√®de un stockage local, on devra transf√©rer ensuite le disque virtuel vers un autre noeud, puis on pourra red√©marrer la machine. Cette m√©thode accuse un temps d'arr√™t important. Or, si la machine virtuelle est stock√© sur un √©l√©ment de stockage partag√©, la dynamique change compl√®tement puisqu'on oubliera alors le transfert de donn√©es. Le temps d'arr√™t sera raccourci de beaucoup.

### En ligne (*online*)

En mode *online*, on d√©coupe le travail en trois phases:
- **PHASE 1:** Copie du disque dur pendant que la machine virtuelle tourne toujours.
- **PHASE 2:** Pendant la copie initiale, la machine virtuelle a subit des changements par les utilisateurs. On proc√®de donc a une copie incr√©mentale.
- **PHASE 3:** Bascule finale. Une pause de quelques secondes sera n√©cessaire pour officialiser les changements et d√©marrer la machine virtuelle sur son nouveau noeud.

Dans tous les cas, vous pouvez proc√©der √† la pi√®ce, ou migrer plusieurs machines virtuelles d'un seul coup (*bulk migrate*)

### √Ä la pi√®ce üß©

Pour effectuer la migration d'une seule machine virtuelle, il vous suffit de s√©lectionner la machine virtuelle en question puis de cliquer sur `Migrate`, en haut √† droite.

![migrate100](../Assets/13/Migrate100.png)

Comme ma machine virtuelle est en marche, Proxmox me propose une migration *online* d'embl√©e.<br/> Remarquez les avertissement ‚ö†Ô∏è que Proxmox m'√©num√®re.

- Migration with local disk might take long: VMDisks:vm-100-disk-0 (1.00 MiB)
- Migration with local disk might take long: VMDisks:vm-100-disk-1 (40 GiB)
- Migration with local disk might take long: VMDisks:vm-100-state-PremierSnap

Ces avertissements l√† sont tr√®s vrais. Normalement, avec des stockages locaux, une migration comme celle-ci pourrait √™tre longue. Cependant, dans notre cas (je r√©f√®re aux laboratoires ici), on a mis en place une t√¢che de r√©plication r√©currente gr√¢ce √† notre utilisation de **ZFS**. Proxmox va donc se comporter l√©g√®rement diff√©remment ici. En effet, Proxmox d√©tectera qu'une r√©plication existe sur le noeud de destination et ne copiera que les changements r√©cents (moins de 15 minutes dans notre cas).

### En vrac üß∫

Pour √©vacuer tous les appareils virtuels d'un noeud, mieux vaut utiliser la migration en vrac (*bulk migrate*) Pour ce faire, s√©lectionnez le noeud qui doit √™tre √©vacu√©, s√©lectionnez `Bulk Actions`, puis `Bulk Migrate`.

![BulkMigrate](../Assets/13/BulkMigrate.png)

De l√†, vous serez amen√© √† choisir le noeud de destination ainsi que d'autres param√®tres.

## Le stockage partag√© : La cl√© du cluster üóùÔ∏è

Le stockage partag√© est le meilleur moyen d'assurer de la haute-disponibilit√© dans un environnement comme Proxmox. Mais pourquoi ?

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/StockageLocal_W.svg'),
            dark: useBaseUrl('/img/Virtu/StockageLocal_D.svg'),
        }}
    />
</div>

Dans le sch√©ma ci-dessus üëÜ, que se passerait-il dans le cas o√π le noeud A venait qu'√† s'√©teindre parce qu'on son bloc d'alimentation est d√©fecteux ? Comme la *VM* 100 est stock√© sur ce noeud qui est maintenant inutilisable, il nous serait impossible de r√©cup√©rer la *VM* 100.

>*Wow, un instant Gabriel, le ZFS n'assurait-il pas la r√©plication ?*
>
>*-Les √©tudiants*

Bonne remarque! Mais ZFS demeure facultatif √† la mise en *cluster*. Donc quelqu'un pourrait tr√®s bien cr√©er un *cluster* sans ZFS. Est-ce que ce serait une bonne chose ? D√©finitivement pas, je ne le recommanderais pas. N√©anmoins, comme pour beaucoup d'autres mauvaises pratiques, on en croise sur le terrain.

Vous l'aurez donc devinez, ZFS peut palier √† cette situation, mais ce n'est pas encore parfait et je vous explique pourquoi. Outre le fait que ZFS consomme **√â-NOR-M√â-MENT** de ressources, ses t√¢ches de r√©plication se font √† des p√©riodes d√©termin√©es. D'ailleurs ces p√©riodes devront √™tre d√©termin√©es en fonction de plusieurs autres param√®tres comme la vites du r√©seau, la grosseur du disque √† r√©pliquer, etc.

Cela veut donc dire que m√™me si vous optez pour le syst√®me ZFS et sa r√©plication, vous pourriez vous retrouver avec une certaine perte de donn√©es telle que d√©crit dans la mise en situation ci-dessus. C'est pourquoi, **<span class="green-text">le stockage partag√© reste la meilleure solution.</span>**

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/StockagePartage_W.svg'),
            dark: useBaseUrl('/img/Virtu/StockagePartage_D.svg'),
        }}
    />
</div>

Dans le sch√©ma ci-dessus, les noeuds utilisent un stockage partag√©. Advenant un incident majeur, comme la perte d'un noeud, il n'y aurait aucun transfert √† effectuer, un autre noeud du *cluster* n'aura qu'√† d√©marrer la machine virtuelle depuis le m√™me stockage qu'utilisait le noeud tomb√© en panne.

### Les principaux types de stockage partag√©

#### NFS (Network File System)

**NFS** est g√©n√©ralement utilis√© sur des serveurs de type NAS. Le stockage est alors accessible directement via le r√©seau et facilement configurable dans Proxmxox.

**Avantages:**<br/>
‚úì Simple √† configurer<br/>
‚úì Compatible<br/>
‚úì Bon pour des *VMs* peu sollicit√©es

**Inconv√©nients:**<br/>
‚úó Point unique de d√©faillance (le NAS)<br/>
‚úó Performance limit√©e par le r√©seau<br/>
‚úó Pas de redondance native

#### CEPH RBD (RADOS Block Device)

CEPH utilise une m√©thode de distribution de block de stockage entre tous les noeuds. On pourrait comparer cela au *striping* d'un RAID, mais ici, on √©talonne sur les serveurs plut√¥t que sur une grappe de disques.

**Avantages :**<br/>
‚úì Haute disponibilit√© native<br/>
‚úì Pas de point unique de d√©faillance<br/>
‚úì Scalable (ajout de disques facile)<br/>
‚úì Snapshots et clones

**Inconv√©nients :**<br/>
‚úó Complexe √† configurer<br/>
‚úó N√©cessite au moins 3 n≈ìuds<br/>
‚úó Consomme beaucoup de ressources<br/>
‚úó R√©seau 10Gbps obligatoire

#### iSCSI

iSCSI permet d'utiliser du stockage sur le r√©seau (SAN) comme s'il √©tait local.

**Avantages :**<br/>
‚úì Performance excellente<br/>
‚úì Mat√©riel d√©di√© robuste<br/>
‚úì Snapshots et r√©plication

**Inconv√©nients :**<br/>
‚úó Tr√®s couteux (mat√©riel)<br/>
‚úó Configuration complexe<br/>
‚úó Point unique de d√©faillance (sauf SAN HA)<br/>

#### R√©sum√©

|**Crit√®re**|**NFS**|**Ceph RBD**|**iSCSI**|
|-----------------|--------|---------------|-----------|
|**Complexit√©**|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s simple| ‚≠ê Tr√®s complexe|‚≠ê‚≠ê‚≠ê Moyenne|
|**Performance**|‚≠ê‚≠ê Moyenne|‚≠ê‚≠ê‚≠ê‚≠ê √âlev√©e|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente|
|**Haute disponibilit√© native**| ‚ùå Non | ‚úÖ Oui | ‚ùå Non*|
|**Noeuds minimum**|1 (NAS externe)|3|1 (SAN externe)|
|**R√©seau requis**|1 Gbps min. 10 Gbps recommand√©|10 Gbps recommand√©|10 Gbps recommand√©|
|**RAM requise**|Minimal|1GB/TB|Minimal|
|**Snapshots**|‚úÖ Oui|‚úÖ Oui|‚ö†Ô∏è D√©pend du SAN|
|**Thin Provisionning**| ‚ö†Ô∏è Via qcow2 | ‚úÖ Oui | ‚ö†Ô∏è D√©pend du SAN|
|**D√©duplication**|‚ùå Non|‚ö†Ô∏è Optionnelle| ‚ö†Ô∏è D√©pend du SAN|
|**Compression**|‚ùå Non|‚úÖ Oui|‚ö†Ô∏è D√©pend du SAN|
|**Co√ªt**|Faible|Faible|Tr√®s √©lev√©|
|**Scalabilit√©**|‚≠ê‚≠ê Limit√©e|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente|‚≠ê‚≠ê‚≠ê Bonne|
|***Use case* id√©al**|PME, Home Lab|Production, Cloud|Entreprise|

