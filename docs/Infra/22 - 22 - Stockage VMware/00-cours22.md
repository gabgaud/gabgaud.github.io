import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 22

# Gestion des stockages sous VMware

* * *

Dans ce cours, nous allons explorer les diff√©rentes solutions de stockage disponibles dans l'environnement VMware ESXi et vCenter. Comme vous avez d√©j√† travaill√© avec Proxmox, vous connaissez les concepts de base de la virtualisation et du stockage. VMware utilise une approche similaire mais avec sa propre terminologie et ses propres technologies.


## Les datastores : concept fondamental üß±

Dans l'√©cosyst√®me VMware, un **datastore** est l'√©quivalent d'un pool de stockage dans Proxmox. C'est un conteneur logique qui abstrait les d√©tails physiques du stockage et fournit un emplacement unifi√© pour stocker les fichiers des machines virtuelles.

Un **datastore** ne correspond pas n√©cessairement √† un disque ni √† une unit√© de stockage particuli√®re, c'est plut√¥t un assemblage de stockages.

### Types de fichiers dans un datastore üìÅ

Lorsque vous cr√©ez une VM dans ESXi, plusieurs fichiers sont automatiquement cr√©√©s dans le datastore. 

Voici les principaux :

    - **Fichiers .vmdk (Virtual Machine Disk)** : <br/>Ce sont vos disques durs virtuels. En r√©alit√©, un disque virtuel est compos√© de deux fichiers :
        - `MaVM.vmdk` : un petit fichier texte descripteur (quelques Ko)
        - `MaVM-flat.vmdk` : le fichier contenant r√©ellement les donn√©es (la taille de votre disque)

    - **Fichiers .vmx (Virtual Machine Configuration)** : <br/>C'est le fichier de configuration de votre VM.
    Il contient toutes les informations : combien de RAM, combien de CPU, quels r√©seaux, etc. C'est un fichier texte que vous pouvez m√™me √©diter manuellement si n√©cessaire (‚ö†Ô∏è mais attention ‚ö†Ô∏è).

    - **Fichiers .nvram** : Stocke les param√®tres du BIOS/UEFI de la VM, comme l'ordre de d√©marrage ou les param√®tres de s√©curit√©.

    - **Fichiers .vswp (Virtual Swap)** : Fichier de m√©moire swap pour la VM. Sa taille est √©gale √† la RAM allou√©e moins la r√©servation de m√©moire.

    - **Fichiers de snapshot** : Quand vous cr√©ez un snapshot, ESXi g√©n√®re des fichiers `-delta.vmdk` et `.vmsd` pour conserver l'√©tat du disque √† un moment pr√©cis.

## Les types de stockage support√©s üíæ

VMware ESXi supporte plusieurs types de stockage, chacun adapt√© √† des besoins et des budgets diff√©rents. Explorons-les en d√©tail.

### Stockage local (Local Storage)

Le stockage local, c'est le plus simple : vous utilisez les disques physiques directement install√©s dans votre serveur ESXi, exactement comme vous le faisiez avec Proxmox.

#### Avantages et inconv√©nients

**<span class='green-text'>Avantages:</span>**
- **Performance maximale** : pas de latence r√©seau, acc√®s direct au disque
- **Configuration simple** : aucune infrastructure r√©seau suppl√©mentaire n√©cessaire
- **Co√ªt minimal** : vous n'achetez que des disques

**<span class='red-text'>Inconv√©nients:</span>**
- **Pas de haute disponibilit√©** : si votre serveur tombe, vos VMs sont inaccessibles
- **Pas de vMotion de stockage** : vous ne pouvez pas d√©placer les VMs vers un autre h√¥te sans arr√™t
- **Capacit√© limit√©e** : vous √™tes limit√© par les baies de disques de votre serveur

#### Quand utiliser le stockage local ?

Le stockage local est parfait pour :
- Les environnements de laboratoire et de test
- Les serveurs autonomes sans besoin de haute disponibilit√©
- Les applications n√©cessitant les meilleures performances possibles
- Les petites infrastructures avec un budget limit√©

#### Proc√©dure : Cr√©er un datastore VMFS local

Mettons en pratique la cr√©ation d'un datastore local. Imaginez que vous venez d'installer un nouveau disque SSD de 500 Go dans votre serveur ESXi.

**√âtape 1 : Connexion et navigation**

Connectez-vous √† votre h√¥te ESXi via le client web (https://esxi.mondomaine.lan). Une fois connect√© :
- Dans le menu de gauche, cliquez sur "Storage" (Stockage)
- Vous verrez les datastores d√©j√† existants (probablement un datastore par d√©faut cr√©√© lors de l'installation)

![DefaultDataStore](../Assets/22/DefaultDataStore.png)

**√âtape 2 : V√©rifier que le disque est d√©tect√©**

Avant de cr√©er le datastore, v√©rifions que ESXi voit bien votre nouveau disque :
- Cliquez sur l'onglet "Devices" (P√©riph√©riques)
- Cherchez votre nouveau disque dans la liste (identifiez-le par sa capacit√© et son mod√®le)

![NouveauDisque](../Assets/22/NouveauPeripherique.png)

Si votre disque n'appara√Æt pas, il peut y avoir un probl√®me de d√©tection mat√©rielle. Red√©marrez l'h√¥te ou v√©rifiez les connexions physiques.

**√âtape 3 : Lancer la cr√©ation du datastore**

- Cliquez sur `Stockage` dans le menu de navigation
- Cliquez sur le bouton `Nouvelle banque de donn√©es`
- Une fen√™tre de cr√©ation s'ouvre

![NewDataStore](../Assets/22/NewDataStore.png)

**√âtape 4 : Choisir le type**

- S√©lectionnez `Cr√©er une banque de donn√©es VMFS` (VMFS est le syst√®me de fichiers propri√©taire de VMware, optimis√© pour les VMs)
- Cliquez sur `Suivant`

![DataStoreType](../Assets/22/DataStore_Type.png)

**√âtape 5 : Nommer le datastore et s√©lectionner le disque**

Le nom est important pour l'organisation, surtout si vous avez plusieurs datastores. Adoptez une convention de nommage logique :
- Exemples : `Local-SSD-500GB`, `DS-Local-ESXi01`, `Datastore-Production`
- √âvitez les espaces et les caract√®res sp√©ciaux
- Entrez votre nom et cliquez sur `Suivant`

**√âtape 6 : S√©lectionner le p√©riph√©rique**

- Une liste de disques disponibles s'affiche
- S√©lectionnez votre disque
- ESXi affiche les d√©tails : capacit√©, type (SSD/HDD), mod√®le
- ‚ö†Ô∏è **Attention** ‚ö†Ô∏è : cette op√©ration effacera toutes les donn√©es du disque !
- Cliquez sur `Suivant`

![NewVMFS](../Assets/22/NewVMFS.png)

**√âtape 7 : Choisir le partitionnement**

Vous avez deux options principales :

**Option 1 - Use full disk (recommand√©)** : 
- Utilise 100% de l'espace disponible
- C'est g√©n√©ralement ce que vous voulez

**Option 2 - Custom** :
- Permet de ne partitionner qu'une partie du disque
- Utile si vous voulez r√©server de l'espace pour un usage futur

Pour notre exemple, choisissez "Use full disk" et laissez VMFS version 6 (la plus r√©cente). Cliquez sur `Suivant`.

![DataStore_VMFS](../Assets/22/DataStore_VMFS.png)

**√âtape 8 : R√©vision et cr√©ation**

- V√©rifiez toutes les informations affich√©es
- Capacit√© totale, capacit√© utilisable (l√©g√®rement inf√©rieure √† cause du formatage)
- Cliquez sur `Terminer`

ESXi va maintenant :
1. Cr√©er une partition GPT sur le disque
2. Formater la partition en VMFS6
3. Monter le datastore

Le processus prend g√©n√©ralement 10 √† 30 secondes.

![DataStore_Finish](../Assets/22/DataStore_Finish.png)

**√âtape 9 : V√©rification**

Une fois termin√© :
- Votre nouveau datastore appara√Æt dans la liste
- Cliquez dessus pour voir les d√©tails
- Vous verrez l'espace total, l'espace utilis√© (presque rien pour l'instant) et l'espace libre
- Vous pouvez maintenant cr√©er des VMs sur ce datastore !

![DataStore_Done](../Assets/22/DataStore_Done.png)

### Stockage en r√©seau (Network Storage) üåê

Le stockage en r√©seau change compl√®tement la donne. Au lieu que chaque serveur ESXi ait son propre stockage isol√©, tous les serveurs peuvent acc√©der √† un stockage partag√© via le r√©seau.

#### Pourquoi du stockage en r√©seau ?

Avec du stockage partag√©, vous d√©bloquez des fonctionnalit√©s essentielles :
- **vMotion** : d√©placer des VMs allum√©es d'un serveur √† un autre sans interruption
- **Haute disponibilit√© (HA)** : si un serveur tombe, ses VMs red√©marrent automatiquement sur un autre serveur
- **Centralisation** : g√©rer le stockage depuis un point central
- **Flexibilit√©** : ajouter de l'espace facilement sans ouvrir les serveurs

#### NFS (Network File System)

NFS est probablement le protocole le plus simple √† configurer pour du stockage partag√©. Vous avez d√©j√† utilis√© NFS avec Proxmox, donc les concepts vous sont familiers.

**Comprendre NFS : l'analogie du partage Windows**

Imaginez NFS comme un partage r√©seau Windows, mais optimis√© pour Linux/Unix. Un serveur partage un r√©pertoire, et les clients (vos h√¥tes ESXi) montent ce r√©pertoire comme s'il √©tait local. La diff√©rence principale avec un partage Windows ? Les performances et la fiabilit√© sont con√ßues pour des charges de travail intensives.

**Caract√©ristiques de NFS**

- **Protocole de niveau fichier** : vous partagez un r√©pertoire, pas un disque brut
- **Versions support√©es** : NFS v3 (simple, fiable) et NFS v4.1 (plus s√©curis√© avec Kerberos)
- **Thin provisioning natif** : les disques virtuels n'occupent que l'espace r√©ellement utilis√©
- **Configuration rapide** : souvent op√©rationnel en moins de 5 minutes

#### iSCSI (Internet Small Computer System Interface)

iSCSI est fondamentalement diff√©rent de NFS. Au lieu de partager un r√©pertoire, vous partagez un disque complet (appel√© LUN - Logical Unit Number) que ESXi voit comme un disque local. C'est comme brancher un disque dur via le r√©seau Ethernet au lieu d'un c√¢ble SATA.

**Comprendre iSCSI : l'analogie du disque dur externe**

Imaginez un disque dur externe USB, mais au lieu d'√™tre connect√© par USB, il est connect√© via votre r√©seau Ethernet. ESXi ne voit pas un partage r√©seau, il voit un disque brut qu'il peut formater en VMFS, exactement comme un disque local.

**Caract√©ristiques d'iSCSI**

- **Protocole de niveau bloc** : vous manipulez des blocs de donn√©es bruts, pas des fichiers
- **Utilise le r√©seau Ethernet** : pas besoin d'infrastructure Fibre Channel co√ªteuse
- **Format VMFS** : vous formatez le LUN en VMFS, comme un disque local
- **Performance √©lev√©e** : peut rivaliser avec Fibre Channel sur du 10 Gbps

**Terminologie iSCSI importante**

Avant de continuer, comprenons le vocabulaire iSCSI :

**Target (Cible)** :
- C'est le serveur qui partage le stockage
- Un target a un nom unique appel√© IQN (iSCSI Qualified Name)
- Exemple : `iqn.2024-11.com.monentreprise:storage01`

**Initiator (Initiateur)** :
- C'est le client qui se connecte au stockage
- Votre h√¥te ESXi est un initiateur
- ESXi a aussi un IQN unique

**LUN (Logical Unit Number)** :
- C'est le "disque" partag√© par le target
- Un target peut partager plusieurs LUNs
- Chaque LUN a un num√©ro (LUN 0, LUN 1, etc.)

**Portail** :
- C'est l'adresse IP et le port du target
- Format : `192.168.1.100:3260` (3260 est le port par d√©faut)

**Software iSCSI vs Hardware iSCSI**

ESXi supporte deux types d'iSCSI :

**Software iSCSI (logiciel)** :
- L'initiateur iSCSI est g√©r√© par ESXi en logiciel
- Utilise les cartes r√©seau standards
- Consomme un peu de CPU
- Gratuit, inclus dans ESXi (Oh la la!)
- Parfait pour commencer et pour la plupart des cas

**Hardware iSCSI (mat√©riel)** :
- N√©cessite une carte d√©di√©e
- D√©charge le travail du CPU
- Performance l√©g√®rement meilleure
- Co√ªt suppl√©mentaire (carte)
- Utile pour des charges tr√®s intensives

#### Fibre Channel (FC)

Fibre Channel est la solution de stockage haut de gamme, traditionnellement utilis√©e dans les grands datacenters et les environnements critiques. Contrairement √† iSCSI qui utilise Ethernet, Fibre Channel poss√®de sa propre infrastructure r√©seau d√©di√©e.

:::note
√âvidemment, nous ne pourrons pas mettre en place et tester fibre channel au c√©gep puisque nous n'avons pas les √©quipements n√©cessaires. Cela dit, rien ne nous emp√™che d'en comprendre le fonctionnement.
:::

**Comprendre Fibre Channel**

Imaginez Fibre Channel comme une autoroute priv√©e, ultra-rapide, construite exclusivement pour le trafic de stockage. Alors qu'iSCSI partage le r√©seau Ethernet avec d'autres trafics, Fibre Channel a son propre r√©seau isol√©, ce qui garantit des performances constantes et pr√©visibles.

**Caract√©ristiques de Fibre Channel**

- **Infrastructure d√©di√©e** : r√©seau compl√®tement s√©par√© avec ses propres switches (appel√©s "fabric")
- **Vitesse √©lev√©e** : 8, 16, 32 ou 64 Gbps par lien
- **Latence ultra-faible** : g√©n√©ralement inf√©rieure √† 1 ms
- **Fiabilit√© maximale** : protocole con√ßu pour ne jamais perdre de donn√©es
- **Co√ªt √©lev√©** : √©quipement sp√©cialis√© co√ªteux

**Quand utiliser Fibre Channel ?**

Fibre Channel est appropri√© pour :
- Les bases de donn√©es critiques n√©cessitant les meilleures performances
- Les environnements o√π la latence doit √™tre minimale et pr√©visible
- Les applications n√©cessitant des milliers d'IOPS
- Les organisations ayant d√©j√† investi dans une infrastructure FC

**Composants n√©cessaires**

Pour d√©ployer Fibre Channel, vous avez besoin de :

**1. Cartes HBA (Host Bus Adapter) FC** :
- Install√©es dans chaque h√¥te ESXi
- G√©n√©ralement 2 HBA par h√¥te (redondance)
- Chaque HBA a un WWN (World Wide Name), identifiant unique comme une adresse MAC

![HBA-FC](../Assets/22/hba-fc.jpg)

**2. Switches Fibre Channel** :
- Switches sp√©cialis√©s FC (pas des switches Ethernet)
- G√©n√©ralement 2 switches (fabric A et fabric B) pour la redondance
- Configuration complexe : zoning, LUN masking

![Switch-FC](../Assets/22/switch-fc.jpg)

**3. Baie de stockage avec ports FC** :
- SAN avec contr√¥leurs FC

#### Multipathing üîÄ

Avec Fibre Channel (et aussi avec iSCSI), vous avez g√©n√©ralement **plusieurs chemins** vers le m√™me stockage.

**Pourquoi ?**

1. **Haute disponibilit√©** : si un chemin tombe, le trafic bascule automatiquement sur un autre chemin
2. **√âquilibrage de charge** : r√©partir le trafic sur plusieurs chemins pour de meilleures performances
3. **Maintenance sans interruption** : vous pouvez mettre un switch hors service sans impact

**Types de politiques de chemin (Path Selection Policy)**

ESXi supporte plusieurs politiques pour g√©rer les chemins multiples :

**Fixed** :
- Utilise toujours le m√™me chemin (le "preferred path")
- Bascule sur un autre seulement si le chemin principal tombe
- Simple mais n'utilise pas tous les chemins

**Most Recently Used** :
- Utilise le chemin le plus r√©cemment actif
- Bascule en cas de panne mais ne revient pas automatiquement
- Utilis√© pour certains arrays Active/Passive

**Round Robin** :
- **Recommand√©** pour la plupart des SAN modernes
- Distribue les I/O sur tous les chemins disponibles
- Meilleure utilisation des ressources
- Meilleures performances

#### vSAN (Virtual SAN)

vSAN est l'approche moderne du stockage chez VMware. Au lieu d'utiliser un SAN externe co√ªteux, vSAN agr√®ge le stockage local de chaque h√¥te ESXi pour cr√©er un pool de stockage partag√© et distribu√©. C'est une approche *software-defined storage*.

**Caract√©ristiques de vSAN**

- **Software-defined** : pas besoin de SAN externe, tout est g√©r√© par ESXi
- **Stockage distribu√©** : les donn√©es sont r√©parties sur tous les h√¥tes du cluster
- **Haute disponibilit√© native** : les donn√©es sont automatiquement r√©pliqu√©es
- **Performance avec SSD** : utilise des SSD pour le cache, des HDD/SSD pour la capacit√© (Un peu comme le *[fleecing](../06%20-%206%20-%20Sauvegardes%20et%20restaurations/00-cours6.md#fleecing-%EF%B8%8F)* dans proxmox)
- **√âvolutif** : ajoutez des h√¥tes pour augmenter capacit√© et performance

**Architecture vSAN typique**

Chaque h√¥te ESXi dans un cluster vSAN contribue :
- **1 ou plusieurs SSD pour le cache** : acc√©l√®re les lectures et √©critures
- **1 ou plusieurs disques pour la capacit√©** : HDD ou SSD selon les besoins de performance

Les donn√©es d'une VM sont automatiquement distribu√©es et r√©pliqu√©es sur plusieurs h√¥tes selon la politique de stockage d√©finie.

**Exemple d'architecture 4 h√¥tes** :
```
Cluster vSAN
‚îú‚îÄ ESXi-01: 1x SSD 400GB cache + 4x HDD 2TB capacit√©
‚îú‚îÄ ESXi-02: 1x SSD 400GB cache + 4x HDD 2TB capacit√©
‚îú‚îÄ ESXi-03: 1x SSD 400GB cache + 4x HDD 2TB capacit√©
‚îî‚îÄ ESXi-04: 1x SSD 400GB cache + 4x HDD 2TB capacit√©

Capacit√© totale utilisable: ~16TB (apr√®s r√©plication)
```

**Politiques de stockage vSAN (Storage Policies)**

C'est ici que vSAN devient vraiment int√©ressant. Pour chaque VM ou disque virtuel, vous d√©finissez une politique qui d√©termine :

**Failures to Tolerate (FTT)** :
- Combien de pannes d'h√¥tes la VM peut survivre
- FTT=1 : survit √† 1 panne d'h√¥te (donn√©es sur 2 h√¥tes minimum)
- FTT=2 : survit √† 2 pannes d'h√¥tes (donn√©es sur 3 h√¥tes minimum)

**Stripe Width** :
- Sur combien d'h√¥tes distribuer les donn√©es pour la performance
- Plus c'est √©lev√©, plus la performance est distribu√©e

**Type de redondance** :
- **RAID 1 (Mirroring)** : copie compl√®te sur plusieurs h√¥tes, rapide mais gourmand en espace
- **RAID 5 (Erasure Coding)** : n√©cessite 4 h√¥tes minimum, plus √©conome en espace
- **RAID 6 (Erasure Coding)** : n√©cessite 6 h√¥tes minimum, tol√®re 2 pannes, tr√®s √©conome

**Exemple de politique** :
- VM de production critique : FTT=2, RAID 6
- VM de d√©veloppement : FTT=1, RAID 1
- VM de test : FTT=0 (aucune r√©plication)

## VMFS : Le syst√®me de fichiers VMware

VMFS (Virtual Machine File System) m√©rite qu'on s'y attarde car c'est le c≈ìur du stockage VMware. Comprendre VMFS vous aide √† mieux comprendre comment fonctionnent les datastores.

### Qu'est-ce que VMFS exactement ?

VMFS est un syst√®me de fichiers en cluster, ce qui signifie que **plusieurs h√¥tes ESXi peuvent lire et √©crire simultan√©ment sur le m√™me datastore** sans risque de corruption. C'est une capacit√© unique qui distingue VMware.

**Comparaison avec d'autres syst√®mes de fichiers** :

- **NTFS/ext4/XFS** : un seul syst√®me √† la fois peut √©crire, sinon corruption garantie
- **VMFS** : plusieurs ESXi peuvent √©crire en m√™me temps gr√¢ce √† un syst√®me de verrous sophistiqu√©
- C'est cette capacit√© qui permet vMotion, HA, et toutes les fonctionnalit√©s avanc√©es

### Comment VMFS √©vite les conflits ?

VMFS utilise un syst√®me de verrouillage au niveau fichier. Quand un h√¥te ESXi veut modifier un fichier (par exemple, √©crire dans un disque virtuel), il :

1. Demande un verrou sur ce fichier
2. Effectue ses modifications
3. Lib√®re le verrou

Les autres h√¥tes attendent leur tour. Tout cela se passe en millisecondes, de fa√ßon transparente.

**Exemple concret** :
Vous avez une VM sur ESXi-01 qui √©crit dans son disque. Vous d√©cidez de faire un vMotion vers ESXi-02. Pendant la migration :
- ESXi-01 garde le verrou sur les fichiers de la VM
- La VM continue de fonctionner normalement
- Une fois la m√©moire transf√©r√©e, ESXi-02 r√©cup√®re les verrous
- La VM fonctionne maintenant sur ESXi-02
- Tout √ßa en quelques secondes, sans interruption

### Versions de VMFS

**VMFS 5** (vSphere 5.0 √† 6.0) :
- Taille de bloc : 1 MB
- Fichier maximum : 2 TB (limitation importante)
- Datastore maximum : 64 TB

**VMFS 6** (vSphere 6.5 et sup√©rieur) :
- Taille de bloc : toujours 1 MB mais gestion am√©lior√©e
- Fichier maximum : 62 TB (√©norme am√©lioration !)
- Datastore maximum : 64 TB
- Meilleure gestion de l'espace

**Conseil** : utilisez toujours VMFS 6 pour vos nouveaux datastores.

### Structure d'un datastore VMFS

Quand vous cr√©ez un datastore VMFS, ESXi cr√©e une structure de r√©pertoires standardis√©e. Explorons-la.

**√Ä la racine du datastore** :
```
[Datastore1]
‚îú‚îÄ‚îÄ .fbb.sf          (fichier de m√©tadonn√©es VMFS)
‚îú‚îÄ‚îÄ .fdc.sf          (cache des descripteurs de fichiers)
‚îú‚îÄ‚îÄ .pb              (m√©tadonn√©es du datastore)
‚îú‚îÄ‚îÄ .sdd.sf          (sub-directory block)
‚îú‚îÄ‚îÄ .vh.sf           (en-t√™te de volume)
‚îú‚îÄ‚îÄ VM-Serveur-Web/  (dossier d'une VM)
‚îú‚îÄ‚îÄ VM-Database/     (dossier d'une autre VM)
‚îî‚îÄ‚îÄ ISO/             (dossier optionnel pour des ISOs)
```

:::danger
Les fichiers `.sf` et cach√©s sont utilis√©s par VMFS pour sa gestion interne. Ne les supprimez jamais !
:::

**√Ä l'int√©rieur du dossier d'une VM** :
```
[Datastore1] VM-Serveur-Web/
‚îú‚îÄ‚îÄ VM-Serveur-Web.vmx              (config de la VM - 5 KB)
‚îú‚îÄ‚îÄ VM-Serveur-Web.vmsd             (m√©tadonn√©es snapshots - 2 KB)
‚îú‚îÄ‚îÄ VM-Serveur-Web.vmdk             (descripteur disque - 1 KB)
‚îú‚îÄ‚îÄ VM-Serveur-Web-flat.vmdk        (disque r√©el - 40 GB)
‚îú‚îÄ‚îÄ VM-Serveur-Web.nvram            (BIOS/UEFI - 8 KB)
‚îú‚îÄ‚îÄ vmware.log                      (log de la VM - variable)
‚îî‚îÄ‚îÄ VM-Serveur-Web-12345.vswp       (swap - √©gal √† la RAM)
```

### Gestion avanc√©e : Thin vs Thick provisioning

Quand vous cr√©ez un disque virtuel, vous devez choisir le format de provisioning. C'est un concept important qui impacte l'utilisation de l'espace.

**Thin Provisioning** :
- Le disque n'occupe que l'espace r√©ellement utilis√©
- Vous cr√©ez un disque de 100 GB, mais s'il n'y a que 10 GB de donn√©es, il occupe 10 GB
- **Avantage** : √©conomie d'espace significative
- **Risque** : le datastore peut se remplir si toutes les VMs utilisent leur espace allou√©
- **Cas d'usage** : environnements de dev/test, VMs avec beaucoup d'espace "au cas o√π"

**Thick Provision - Z√©ro paresseux (Lazy Zeroed)** :
- L'espace est r√©serv√© imm√©diatement, mais les blocs ne sont pas effac√©s
- Cr√©ation rapide du disque
- L'espace est garanti disponible
- **Cas d'usage** : environnements de production standards

**Thick Provision - Z√©ro avard (Eager Zeroed)** :
- L'espace est r√©serv√© ET tous les blocs sont effac√©s √† z√©ro
- Cr√©ation lente (peut prendre des heures pour un gros disque)
- **Avantage** : meilleures performances, requis pour certaines fonctionnalit√©s (FT)
- **Cas d'usage** : VMs critiques n√©cessitant performances maximales

**Exemple pratique** :

Vous cr√©ez 5 VMs avec des disques de 100 GB chacune :
- **Thin** : pourrait n'occuper que 200 GB si les VMs sont peu remplies
- **Thick** : occupe imm√©diatement 500 GB

**Surveillance importante** : avec du Thin, surveillez l'espace du datastore ! Si toutes les VMs remplissent leurs disques simultan√©ment, le datastore peut saturer, causant l'arr√™t de toutes les VMs.

## Comparaison des types de stockage ü§π‚Äç‚ôÇÔ∏è

Maintenant que nous avons explor√© chaque type de stockage en d√©tail, comparons-les pour vous aider √† choisir la bonne solution selon vos besoins.

| Crit√®re | Local | NFS | iSCSI | Fibre Channel | vSAN |
|---------|-------|-----|-------|---------------|------|
| **Complexit√© installation** | Tr√®s faible | Faible | Moyenne | √âlev√©e | Moyenne |
| **Co√ªt initial** | Faible | Faible-Moyen | Moyen | √âlev√© | Moyen |
| **Performance** | Excellente | Bonne | Tr√®s bonne | Excellente | Excellente |
| **Latence** | moins de 1ms | 1-3ms | 1-2ms | moins de 1ms | moins de 1ms |
| **Support vMotion** | Non | Oui | Oui | Oui | Oui |
| **Support HA** | Non | Oui | Oui | Oui | Oui |
| **Infrastructure r√©seau** | Aucune | Ethernet | Ethernet | D√©di√©e (Fibre Channel) | Ethernet 10G+ |
| **√âvolutivit√©** | Limit√©e | Bonne | Bonne | Excellente | Excellente |
| **Thin provisioning** | Oui (VMFS) | Natif | Oui (VMFS) | Oui (VMFS) | Natif |
| **Cas d'usage id√©al** | Lab, serveur isol√© | PME, simplicit√© | Entreprise, bon compromis | Grands datacenters | Infrastructures modernes |

## Storage DRS (Distributed Resource Scheduler)

Storage DRS est l'√©quivalent de DRS mais pour le stockage. Il √©quilibre automatiquement l'utilisation du stockage dans un cluster de datastores.

**Qu'est-ce qu'un datastore cluster ?**

Un datastore cluster est un groupe de datastores g√©r√©s comme une seule entit√©. Au lieu de choisir manuellement sur quel datastore placer chaque VM, vous choisissez le datastore cluster, et Storage DRS d√©cide pour vous.

**Fonctionnalit√©s de Storage DRS** :

**1. √âquilibrage de l'espace** :
- Storage DRS surveille l'utilisation de l'espace de chaque datastore
- Si un datastore atteint un seuil (ex: 80% plein), Storage DRS migre automatiquement des VMs vers des datastores moins remplis
- Vous ne manquez jamais d'espace par surprise

**2. √âquilibrage des I/O** :
- Storage DRS surveille les performances (latence, IOPS)
- Si un datastore devient lent, Storage DRS migre des VMs vers des datastores moins charg√©s
- Les performances restent √©quilibr√©es automatiquement

**3. Placement initial intelligent** :
- Quand vous cr√©ez une VM, Storage DRS choisit automatiquement le meilleur datastore
- Il consid√®re l'espace disponible et la charge actuelle

**4. R√®gles d'affinit√©** :
- Vous pouvez cr√©er des r√®gles pour garder certaines VMs ensemble ou s√©par√©es
- Exemple : garder les VMs d'une application sur le m√™me datastore pour les performances