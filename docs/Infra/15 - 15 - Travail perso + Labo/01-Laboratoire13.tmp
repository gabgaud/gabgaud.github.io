import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Laboratoire 13

## Ajout d'un stockage CephFS pour vos fichiers ISOs et sauvegardes

## Pr√©alables ‚úÖ
  - Avoir compl√©t√© et test√© l'infrastructure du laboratoire 12

## Objectifs üéØ
  - Ajouter un stockage CEPH suppl√©mentaire pour vos Sauvegardes (et ISOs)

## √âtapes de r√©alisation üî¢

Lors de votre laboratoire pr√©c√©dent, vous avez mis en place un cluster Proxmox/Ceph, capable de d'√©talonner des donn√©es en blocs entre plusieurs serveurs. Vous avez pu cr√©er une premi√®re machine virtuelle et y stocker son disque dur. Or, qu'en est-il des fichiers ?

√Ä l'heure actuelle, votre cluster Ceph est incapable d'h√©berger des fichiers pour plusieurs raisons:

    1. Il utilise le stockage `RBD` et non un syst√®me de fichiers standard.

    2. `RBD` utilise l'algorithme **CRUSH** pour d√©terminer o√π sont stock√©s les *Placement Groups*.

    3. **CRUSH** est incapable de g√©rer un grand nombre de m√©tadonn√©es. Or, les fichiers en ont beaucoup (permissions, date de cr√©ation, taille, etc.)

Voyons donc comment nous pouvons utiliser Ceph pour accueillir **aussi** des fichiers.

### 1. Cr√©ation de serveurs de m√©tadonn√©es

C'est primordial! Comme je l'ai mentionn√© ci-dessus, **CRUSH** est incapable de g√©rer toutes les m√©tadonn√©es qu'utilise un syst√®me de fichiers standard tel que *ntfs* par exemple. Il nous faut donc mettre un √©l√©ment en place qui saura g√©rer ces m√©tadonn√©es.

S√©lectionnez l'un de vos noeuds Proxmox et dirigez-vous dans le sous-menu `Ceph`, puis s√©lectionnez `CephFS`. Dans la section *Metadata Servers*, cliques sur `Create`:

![CreateMDS](../Assets/15/CreateMDS.png)

Vous pouvez choisir un *ID* personnalis√© si vous le souhaitez, mais je ne vous le recommande pas. Oui, nous mettrons plus d'un *MDS* en place, mais sur diff√©rent noeud Proxmox afin d'augmenter la r√©silience du *cluster*. Ce n'est pas tr√®s utile d'avoir plus d'un *MDS* sur un m√™me noeud.

Ensuite cr√©ez au moins un autre *MDS* dans un second noeud. Vous pouvez aussi en cr√©er un troisi√®me si vous d√©sirez en avoir un dans chacun de noeuds Proxmox.

### 2. Cr√©ation du stockage pour les fichiers

Une fois vos serveurs de m√©tadonn√©es en place, vous aurez acc√®s au bouton `Create CephFS`. Cliquez sur ce-dernier et donnez-lui un nom du type `FilesAndBackups`.

#### 2.1 Le nombre de *Placement Groups*

Dans la m√™me fen√™tre, vous aurez la possibilit√© de d√©terminer le nombre de *PGs (Placement Groups)*. C'est une valeur importante √† ne pas n√©gliger. **Rappelez-vous:** Les *PGs* sont le moyen par lequel Proxmox distribue les donn√©es de mani√®re √©gale √† travers tous vos *OSDs (disques)*. Le r√¥le de l'algorithme **CRUSH** est de d√©terminer l'ensemble des *OSDs* pour chaque *PG*.

**DONC:**

  - Pas assez de *PGs*:<br/>
  La distribution des donn√©es est in√©gale, et si un *OSD* tombe en panne, le processus de r√©cup√©ration est concentr√© sur un petit nombre d'*OSDs* restants, ce qui ralentit la reconstruction.

  - Trop de *PGs*:<br/>
  Chaque PG consomme de la m√©moire et du CPU sur les moniteurs (*MON*) et les *OSDs*. Un nombre excessif peut d√©grader les performances globales et consommer trop de ressources, surtout dans les petits clusters.

>*Oui mais Gabriel, comment peut-on s'assurer d'indiquer le bon nombre alors ?*
>
>*-Les √©tudiants*

Bonne question! On utilisera une petit formule math√©matique pour √ßa. Les facteurs √† consid√©rer sont le nombre total d'*OSDs* ainsi que le nombre de r√©plicas (par d√©faut 3). La formule va comme suit:
<br/>
<div style={{textAlign: 'center'}}>
$$\LARGE \text{PGs} = \frac{\text{Nombre total d'OSDs} \times 100}{\text{Nombre de r√©plicas}}$$
</div>
<br/>
Le r√©sultat doit √™tre arrondi √† la puissance de 2 **sup√©rieure** la plus proche.

|Nombre d'*OSDs* total|R√©plicas|Calcul|Puissance de 2 sup√©rieure|
|:---------------------:|:--------:|------|:-------------------------:|
|3|3|(3 x 100)/3 = 100|**128**|
|6|3|(6 x 100)/3 = 200|**256**|
|12|3|(12 x 100)/3 = 400|**512**|
|20|3|(20 x 100)/3 = 667|**1024**|

:::tip
On a 6 *OSDs* au total dans notre laboratoire üòä
:::

### 3. D√©placement des fichiers ISOs

Une fois votre nouveau stockage CephFS en place. Il serait int√©ressant d'y d√©placer nos fichiers *ISOs*. Si vous avez utilis√© le mod√®le de Proxmox que je vous ai fourni, vos noeuds poss√®dent tous respectivement une copie des *ISOs*, ce qui n'est vraiment pas optimal.

Les *ISOs* sont accessible via `/var/lib/vz/template/iso` depuis le shell Proxmox. Votre stockage CephFS, quant √† lui, se trouve dans `/mnt/pve/[NomDuStockage]`. Pour que Proxmox puisse d√©tecter les *ISOs* convenablement, vous devez absolument les mettre dans les sous-dossiers `/template/iso`.

D√©placez donc vos *ISOs* dans votre stockage CephFS. Une fois que ce sera fait, supprimer les *ISOs* dans les stockages `local` de votre autres noeuds.