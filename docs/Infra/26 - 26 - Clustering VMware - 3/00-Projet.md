import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Projet final - Phase 3

:::danger[Travail individuel]
Le projet doit √™tre r√©alis√© individuellement. L'entraide est permise, cependant:
  - Vos captures d'√©cran doivent √™tre uniques et prises par vous.
  - Votre documentation et vos explications doivent √™tre votre composition (pas celle de l'IA ü•≤ ni celle d'un coll√®gue)
  - Le plagiat (IA, coll√®gue ou autre) entrainera syst√©matiquement une note de z√©ro.
:::

* * *

Il est essentiel que vous ayez d√ªment compl√©t√© la Phase 1 & 2 du projet avant de vous lancer dans cette portion du projet. Prendre le risque de sauter certaines √©tapes pourrait vous obliger √† tout recommencer.

* * *

## Phase 3

Dans la phase 3, nous mettons en place un bloc de stockage distribu√© √† la mani√®re de CEPH dans Proxmox.

### 13. vSAN

**Rappelez-vous:** dans les phases pr√©c√©dentes du projet, vous avez mis plusieurs disques durs dans chacun de vos noeuds ESXi. Voici un sch√©ma de l'infrastructure √† mettre en place:

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ArchitecturevSAN_W.svg'),
            dark: useBaseUrl('/img/Virtu/ArchitecturevSAN_D.svg'),
        }}
    />
</div>

#### 13.1 Modifications au niveau des disques durs

Pour cette section, nous devront apporter des modifications au ¬´ mat√©riel ¬ª de vos hyperviseurs ESXi. En effet, vous vous souvenez peut-√™tre que je vous ai fait ajouter un disque suppl√©mentaire de 40Go et un disque suppl√©mentaire 250Go dans la phase 1. Je vous ai fait branch√© ces deux disques durs sur un contr√¥leur SATA. Or, nous allons devoir changer ce contr√¥leur pour mette en place **vSAN**.

Cette situation simule parfaitement la mise √† jour d'un serveur physique dans un *cluster*. Je vous donne les √©tapes √† suivre pour faite cette modification sur chacun de vos noeuds.

:::danger[Risque important!]
**ATTENTION:** Si vous faites une mauvaise manipulation ici, vous risquez de supprimer le disque dur syst√®me de l'un de vos hyperviseurs. Soyez vigilants! Au besoin, faites des *snapshots*.
:::

**Pour chacun des noeuds:**

1. Passez le noeud en mode maintenance üöß dans **vCenter** (clic droit sur le noeud). Cela d√©clenchera la migration des machines virtuelles vers d'autres noeuds disponibles. Avant que vous me le demandiez: oui, **vCenter** survivra √† ce transfert temporaire. Inutile d'entrez dans des modifications de RAM etc.

2. Une fois le noeud pass√© en mode maintenance (t√¢che termin√©e), √©teignez le noeud directement depuis l'interface de **vCenter**.

3. Dans LabInfo, supprimez les disques de 40Go et de 250Go suppl√©mentaires. ‚ö†Ô∏è **ATTENTION** ‚ö†Ô∏è de ne pas supprimez le disque syst√®me! (Le disque syst√®me est g√©n√©ralement le premier de la liste, en haut).

4. Toujours dans LabInfo, ajoutez un contr√¥leur NVMe.

5. Ajoutez de nouveau vos disques durs de 40Go et 250Go respectivement, en vous assurant que ceux-ci sont reli√©s au contr√¥leur NVMe que vous avez ajout√©.

6. Par d√©faut, LabInfo ajoute un nouveau contr√¥leur de stockage SCSI lorque vous ajoutez un disque dur. Supprimez ce-dernier. Nous n'en avons pas besoin.

7. Red√©marrez votre noeud.

8. Une fois le noeud d√©marr√© et bien visible dans **vCenter**, quittez le mode maintenance.

#### 13.2 Cr√©ation des VMkernel vSAN

Pour chaque noeud, cr√©ez une nouvelle interface de type *VMKernel port* que vous brancherez dans le groupe de port `DPG-vSan` de votre vDS. **N'oubliez pas de lui configurer une IP dans un subnet seul ainsi que de cocher le service vSAN**.

*pssst:* N'oubliez pas que nous utilisons un MTU de 9000.

#### 13.3 Lancer l'assistant vSAN

M√™me si cela peut para√Ætre contre-productif, il nous faudra d√©sactiver <u>temporairement</u> **vSphere HA** pour activer **vSAN**. Dans un contexte de production, nous aurions probablement activ√© tout ces services en m√™me temps √† la cr√©ation du *cluster*. Or, l'objectif de ce laboratoire est de vous faire exp√©rimenter chaque √©l√©ment s√©par√©ment. Dirigez-vous donc dans votre inventaire et s√©lectionnez votre *cluster*. Allez dans le menu `configurer` ‚Üí `Disponibilit√© vSphere`. Cliquez sur `Modifier...` et arr√™tez le service.

Une fois cette premi√®re √©tape compl√©t√©e, restez dans le menu `configurer` du *cluster* et rep√©rez le menu `vSan` ‚Üí `Services`.

Laissez les options par d√©faut coch√©s:
- vSAN HCI
- Cluster vSAN √† un seul site

Cliquez sur `Configurer`

![ConfigurevSAN](../Assets/Projet/ConfigurevSAN.png)

:::tip[psst!ü§´]
Vous vous demandez quels sont les autres types de d√©ploiement ? Vous voulez en savoir davantage sur la technologie vSAN ? [Consultez cette page](https://techdocs.broadcom.com/fr/fr/vmware-cis/vsan/vsan/8-0/planning-and-deployment/building-a-virtual-san-cluster/vsan-deployment-options.html)
:::

Dans la premi√®re fen√™tre, plusieurs √©l√©ments apparaitront et il est important de bien les distinguer:

![vSANESA](../Assets/Projet/vSANESA.png)

1. **vSAN ESA:**<br/>
vSAN ESA (*Express Strorage Architecture*) est une version que je qualifierais d'abr√©g√©e de **vSAN**. Cette version fonctionne diff√©remment de la version originale et poss√®de moins de param√©trabilit√©. C'est pourquoi nous ne l'utiliseront pas. Laissez cette option d√©sactiv√©e.

2. **Avertissement p√©riph√©rique certifi√©:**<br/>
Vous verrez apparaitre un avertissement concernant votre p√©riph√©rique NVMe. Vous pouvez l'ignorer. *Grosso modo*, **VMware** nous avise que le p√©riph√©rique en question n'a pas √©t√© certifi√© par l'entreprise pour ce genre d'utilisation. C'est un peu normal; c'est un p√©riph√©rique virtuel... Le contr√¥leur NVMe en question n'existe pas.

![vSANServices](../Assets/Projet/vSANServices.png)

3. **Efficacit√© de l'utilisation de l'espace:**<br/>
**vSAN** peut mettre en place des strat√©gies de compression et de d√©duplication pour √©conomiser de l'espace. La compression permet de r√©duire l'espace que prend certaines donn√©es en les passant dans un algorithme de compression, alors que la d√©duplication d√©tecte les donn√©es qui sont stock√©s en double pour n'en conserver qu'une seule copie. Dans les deux cas, ce sont des strat√©gies tr√®s int√©ressantes √† mettre en place dans un contexte de production. D'ailleurs ces strat√©gies sont activ√©es sur LabInfo.

4. **Chiffrement:**<br/>
Vous pouvez chiffrer les donn√©es pour des raisons de s√©curit√©. Cela dit, vous devez consid√©rer que le chiffrement n√©cessite de la puissance de calcul suppl√©mentaire. C'est √† consid√©rer lors de la mise en place de vos serveurs.

5. **Redondance r√©duite et RDMA:**<br/>
La redondance r√©duite dont l'utilitaire nous parle ici concerne les cas o√π un noeud pourrait √™tre en panne. L'utilisation de **vSAN** pr√©vient la perte de donn√©es lorsque tous les noeuds sont op√©rationnels. Or, si un noeud tombe en panne et que vous modifiez l'√©tat d'une *VM* stock√©e sur **vSAN**, ses donn√©es ne seront plus prot√©g√©es. N'autorisez donc pas cette situation. Quant √† RDMA, il s'agit d'une technologie r√©seau avanc√©e permettant un transfert plus rapide des donn√©es. Pour b√©n√©ficier de cette technologie, il faut des cartes r√©seaux sp√©cifiques, que nous n'avons pas.

![vSANDisks](../Assets/Projet/vSANDisks.png)

6. **R√©clamation des disques:**<br/>
Cette √©tape est cruciale. C'est ici que **vSAN** cr√©era les groupes de disques. G√©n√©ralement, **vSAN** arrive √† r√©clamer les disques de mani√®re intelligente. Dans l'image que je vous ai partag√©, **vSAN** a automatiquement associ√© mes disques dur de 40Go au cache et mes disques durs de 250Go au stockage.

    M√™me si **vSAN** semble attribuer stockages intelligemment chaque fois, assurez-vous de valider avant de continuer.

:::info[cache versus capacit√©]
Cette fa√ßon de proc√©der de **vSAN** est tout √† fait ing√©nieuse. En production, on utilisera les disques durs tr√®s rapides (stockage flash) pour la cache ainsi que les disques durs plus lents mais plus gros pour le r√©el stockage √† long terme. Cette fa√ßon de proc√©der acc√©l√®re **grandemment** le transfert de donn√©es entre les noeuds **et** le stockage lui-m√™me.
:::

7. **Cr√©er les domaines de pannes:**<br/>
Normalement, tous vos noeuds devraient apparaitre ici. Vous n'avez rien √† faire.

8. **Terminer**

F√©l√©licitations, vous avez maintenant un ensemble de stockage distribu√© √† la mani√®re de CEPH sous Proxmox. Une fois les t√¢ches li√©es √† **vSAN** termin√©es, r√©activez **vSphere HA**.

![vSANHealth](../Assets/Projet/vSANHealth.png)

### 14. Politiques et strat√©gies de stockage

Notre **vSAN** est fonctionnel et pr√™t √† √™tre exploit√©. Nous pourrions d√®s maintenant cr√©er des machines virtuelles et les stocker dans notre syst√®me de stockage distribu√©. Or, VMware ne s'est pas arr√™t√© ici. L'entreprise a voulu distinguer les machines virtuelles entre elles. Toutes les *VMs* ne sont pas √©gales, nous le savons tous. On ne traitera pas une *VM* qui ne sert qu'√† effectuer des tests de la m√™me fa√ßon qu'une *VM* h√©bergant un contr√¥leur de domaine.

C'est ici qu'entre en jeu les strat√©gies de stockage pour **vSAN**. En gros, **vSAN** peut accentuer ou, au contraire, amoindrir le niveau de s√©curit√© sur le stockage d'une *VM*.

Dans **vCenter**, cliquez sur cet ic√¥ne: <FAIcon icon="fa-icons fa-bars"/> (au haut √† gauche de l'interface web) et s√©lectionnez `Strat√©gies et profils`.

![Strat√©giesProfils](../Assets/Projet/StrategiesProfils.png)

#### 14.1 Strat√©gie VM Production

Cliquez sur `Cr√©er` puis entrez les informations suivantes:

- **Nom:** VM Production
- **Structure de la strat√©gie:** ‚úÖ Activez les r√®gles du stockage **vSAN**
- **vSAN:**
    - **Tol√©rance aux pannes du site:** Aucun - Cluster Standard
    - **Pannes tol√©r√©es:** 1 panne : Raid 1 (mise en mirroir)
    - **Services de chiffrement:** Aucune pr√©f√©rence
    - **Efficacit√© de l'utilisation de l'espace:** D√©duplication et compression
    - **Niveau de stockage:** Int√©gralement flash
    - **R√®gles de strat√©gie avanc√©es:** Tous par d√©faut
- **Compatibilit√© de stockage:** Vous devriez voir votre **vSAN**
- **Terminer**

Cette premi√®re strat√©gie vise √† utiliser notre **vSAN** pour prot√©ger et optimiser nos machines de production.

#### 14.2 Strat√©gie VM D√©veloppement

Cliquez sur `Cr√©er` puis entrez les informations suivantes:

- **Nom:** VM D√©veloppement
- **Structure de la strat√©gie:** ‚úÖ Activez les r√®gles du stockage **vSAN**
- **vSAN:**
    - **Tol√©rance aux pannes du site:** Aucun - Cluster Standard
    - **Pannes tol√©r√©es:** Aucune - Aucune redondance
    - **Services de chiffrement:** Aucune pr√©f√©rence
    - **Efficacit√© de l'utilisation de l'espace:** Aucune pr√©f√©rence
    - **Niveau de stockage:** Aucune pr√©f√©rence
    - **R√®gles de strat√©gie avanc√©es:** Tous par d√©faut
- **Compatibilit√© de stockage:** Vous devriez voir votre **vSAN**
- **Terminer**

Cette deuxi√®me strat√©gie vise √† utiliser notre **vSAN** pour nos machines de tests et n'assure aucune protection.

### 15. Migration de nos VMs sur vSAN

Enfin! Nous y voil√†, la derni√®re √©tape du projet üòä. Nous avons mis en place stockage distribu√© **vSAN**, mais nos machines virtuelles, elles, sont toujours stock√©es sur **TrueNAS**. Ce serait bien de migrer **vCenter** et notre *VM* de test sur **vSAN**.

Heureusement, c'est assez simple. 

#### 15.1 Migration de la VM de test

Commencons par notre machine de test. Dans mon cas, j'avais install√© une machine Ubuntu Server. Pour la migrer, clic-droit ‚Üí `migrer`. Cependant, je vais choisir `Modifier uniquement le stockage`:

![StorageOnly](../Assets/Projet/StorageOnly.png)

√Ä l'√©tape suivante, je s√©lectionnerai √©videmment mon stockage **vSAN** et la politique de stockage **VM D√©veloppement**:

![vSANDataStore](../Assets/Projet/vSANDataStore.png)

Il faudra calculer quelques minutes pour que le transfert soit complet. (~5 min)

#### 15.2 Migration de la VM vCenter

Vous l'aurez compris, c'est exactement la m√™me proc√©dure √† l'exception de la politique de stockage. Dans le cas de **vCenter**, utilisez la politique `VM Production`.

<span class="red-text"><u>Une fois le transfert compl√©t√©</u></span>, vous pourrez √©teindre **TrueNAS**, il ne servira plus √† rien. ADIEU TRUENAS! ON SE REVOIT DE L'AUTRE BORD! ü™¶üò≠

### 16. Grille de correction et remise

<span class="red-text fonttaller">**DATE DE REMISE: 19 D√âCEMBRE 2025 - 23h59**</span><br/><br/>

Pour ce travail, vous devrez me remettre :

- 5 captures d'√©cran tr√®s sp√©cifiques
- 1 vid√©o de type ¬´ Preuve de vie ¬ª

#### 16.1 Captures d'√©cran

:::danger[Crit√®res stricts pour les captures d'√©cran]
Toutes les captures d'√©cran que vous remettrez devront r√©pondre aux crit√®res suivants:
- La capture d'√©cran doit afficher l'heure syst√®me de votre poste Windows **physique** (barre des t√¢ches visible).
- Le nom du fichier doit √™tre indiqu√© comme suit: Nomdefamille_Prenom_Capture. Ex: Gaudreault_Gabriel_Capture01.png

Les captures d'√©cran ne r√©pondant pas √† ces crit√®res ne seront pas analys√©es.
:::

##### 16.11 Capture d'√©cran 1

Prenez une capture d'√©cran de la commande `esxcli system uuid get` ex√©cut√© depuis une invite de commande SSH sur l'un de vos hyperviseurs. <br/>**Exemple:**

![Capture01](../Assets/Projet/Capture01.png)

##### 16.12 Capture d'√©cran 2

Prenez une capture d'√©cran de votre serveur DNS montrant votre zone de recherche directe. On doit y voir votre domaine personnalis√©e et les enregistrements A pour vCenter et les 3 ESXi.<br/>**Exemple:**

![Capture02](../Assets/Projet/Capture02.png)

##### 16.13 Capture d'√©cran 3

Prenez une capture d'√©cran de la topologie de votre vDS Production dans **vCenter**. On doit y voir les machines virutelles connect√©es.<br/>**Exemple:**

![Capture03](../Assets/Projet/Capture03.png)

##### 16.14 Capture d'√©cran 4

Prenez une capture d'√©cran de l'√©tat de **vSphere HA** dans **vCenter**. On doit y voir le nom de votre *cluster*, l'√©tat et les conditions de r√©ponse en cas de panne.<br/>**Exemple:**

![Capture04](../Assets/Projet/Capture04.png)

##### 16.15 Capture d'√©cran 5

Prenez une capture d'√©cran de votre cluster **vSAN**. On doit y voir le nom de votre *cluster*, l'√©tat de sant√© de chaque noeud ainsi que le nombre de disque utilis√©s sur chacun de ceux-ci.<br/>
**Exemple:**

![Capture05](../Assets/Projet/Capture05.png)

#### 16.2 Vid√©o ¬´ preuve de vie ¬ª 

Vous devez enregistrer votre √©cran et votre voix. Vous pouvez utiliser OBS, Teams ou m√™me votre t√©l√©phone s'il est stable pour vous enregistrer.

Sc√©nario √† suivre **obligatoirement** en une seule prise (pas de montage/coupage):

1. **Identit√©:**<br/>
Identifiez-vous clairement d√®s les premi√®res secondes du vid√©o. Dites clairement votre nom et le nom du cours.

2. **Contexte:**<br/>
Montrez l'interface de **vCenter**. On doit voir clairement le nom de votre *cluster* et vos h√¥tes.

3. **DNS:**<br/>
Montrez votre zone de recherche direct au sein de votre serveur DNS. On doit y voir vos enregistrements A pour vos hyperviseurs et **vCenter**.

4. **Test:**<br/>
Ouvrez la console de votre machine virtuelle de test dans **vCenter**. Lancez la commande: `ping -i 1 google.com` et laissez-la rouler.

5. **Migration:**<br/>
Retournez sur **vCenter**, faites un clic-droit sur la vm ‚Üí **Migrer** ‚Üí **Changer de ressource de calcul uniquement.** S√©lectionnez un autre noeud et validez.

6. **Preuve de fonctionnement**<br/>
Pendant que la machine virtuelle se fait migr√©, retournez sur la console de celle-ci et d√©montrez que la commande `ping` n'a jamais cess√© de fonctionner. Une fois la migration termin√©e, montrez le nouvel emplacement de la machine et stoppez la commande `ping`. Prenez soin de bien montrer le r√©sultat de la commande `ping` une fois celle-ci arr√™t√©e. Il ne devrait pas y avoir de paquet perdu.

#### 16.3 D√©p√¥t

Lorsque vous aurez r√©uni tous les √©l√©ments n√©cessaires (vid√©o et captures d'√©cran), d√©posez-les dans un dossier que vous nommerez *NomDeFamille_Prenom_Projet* et compressez ce-dernier en `.zip`.<br/> [D√©posez votre dossier compress√© ici.](https://cloud.tonprof.ca/index.php/s/KiEbH5LzMMYeGC5)

#### 16.4 Grille de correction

:::important
La correction est bas√©e sur des preuves visuelles. Si une preuve est floue, illisible, incompl√®te ou ne respecte pas les crit√®res qui sont explicitement demand√©s, la note attribu√©e au crit√®re sera de 0.
:::

:::danger[Plagiat = 0]
Tout indice sugg√©rant que les captures d'√©cran ou la vid√©o proviennent d'un autre √©tudiant entra√Ænera automatiquement la note de 0% au projet.
:::

|Crit√®re|D√©tail des exigences pour obtenir les points|Points|
|-------|--------------------------------------------|:------:|
|**1. Conformit√© administrative**|-Les 5 captures d'√©cran demand√©es sont fournies.<br/>-Le vid√©o est clair, stable et pr√©cis.<br/>-L'heure syst√®me de l'ordinateur h√¥te est visible sur **TOUTES** les captures d'√©cran.|2|
|**2. Capture UUID**|-La commande `esxcli system uuid get` est visible<br/>-Le UUID retourn√© est unique|2|
|**3. Capture DNS**|-Le nom de domaine est bien `prenom.lan`<br/>-Les enregistrements A pour les 3 ESXi et vCenter sont pr√©sents<br/>-Le domaine cemti.ca n'a pas √©t√© utilis√©|2|
|**4. Topologie vDS**|-On voit clairement les machines connect√©es<br/>-On voit bien les groupes de ports|2|
|**5. vSphere HA**|-vSphere HA est bien activ√©<br/>-Les conditions de r√©ponse sont les bonnes.|2|
|**6. vSAN**|-L'√©tat de sant√© global est vert<br/>-Le nombre de disques utilis√©s est bon.|2|
|**7. Vid√©o: Identit√©**|L'√©tudiant √©nonce son nom et le cours vocalement et clairement.<br/>-Le nom du *cluster* vCenter contient les initiales de l'√©tudiant.<br/>-Le vid√©o est fait en une seule prise.|2|
|**8. Vid√©o: Migration**|La migration est initi√©e convenablement (ping, etc.)<br/>-La t√¢che se compl√®te avec succ√®s et c'est bien d√©montr√©.|3|
|**9. Vid√©o: Continuit√©**|Le `ping` roule durant tout le processus de migration.<br/>-Aucun paquet (1 acceptable) perdu lors du basculement.|3|