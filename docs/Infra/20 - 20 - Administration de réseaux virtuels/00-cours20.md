import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 20

# Administration de r√©seaux virtuels

## Vue d'ensemble de l'architecture

Dans un environnement virtuel tel que vSphere, les machines virtuelles doivent pouvoir communiquer entre elles et avec le monde ext√©rieur, tout comme des serveurs physiques le feraient sur un r√©seau standard. La virtualisation du r√©seau nous permettra de:

- **Isoler le trafic:** S√©parer diff√©rents types de communication.
- **Optimiser les ressources:** Partager intelligemment les cartes r√©seaux physiques.
- **Simplifier la gestion:** Configurer le r√©seau de mani√®re centralis√©e.
- **Assurer la disponibilit√©:** Cr√©er de la redondance sans multiplier le mat√©riel.

L'architecture r√©seau de vSphere peut se visualiser en **trois couches:**

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/3couches_W.svg'),
            dark: useBaseUrl('/img/Virtu/3couches_D.svg'),
        }}
    />
</div>

1. **Couche physique (Infrastructure mat√©rielle)**
    - Le mat√©riel r√©el et tangible
    - Cartes r√©seau physiques (*vmnic*), c√¢bles, switches, etc.

2. **Couche de virtualisation**
    - Le ¬´ cerveau ¬ª qui fait le lien entre la couche physique et la couche virtualis√©
    - *vSwitches*,*Port Groups*,*VMkernel Ports*

3. **Couche virtualis√©**
    - Les *VMs* et leurs cartes r√©seau virtuelles (*vNIC*)
    - Ce que voient et utilisent les *VMs*

## Les composants de la couche physique

### Les vmnic (cartes r√©seau physiques)

Une **vmnic** (*virtual machine network interface card*) est simplement une carte r√©seau physique install√©e dans votre serveur ESXi. Elle constitue le point de connexion entre le monde virtuel et le r√©seau physique.

:::important
- Les *vmnic* sont d√©tect√©es automatiquement par ESXi au d√©marrage.
- Elles sont num√©rot√©es dans l'ordre de d√©tection et non dans l'ordre physique.
- On ne configure pas directement les *vmnic*, on les assigne √† des *vSwitches*.
:::

Dans **vCenter**, vous retrouverez les *vmnic* dans les configurations de chaque noeud, dans le menu `configurer`, puis `mise en r√©seau`:

![vmnic](../Assets/20/vmnic.png)


**Exemple:**

Un serveur ESXi avec 4 ports r√©seau aura *vmnic0, vmnic1, vmnic2 et vmnic3*. Nous pourrions alors faire un attribution similaire √† √ßa:

- *vmnic0 et vmnic1* pour le trafic des *VMs*.
- *vmnic2* pour la gestion de l'h√¥te ESXi lui-m√™me.
- *vmnic3* pour vMotion et le stockage.

#### Bonnes pratiques pour les vmnic üí°

Tentez toujours de s√©parer les diff√©rents types de trafic tout en assurant une redondance √† l'aide d'au moins deux *vmnic* par fonction critique.

Une fonction critique d√©signe un type de trafic r√©seau important pour l'infrastructure. On parle de:

- **Gestion de l'ESX** (*management*)
- **Trafic des VMs** (production)
- **vMotion** (migration)
- **Storage** (iSCSI, NFS)
- **Replication**

**Exemple #1 (4 cartes r√©seau) :**

```
vSwitch0 (Management + VMs de production) :
‚îú‚îÄ vmnic0 ‚Üí Fonction critique = Management + VMs
‚îî‚îÄ vmnic1 ‚Üí Redondance pour cette fonction

vSwitch1 (vMotion + Storage) :
‚îú‚îÄ vmnic2 ‚Üí Fonction critique = vMotion + Storage  
‚îî‚îÄ vmnic3 ‚Üí Redondance pour cette fonction
```

**Exemple #2 (6 cartes r√©seau) :**

```
vSwitch0 (Management) :
‚îú‚îÄ vmnic0 ‚Üí Fonction = Management ESXi
‚îî‚îÄ vmnic1 ‚Üí Redondance

vSwitch1 (VMs Production) :
‚îú‚îÄ vmnic2 ‚Üí Fonction = Trafic des VMs
‚îî‚îÄ vmnic3 ‚Üí Redondance

vSwitch2 (Storage) :
‚îú‚îÄ vmnic4 ‚Üí Fonction = iSCSI/NFS
‚îî‚îÄ vmnic5 ‚Üí Redondance
```

## Les composants de la couche de virtualisation

### Les VMkernel ports

Un **VMkernel Port** est une interface r√©seau utilis√© par **<mark>l'h√¥te ESXi lui-m√™me pour ses propres services (pas les *VMs*)</mark>**. C'est une carte r√©seau pour le serveur ESXi.

:::caution
- **vNIC** ‚Üí Utilis√©e par les *VMs*
- **VMKernel Port** ‚Üí Utilis√©e par ESXi
:::

Voici une liste des diff√©rents services qui passeront via un **VMkernel port**:

|Service|Caract√©ristiques|
|-------------------------------------|-------------------------------------------------|
|**Management** (Gestion de l'ESXi)|- Acc√®s √† l'interface web<br/>- Connexion √† vCenter<br/>- SSH<br/>- Port par d√©faut: `vmk0`|
|**vMotion** (Migration)|- Migration de *VMs* √† chaud entre h√¥tes<br/>- N√©cessite beaucoup de bande passante<br/>- Devrait avoir son propre r√©seau d√©di√©|
|**vSAN** (Stockage distribu√©)|- Communication pour le stockage distribut√© vSAN<br/>- Trafic intensif, n√©cessite un r√©seau d√©di√© aussi|
|**Stockage r√©seau** (iSCSI, NFS)|- Connexion aux syst√®mes de stockage en r√©seau<br/>- Performance critique pour les *VMs*|
|**Approvisionnement** (Provisionning)|- Utilis√© pour le clonage et le d√©ploiement de *VMs*<br/>- Trafic moins fr√©quent mais volumineux|
|**R√©plication**|- Pour vSphere Replication<br/>- Synchronisation des donn√©es inter-sites|

Bref, les ports de type *VMkernel* ne concerne que le trafic qui s'adresse aux hyperviseurs. Exemple:

```
vmk0 ‚Üí Management (10.0.1.10)
vmk1 ‚Üí vMotion (10.0.2.10)
vmk2 ‚Üí iSCSI Storage (10.0.3.10)
```

### Les vSwitches Standard (VSS)

Un **vSwitch Standard** (vSS) est un commutateur r√©seau virtuel qui fonctionne au niveau de l'hyperviseur ESXi. Pensez-y comme un switch r√©seau physique, mais compl√®tement virtuel.

**Caract√©ristiques principales:**

- Cr√©√© et g√©r√© **individuellement sur chaque h√¥te** ESXi
- Gratuit, inclus avec ESXi (**OMG** üòÆ)

:::tip[*PSSST*]
√áa ne vous fait pas penser √† un concept que nous avons vu avec Proxmox ?
:::

#### Composants d'un vSS

Un vSwitch est compos√© de plusieurs √©l√©ments qui travaillent ensemble:

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/vSwitchStandard_W.svg'),
            dark: useBaseUrl('/img/Virtu/vSwitchStandard_D.svg'),
        }}
    />
</div>

##### Les Ports Groups

Un **Port Group** est un regroupement logique de ports sur un *vSwitch*. C'est le point de connexion pour les *VMs* et les *VMkernel Ports*. Il d√©finit les param√®tres r√©seau qui seront appliqu√©s aux machines qui s'y connectent:

- VLAN ID
- Politiques de s√©curit√©
- Politiques de *load balancing* et de *failover*

C'est l'√©quivalent d'un groupe de ports configur√© de la m√™me fa√ßon sur un vrai *Switch*.

##### Les *uplinks* (liaison montante)

Les *uplinks* repr√©sentent le lien entre le *vSwitch* et la carte r√©seau physique (*vmnic*). Quelques point √† retenir en ce qui concerne les *uplinks*:

- Un vSwitch peut avoir plusieurs *uplinks* (redondance)
- Chaque *uplink* est associ√© √† une *vmnic* sp√©cifique. Autrement dit: 1 *uplink* = 1 *vmnic*
- L'ordre des *uplinks* d√©termine les priorit√©s de *failover*

#### Configuration d'un vSwitch Standard (ESXi)

##### √âtape 1 - Cr√©er le vSwitch

![CreateVSS](../Assets/20/CreateVSS.png)
![CreateVSS2](../Assets/20/CreateVSS2.png)


:::note
Ne vous pr√©occupez pas des petis menus *D√©couverte de liaison* et *s√©curit√©*. Nous analyserons ces options dans les prochaines √©tapes.
:::

##### √âtape 2 - Cr√©er des groupes de ports

![CreatePortGroup](../Assets/20/PortGroup.png)
![CreatePortGroup2](../Assets/20/PortGroup2.png)

:::caution
**VLAN 0** = Pas de VLAN<br/>
**VLAN 1-4094** = Num√©ro de VLAN sp√©cifique<br/>
**VLAN 4095** = VLAN *trunking*
:::

##### √âtape 3 - Cr√©er un VMKernel Port si n√©cessaire

![NewVMKernelNIC](../Assets/20/NewVMKernelNIC.png)
![NewVMKernelNIC2](../Assets/20/NewVMKernelNIC2.png)

#### Les politiques de s√©curit√© d'un VSS üîê

Les politiques de s√©curit√© d'un *vSwitch Standard* permettent de contr√¥ler le comportement du trafic. Vous pouvez √©diter ces politiques en s√©lectionnant votre *vSwitch* et en cliquant sur `Modifier les param√®tres`.

![vSwitchPols](../Assets/20/vSwitchPols.png)

En mati√®re de s√©curit√©, voici les √©l√©ments que vous pourriez ajuster en fonction de votre contexte:

##### Mode Promiscuit√© (Promiscuous Mode) üëÇ

Ce mode permet d'analyser tout le trafic passant sur un m√™me groupe de ports. Par d√©faut, ce mode est **d√©sactiv√©** pour des raisons √©videntes de s√©curit√©. N√©anmoins, un administrateur pourrait vouloir activer ce mode dans des circonstances pr√©cises telle que la mise en place d'un IDS/IPS ou un analyseur de bande passante.

##### Changement d'adresse MAC

Ce mode permet aux diff√©rentes *VMs* de changer leur adresse MAC. C'est g√©n√©ralement accept√© (c'est d'ailleurs le comportement par d√©faut), mais on pourrait le refuser dans certains environnements tr√®s s√©curis√©.

##### Transmissions forg√©es

Cette politique permet aux *VMs* d'envoyer des trames sur le r√©seau avec une adresse MAC source diff√©rente de la leur. D√©pendamment du contexte dans lequel vous oeuvrez, vous pourriez vouloir pr√©venir certaines attaques qui utilisent ce stratag√®me ou tout simplement √©viter ce genre de comportement.

#### Les politiques de *teaming* et de *failover*

Ces politiques d√©finissent comment les *uplinks* sont utilis√©s et comment la redondance fonctionne.

![TeamingFO](../Assets/20/TeamingFO.png)

##### √âquilibrage de charge (*load balancing*) ‚öñÔ∏è

1. *Route based on originating virtual port* **(D√©faut)**
    - Chaque VM est assign√©e √† un *uplink* au d√©marrage.
    - Simple, rapide et efficace.
    - **Recommand√©** pour la plupart des cas.

2. *Route based on IP hash*
    - Distribution bas√©e sur les adresses IP source et destination
    - N√©cessite EtherChannel/LACP sur le switch physique
    - Meilleur √©quilibrage pour les *VMs* avec beaucoup de connexions

3. *Route based on MAC hash*
    - Distribution bas√©e sur l'adresse MAC de la *VM*
    - Similaire √† *virtual port*, mais avec un calcul diff√©rent.

4. *Use explicit failover order*
    - Utilise toujours l'*uplink* actif prioritaire
    - Les autres *uplinks* sont en standby seulement
    - Pas d'√©quilibrage de charge

##### D√©tection de r√©seau en panne (*Network Failover Detection*)

1. *Link Status Only* **(D√©faut)**
    - D√©tecte seulement si le c√¢ble est d√©branch√©
    - Rapide mais limit√©

2. *Beacon probing*
    - Envoie des paquets de test entre *uplinks*
    - D√©tecte plus de probl√®mes r√©seau, plus fiable
    - Cr√©e l√©g√®rement plus de trafic

3. *Failover Order*
    - Permet de configurer la priorit√© des *uplinks*

#### Exemple de configuration r√©aliste

Dans un sc√©narion o√π nous aurions un cluster de 3 noeuds, dont les noeuds poss√®dent respectivement 4 cartes r√©seaux, nous pourrions cr√©er quelque chose comme suit:

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ExempleConfig_W.svg'),
            dark: useBaseUrl('/img/Virtu/ExempleConfig_D.svg'),
        }}
    />
</div>

:::caution
Cette configuration devrait √™tre cr√©√© et r√©p√©t√© dans chaque noeud ESXi manuellement! Voici ce qui doit √™tre identique dans chaque noeud:

- Noms des *vSwitch*
- Noms des groupes de ports
- Num√©ros des *VLANs*
- MTU
- Politiques de s√©curit√© et de *teaming*
- Assignation des *vmnic* aux *uplinks*

:::

### Les vSwitches distribu√©es (VDS)

Un *Virtual Distributed Switch* est un commutateur virtuel g√©r√© de mani√®re centralis√©e par *vCenter* et qui s'√©tend sur plusieurs h√¥tes ESXi. Voici les diff√©rences fondamentales entre les VSS et les VDS

|Aspect|vSwitch Standard|vSwitch Distributed|
|------|----------------|-------------------|
|**Gestion**|Locale √† chaque noeud|Centralis√©e dans vCenter|
|**Configuration**|Sur chaque ESXi|Une seule fois pour tous les h√¥tes|
|**Fonctionnalit√©s**|Basiques|Avanc√©es|
|**Licence**|Gratuit|N√©cessite licence Enterprise Plus|
|**Compl√©xit√©**|Simple|Plus complexe|

#### Architecture du vDS

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/VDS_W.svg'),
            dark: useBaseUrl('/img/Virtu/VDS_D.svg'),
        }}
    />
</div>

##### Composants d'un vDS

1. vDS : Configuration centralis√©e
    - D√©finit les param√®tres du switch distribu√©
    - G√®re les *distributed port groups*
    - Pousse la configuration vers les noeuds ESXi
    - Point unique de gestion

2. *Host Proxy Switch*
    - Agent local qui re√ßoit et applique la configuration du vDS
    - Fait le lien entre le vDS et les *vmnic*
    - Invisible dans l'interface, fonctionne en arri√®re-plan

3. *Distributed Port Groups*
    - D√©finis une seule fois dans vCenter
    - Automatiquement disponibles sur tous les h√¥tes membres du vDS
    - Configuration coh√©rente garantie (sans erreur)

#### Avantages du vDS

|Avantages|D√©tails|
|---------|-------|
|Coh√©rence|- Configuration identique sur tous les h√¥tes<br/>- Pas de risque d'erreur de configuration manuelle<br/>-Changement une fois = appliqu√© partout|
|Fonctionnalit√©s avanc√©es|- Netflow<br/>- Port mirroring<br/>- Private VLANs<br/>- LACP<br/>- Plusieurs autres|
|Mobilit√© simplifi√©e|Les *VMs* conservent leur configuration lorsqu'elles sont migr√©es.|
|Statistiques|Statistiques, monitoring et d√©pannage facilit√© avec vCenter.|

#### Configuration d'un vDS

La cr√©ation et la configuration d'un vDS doit absolument se faire via vCenter.

##### Cr√©ation

Au niveau du *Datacenter* :<br/>
Menu `Action` ‚Üí `Distributed Switch` ‚Üí `New Distributed Switch`

![NewVDS](../Assets/20/NouveauVDS.png)

Octroyez un nom √† votre nouveau *switch distribu√©* et s√©lectionnez la version de celui-ci. √Ä moins de travailler avec des versions plus anciennes d'**ESXi**, je vous recommande d'utiliser la version la plus r√©cente pour b√©n√©ficier des derni√®res fonctionnalit√©s.

![ConfigVDS](../Assets/20/ConfigVDS.png)

√Ä l'√©tape suivante, vous devrez param√©trer les options suivantes:

- **Compatibilit√© des d√©chargements r√©seau:**<br/>
Ce param√®tre concerne certaines cartes r√©seaux sp√©cialis√©s sur lesquelles on retrouve un processeur qui peuvent ex√©cuter des fonctions r√©seau avanc√©es. **Dans 99% des cas, l'option `aucun` est la bonne.**

- **Nombre de liaisons montantes:**<br/>
D√©finit combien de *vmnic* chaque h√¥te pourra connecter au vDS. Ce param√®tre peut √™tre modifi√© plus tard si n√©cessaire. G√©n√©ralement on recommande deux *vmnic* ou quatre pour les environnements critiques.

- **Network I/O Control:**<br/>
Cette option permet de prioriser et garantir la bande passante par type de trafic. C'est un √©l√©ment essentiel √† activer dans un environnement de production.

- **Groupe de ports par d√©faut:**<br/>
Cette option permet tout simplement de cr√©er un groupe de ports par d√©faut. Ce sera √† vous de voir, selon le contexte, si cette option est pertinente pour vous.

![ConfigVDS2](../Assets/20/ConfigVDS2.png)

Une fois cr√©√©, le **vDS** sera accessible dans le volet de gauche, sous la section r√©seau:

![AccesVDS](../Assets/20/AccesVDS.png)

Il nous faudra configurer les noeuds qui auront acc√®s √† ce **vDS**. Pour ce faire, cliquez sur le menu `actions` ‚Üí `Ajouter et g√©rez des h√¥tes...`

![AjouterHostVDS](../Assets/20/AjouterHostVDS.png)

Une fois que vous aurez s√©lectionn√© les noeuds qui auront acc√®s au **vDS**, vous serez amen√© √† s√©lectionner les *vmnic* qui devront √™tre attitr√©s √† ce **vDS**. Cette s√©lection peut se faire diff√©remment pour chaque h√¥te ou de mani√®re identique.

![vDSAdapteurs](../Assets/20/vDSAdapteurs.png)

L'√âtape suivante consiste √† associer les ports **VMKernel** <u>au besoin.</u> Si vous n'avez pas l'intention d'utiliser ce *switch* pour des services r√©serv√©s √† ESXi ou vSphere, il est inutile d'y associer des ports **VMKernel**.

Finalement, vous aurez la possibilit√© de migrer des √©l√©ments vers ce **vDS** au besoin.

Une fois le **vDS** en place, vous pourrez le retrouver dans les commutateurs virtuels de chaque h√¥te ESXi concern√©. Vous pourrez √©galement y brancher des machines virtuelles. N'oubliez pas, si vous d√©sirez utiliser diff√©rents groupes de ports, vous devrez cr√©er ceux-ci dans **vCenter.**

## Les composants de la couche virtualis√©e

### Les vnic (cartes r√©seau virtuelles)

Une *vNIC* est une carte r√©seau virtuelle assign√©e √† une machine virtuelle. Du point de vue de la *VM*, c'est une carte r√©seau normale (comme une Intel E1000 ou une VMXNET3).

**Caract√©ristiques:**

- Chaque VM peut avoir plusieurs *vNIC* (g√©n√©ralement 1 √† 4).
- Chaque *vNIC* a sa propre MAC.
- Diff√©rents types disponibles selon les besoins de performance
