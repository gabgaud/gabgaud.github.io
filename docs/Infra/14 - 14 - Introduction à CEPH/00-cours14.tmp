import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 14

# Introduction à CEPH dans Proxmox

Pendant longtemps, le stockage informatique reposait sur des systèmes centralisés et monolithiques : les fameux **SAN** (*Storage Area Network*) ou **NAS** (*Network Attached Storage*). Cette façon de procéder, quoique longtemps exploitée, possède des inconvénients majeurs:

- **Coût élevé**: Matériel propriétaire et licences onéreuses.
- **Point de défaillance unique**: Si le contrôleur tombe en panne, tout le stockage devient inaccessible.
- **Manque de flexibilité**: Difficile et coûteux à faire évoluer (scalabilité).

## L'arrivée du SDS et de CEPH

Le **stockage défini par logiciel (*Software-Defined Storage*)** change complètement la donne. L'idée est de séparer le logiciel du matériel. On peut désormais utiliser des serveurs standards avec des disques durs classiques pour créer un système de stockage puissant, flexible et résilient.

Ainsi est né **CEPH**.

**Qu'est-ce que CEPH ?** CEPH est un système de stockage distribué, open-source (🥳), qui n'a **aucun point de défaillance unique**. Il est conçu pour être massivement scalable (de quelques serveurs à des milliers) et auto-réparateur.

### Architecture et composants ⚙️

CEPH peut sembler complexe aux premiers abords. C'est pourquoi il est nécessaire d'en saisir les éléments qui le compose. Analysons donc ses composants fondamentaux:

1. **OSD (*Object Storage Daemon*)**: C'est le « muscle » de CEPH. Plus techniquement, un OSD est un processus qui tourne sur un serveur et qui est responsable d'un disque de stockage. C'est lui qui stocke les données, gère la réplication, la récupération et l'équilibrage des données avec les autres OSDs. **En bref : 1 disque = 1 OSD.**

2. **MON (*Monitor*)**: C'est le « cerveau » du *cluster*. Les moniteurs maintiennent la carte du cluster (la «CRUSH map», on y revient plus bas). Ils savent quels OSDs sont actifs, où les données devraient se trouver, et ils gèrent l'état général du *cluster*. Pour éviter d'être un point de défaillance, on déploie toujours un nombre impair de moniteurs (typiquement 3 ou 5) qui se surveillent mutuellement.

3. **MGR (*Manager*)**: C'est le « gestionnaire » du *cluster*. Le MGR collecte des statistiques détaillées sur le fonctionnement du cluster (performance, capacité, etc.) et expose ces informations, notamment pour le tableau de bord de Proxmox.

4. **RADOS (*Reliable Autonomic Distributed Object Store*)**: C'est la couche fondamentale de CEPH. Tout ce qui est stocké dans CEPH, que ce soit un disque de VM, un fichier ou peu importe, est au final un **objet** stocké dans le *cluster* RADOS. C'est le service fourni par les OSDs, les MONs et les MGRs.

### Le secret de CEPH : L'algorithme CRUSH 🤐

Comment CEPH sait-il où placer une donnée sans avoir une base de données centralisée qui deviendrait un goulot d'étranglement ? Grâce à **CRUSH (*Controlled Replication Under Scalable Hashing*)**.

CRUSH est un algorithme qui calcule *dynamiquement* où une donnée doit être écrite et lue. Il n'a pas besoin de consulter une table de métadonnées.

**Exemple:**

1. Vous voulez stocker une image de VM (qui est un « objet » pour CEPH).
2. CEPH prend l'identifiant de cet objet et le « hashe ».
3. Il utilise ensuite cet identifiant hashé et la carte CRUSH (qui décrit la hiérarchie de vos serveurs, racks, etc.) pour calculer sur quel OSD (et ses répliques) l'objet doit être stocké.  

**L'avantage majeur:** N'importe quel client (comme un hyperviseur Proxmox) peut calculer de manière autonome où se trouve une donnée. C'est ce qui rend CEPH si rapide et si scalable. La carte CRUSH permet aussi de définir des règles de placement intelligentes (ex: je veux que les copies de mes données ne soient jamais dans le même rack physique).

## Ceph & Proxmox ; Synergie parfaite 🤹

Proxmox VE intègre Ceph de manière native, ce qui simplifie énormément son déploiement et sa gestion. Mais outre son intégration, pourquoi devriez-vous l'utiliser ?

- **Hyperconvergence:** Vos serveurs Proxmox sont à la fois des serveurs de calcul (hyperviseurs) et des serveurs de stockage. Plus besoin d'un SAN.

- **Haute-Disponibilité:** Si un noeud Proxmox tombe en panne, les *VMs* qui y tournaient peuvent être redémarrées automatiquement sur un autre noeud car leur disque est stocké sur le *cluster* Ceph, qui est toujours accessible.

- **Migration en direct:** Vous pouvez déplacer une *VM* d'un noeud à un autre sans aucune interruption de service.

- **Scalabilité facile:** Besoin de plus de stockage ou de performance ? Il suffit d'ajouter un nouveau disque (OSD) ou un nouveau serveur au cluster.

### Les types de stockage Ceph dans Proxmox 💾

Proxmox utilise Ceph principalement via deux types de stockage:

1. **RDB (RADOS Block Device) :** C'est le mode le plus courant. Ceph présente un « périphérique bloc » à la *VM*. C'est très performant et idéal pour les disques des *VMs* et des conteneurs.

2. **CephFS (Ceph File System):** C'est un système de fichiers distribué compatible POSIX. Utile pour du stockage partagé accessible par plusieurs *VMs* ou conteneurs en même temps.

### Déploiement et gestion via Proxmox

