import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 14

# Introduction Ã  CEPH dans Proxmox

Pendant longtemps, le stockage informatique reposait sur des systÃ¨mes centralisÃ©s et monolithiques : les fameux **SAN** (*Storage Area Network*) ou **NAS** (*Network Attached Storage*). Cette faÃ§on de procÃ©der, quoique longtemps exploitÃ©e, possÃ¨de des inconvÃ©nients majeurs:

- **CoÃ»t Ã©levÃ©**: MatÃ©riel propriÃ©taire et licences onÃ©reuses.
- **Point de dÃ©faillance unique**: Si le contrÃ´leur tombe en panne, tout le stockage devient inaccessible.
- **Manque de flexibilitÃ©**: Difficile et coÃ»teux Ã  faire Ã©voluer (scalabilitÃ©).

## L'arrivÃ©e du SDS et de CEPH

Le **stockage dÃ©fini par logiciel (*Software-Defined Storage*)** change complÃ¨tement la donne. L'idÃ©e est de sÃ©parer le logiciel du matÃ©riel. On peut dÃ©sormais utiliser des serveurs standards avec des disques durs classiques pour crÃ©er un systÃ¨me de stockage puissant, flexible et rÃ©silient.

Ainsi est nÃ© **CEPH**.

**Qu'est-ce que CEPH ?** CEPH est un systÃ¨me de stockage distribuÃ©, open-source (ğŸ¥³), qui n'a **aucun point de dÃ©faillance unique**. Il est conÃ§u pour Ãªtre massivement scalable (de quelques serveurs Ã  des milliers) et auto-rÃ©parateur.

### Architecture et composants âš™ï¸

CEPH peut sembler complexe aux premiers abords. C'est pourquoi il est nÃ©cessaire d'en saisir les Ã©lÃ©ments qui le compose. Analysons donc ses composants fondamentaux:

1. **OSD (*Object Storage Daemon*)**: C'est le Â« muscle Â» de CEPH. Plus techniquement, un OSD est un processus qui tourne sur un serveur et qui est responsable d'un disque de stockage. C'est lui qui stocke les donnÃ©es, gÃ¨re la rÃ©plication, la rÃ©cupÃ©ration et l'Ã©quilibrage des donnÃ©es avec les autres OSDs. **En bref : 1 disque = 1 OSD.**

2. **MON (*Monitor*)**: C'est le Â« cerveau Â» du *cluster*. Les moniteurs maintiennent la carte du cluster (la Â«CRUSH mapÂ», on y revient plus bas). Ils savent quels OSDs sont actifs, oÃ¹ les donnÃ©es devraient se trouver, et ils gÃ¨rent l'Ã©tat gÃ©nÃ©ral du *cluster*. Pour Ã©viter d'Ãªtre un point de dÃ©faillance, on dÃ©ploie toujours un nombre impair de moniteurs (typiquement 3 ou 5) qui se surveillent mutuellement.

3. **MGR (*Manager*)**: C'est le Â« gestionnaire Â» du *cluster*. Le MGR collecte des statistiques dÃ©taillÃ©es sur le fonctionnement du cluster (performance, capacitÃ©, etc.) et expose ces informations, notamment pour le tableau de bord de Proxmox.

4. **RADOS (*Reliable Autonomic Distributed Object Store*)**: C'est la couche fondamentale de CEPH. Tout ce qui est stockÃ© dans CEPH, que ce soit un disque de VM, un fichier ou peu importe, est au final un **objet** stockÃ© dans le *cluster* RADOS. C'est le service fourni par les OSDs, les MONs et les MGRs.

### Le secret de CEPH : L'algorithme CRUSH ğŸ¤

Comment CEPH sait-il oÃ¹ placer une donnÃ©e sans avoir une base de donnÃ©es centralisÃ©e qui deviendrait un goulot d'Ã©tranglement ? GrÃ¢ce Ã  **CRUSH (*Controlled Replication Under Scalable Hashing*)**.

CRUSH est un algorithme qui calcule *dynamiquement* oÃ¹ une donnÃ©e doit Ãªtre Ã©crite et lue. Il n'a pas besoin de consulter une table de mÃ©tadonnÃ©es.

**Exemple:**

1. Vous voulez stocker une image de VM (qui est un Â« objet Â» pour CEPH).
2. CEPH prend l'identifiant de cet objet et le Â« hashe Â».
3. Il utilise ensuite cet identifiant hashÃ© et la carte CRUSH (qui dÃ©crit la hiÃ©rarchie de vos serveurs, racks, etc.) pour calculer sur quel OSD (et ses rÃ©pliques) l'objet doit Ãªtre stockÃ©.  

**L'avantage majeur:** N'importe quel client (comme un hyperviseur Proxmox) peut calculer de maniÃ¨re autonome oÃ¹ se trouve une donnÃ©e. C'est ce qui rend CEPH si rapide et si scalable. La carte CRUSH permet aussi de dÃ©finir des rÃ¨gles de placement intelligentes (ex: je veux que les copies de mes donnÃ©es ne soient jamais dans le mÃªme rack physique).

## Ceph & Proxmox ; Synergie parfaite ğŸ¤¹

Proxmox VE intÃ¨gre Ceph de maniÃ¨re native, ce qui simplifie Ã©normÃ©ment son dÃ©ploiement et sa gestion. Mais outre son intÃ©gration, pourquoi devriez-vous l'utiliser ?

- **Hyperconvergence:** Vos serveurs Proxmox sont Ã  la fois des serveurs de calcul (hyperviseurs) et des serveurs de stockage. Plus besoin d'un SAN.

- **Haute-DisponibilitÃ©:** Si un noeud Proxmox tombe en panne, les *VMs* qui y tournaient peuvent Ãªtre redÃ©marrÃ©es automatiquement sur un autre noeud car leur disque est stockÃ© sur le *cluster* Ceph, qui est toujours accessible.

- **Migration en direct:** Vous pouvez dÃ©placer une *VM* d'un noeud Ã  un autre sans aucune interruption de service.

- **ScalabilitÃ© facile:** Besoin de plus de stockage ou de performance ? Il suffit d'ajouter un nouveau disque (OSD) ou un nouveau serveur au cluster.

### Les types de stockage Ceph dans Proxmox ğŸ’¾

Proxmox utilise Ceph principalement via deux types de stockage:

1. **RDB (RADOS Block Device) :** C'est le mode le plus courant. Ceph prÃ©sente un Â« pÃ©riphÃ©rique bloc Â» Ã  la *VM*. C'est trÃ¨s performant et idÃ©al pour les disques des *VMs* et des conteneurs.

2. **CephFS (Ceph File System):** C'est un systÃ¨me de fichiers distribuÃ© compatible POSIX. Utile pour du stockage partagÃ© accessible par plusieurs *VMs* ou conteneurs en mÃªme temps.

### DÃ©ploiement et gestion via Proxmox

