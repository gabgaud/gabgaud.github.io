import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 14

# Introduction Ã  CEPH dans Proxmox

Pendant longtemps, le stockage informatique reposait sur des systÃ¨mes centralisÃ©s et monolithiques : les fameux **SAN** (*Storage Area Network*) ou **NAS** (*Network Attached Storage*). Cette faÃ§on de procÃ©der, quoique longtemps exploitÃ©e, possÃ¨de des inconvÃ©nients majeurs:

- **CoÃ»t Ã©levÃ©**: MatÃ©riel propriÃ©taire et licences onÃ©reuses.
- **Point de dÃ©faillance unique**: Si le contrÃ´leur tombe en panne, tout le stockage devient inaccessible.
- **Manque de flexibilitÃ©**: Difficile et coÃ»teux Ã  faire Ã©voluer (scalabilitÃ©).

## L'arrivÃ©e du SDS et de CEPH

Le **stockage dÃ©fini par logiciel (*Software-Defined Storage*)** change complÃ¨tement la donne. L'idÃ©e est de sÃ©parer le logiciel du matÃ©riel. On peut dÃ©sormais utiliser des serveurs standards avec des disques durs classiques pour crÃ©er un systÃ¨me de stockage puissant, flexible et rÃ©silient.

Ainsi est nÃ© **CEPH**.

**Qu'est-ce que CEPH ?** CEPH est un systÃ¨me de stockage distribuÃ©, open-source (ğŸ¥³), qui n'a **aucun point de dÃ©faillance unique**. Il est conÃ§u pour Ãªtre massivement scalable (de quelques serveurs Ã  des milliers) et auto-rÃ©parateur.

### Architecture et composants âš™ï¸

CEPH peut sembler complexe aux premiers abords. C'est pourquoi il est nÃ©cessaire d'en saisir les Ã©lÃ©ments qui le compose. Analysons donc ses composants fondamentaux:

1. **OSD (*Object Storage Daemon*)**: C'est le Â« muscle Â» de CEPH. Plus techniquement, un OSD est un processus qui tourne sur un serveur et qui est responsable d'un disque de stockage. C'est lui qui stocke les donnÃ©es, gÃ¨re la rÃ©plication, la rÃ©cupÃ©ration et l'Ã©quilibrage des donnÃ©es avec les autres OSDs. **En bref : 1 disque = 1 OSD.**

2. **MON (*Monitor*)**: C'est le Â« cerveau Â» du *cluster*. Les moniteurs maintiennent la carte du cluster (la Â«CRUSH mapÂ», on y revient plus bas). Ils savent quels OSDs sont actifs, oÃ¹ les donnÃ©es devraient se trouver, et ils gÃ¨rent l'Ã©tat gÃ©nÃ©ral du *cluster*. Pour Ã©viter d'Ãªtre un point de dÃ©faillance, on dÃ©ploie toujours un nombre impair de moniteurs (typiquement 3 ou 5) qui se surveillent mutuellement.

3. **MGR (*Manager*)**: C'est le Â« gestionnaire Â» du *cluster*. Le MGR collecte des statistiques dÃ©taillÃ©es sur le fonctionnement du cluster (performance, capacitÃ©, etc.) et expose ces informations, notamment pour le tableau de bord de Proxmox.

4. **RADOS (*Reliable Autonomic Distributed Object Store*)**: C'est la couche fondamentale de CEPH. Tout ce qui est stockÃ© dans CEPH, que ce soit un disque de VM, un fichier ou peu importe, est au final un **objet** stockÃ© dans le *cluster* RADOS. C'est le service fourni par les OSDs, les MONs et les MGRs.

### Le secret de CEPH : L'algorithme CRUSH ğŸ¤

Comment CEPH sait-il oÃ¹ placer une donnÃ©e sans avoir une base de donnÃ©es centralisÃ©e qui deviendrait un goulot d'Ã©tranglement ? GrÃ¢ce Ã  **CRUSH (*Controlled Replication Under Scalable Hashing*)**.

CRUSH est un algorithme qui calcule *dynamiquement* oÃ¹ une donnÃ©e doit Ãªtre Ã©crite et lue. Il n'a pas besoin de consulter une table de mÃ©tadonnÃ©es.

**Exemple:**

1. Vous voulez stocker une image de VM (qui est un Â« objet Â» pour CEPH).
2. CEPH prend l'identifiant de cet objet et le Â« hashe Â».
3. Il utilise ensuite cet identifiant hashÃ© et la carte CRUSH (qui dÃ©crit la hiÃ©rarchie de vos serveurs, racks, etc.) pour calculer sur quel OSD (et ses rÃ©pliques) l'objet doit Ãªtre stockÃ©.  

**L'avantage majeur:** 