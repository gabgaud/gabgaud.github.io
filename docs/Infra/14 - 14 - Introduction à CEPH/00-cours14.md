import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 14

# Introduction √† CEPH dans Proxmox

Pendant longtemps, l'informatique a repos√© sur du stockage centralis√© : les **SAN** (*Storage Area Network*) et les **NAS** (*Network Attached Storage*). Cette approche souffre de trois d√©fauts majeurs :

1. **Co√ªt √©lev√©** : Mat√©riel propri√©taire et licences on√©reuses.
2. **Point de d√©faillance unique** : Si le serveur de stockage tombe, TOUT s'arr√™te.
3. **Manque de flexibilit√©** : Ajouter de la capacit√© ou de la performance est co√ªteux et complexe.

## L'arriv√©e du stockage d√©fini par logiciel (SDS)

L'id√©e nouvelle et puissante : **s√©parer le logiciel du mat√©riel**. Au lieu d'utiliser du mat√©riel propri√©taire co√ªteux, utilisez des serveurs standards avec des disques classiques, et laissez le logiciel g√©rer le stockage.

C'est l√† que **Ceph** intervient.

## Qu'est-ce que Ceph ? ü§î

Ceph est un syst√®me de stockage distribu√© et open-source qui r√©pond √† trois exigences fondamentales :

- **Aucun point de d√©faillance unique** : Tout est redondant
- **Massivement scalable** : De quelques serveurs √† des milliers
- **Auto-r√©parateur** : Se r√©pare lui-m√™me en cas de panne

**En pratique :** Ceph prend les disques de plusieurs serveurs standards et les fait fonctionner comme un seul √©norme syst√®me de stockage r√©silient.

## Les composants de Ceph

Ceph peut sembler complexe, mais comprendre ses composants simplifie tout. Il y a **<mark>quatre</mark>** briques essentielles :

### OSD (Object Storage Daemon) : Les muscles üí™

Un OSD est un processus qui g√®re **un disque de stockage**. C'est simple : 1 disque = 1 OSD.

**R√¥le de l'OSD :**
- Stocker les donn√©es
- R√©pliquer les donn√©es vers les autres OSDs
- Se r√©tablir apr√®s une panne
- R√©√©quilibrer les donn√©es si un OSD tombe

Les OSDs travaillent ensemble. Il n'y en a pas un qui commande les autres. Ils communiquent directement entre eux.

### MON (Monitor) : Le cerveau üß†

Les Monitors sont les gardiens de la carte du cluster. Ils maintiennent des informations critiques :

- Quels OSDs sont actifs ou en panne
- O√π les donn√©es doivent √™tre r√©pliqu√©es (la "CRUSH map")
- L'√©tat g√©n√©ral du cluster

**Pourquoi plusieurs Monitors ?** Parce qu'un Monitor unique serait un point de d√©faillance. On d√©ploie toujours un nombre **impair** de Monitors (3 ou 5) qui se surveillent mutuellement et √©tablissent un quorum (la majorit√© d√©cide)... √áa vous penser √† quoi ? üòâ

**Le quorum expliqu√© :**
- Avec 3 Monitors : Si 1 tombe, les 2 restants forment la majorit√©. Le cluster continue.
- Avec 2 Monitors : Aucun n'est majorit√©. Le cluster s'arr√™te.
- **Conclusion :** Vous avez besoin d'au minimum 3 n≈ìuds pour un cluster Ceph fonctionnel...tout comme Proxmox d'ailleurs.

### MGR (Manager) : Le gestionnaire ü•∏

Le Manager collecte des statistiques d√©taill√©es sur le cluster :
- Performance (IOPS, bande passante)
- Utilisation de la capacit√©
- √âtat des pools

Il fournit aussi le tableau de bord web que Proxmox utilise pour afficher l'√©tat de Ceph. C'est utile mais pas critique pour le fonctionnement.

### RADOS : La fondation üß±

RADOS signifie "Reliable Autonomic Distributed Object Store". C'est crucial de bien comprendre ce que c'est.

**Ce que RADOS n'est PAS :**
- Ce n'est pas un syst√®me de fichiers (pas de r√©pertoires, de permissions POSIX, etc.)
- Ce n'est pas un classement hi√©rarchique (pas de dossiers, pas de chemins)
- Ce n'est pas une base de donn√©es

**Ce que RADOS est :**
RADOS est un **syst√®me de stockage d'objets distribu√©s et autonome**. C'est une couche tr√®s basique qui sait faire trois choses seulement :

1. **Stocker des objets** : Chaque objet a un identifiant unique (une cl√©) et des donn√©es
2. **Les r√©pliquer** : Via CRUSH, les OSDs s'arrangent pour que chaque objet existe en plusieurs copies
3. **Se r√©parer** : Si un OSD tombe, RADOS redistribue automatiquement les donn√©es

**Exemple concret :**
Vous avez un disque de VM de 100 GB. Ceph le d√©coupe en 25 000 petits objets de 4 MB. Chaque objet a un identifiant unique comme "vm-100-disk-0-object-0001". RADOS stocke ces 25 000 objets sur les OSDs, les r√©plique 3 fois, et s'assure qu'ils sont tous l√†.

**La cl√© :** RADOS ne sait rien des fichiers, des r√©pertoires, des permissions. C'est un magasin de paires cl√©-valeur distribu√© et r√©silient. C'est tout.

**Couches au-dessus de RADOS :**
D'autres services de Ceph construisent des couches au-dessus de RADOS :

- **RBD (RADOS Block Device)** : Prend les objets RADOS et les pr√©sente comme un disque virtuel √† une VM
- **CephFS (Ceph File System)** : Prend les objets RADOS et construit un syst√®me de fichiers complet dessus

Donc la hi√©rarchie est :

```
Application (VM)
       ‚Üì
RBD (bloc) / CephFS (fichiers)
       ‚Üì
RADOS (couche de base : stockage d'objets distribu√©s)
       ‚Üì
OSDs (disques physiques qui font le travail r√©el)
```

**Pourquoi cette abstraction est g√©niale :**
RADOS g√®re la distribution, la r√©plication, la d√©tection de pannes. Les couches au-dessus (RBD, CephFS) n'ont pas besoin de se pr√©occuper de √ßa. Elles re√ßoivent juste un syst√®me de stockage fiable et r√©pliqu√©.

## Comment g√©rer autant d'objets ? ü§Ø

Un cluster Ceph peut contenir des millions, voire des milliards d'objets (les morceaux de 4 Mo de vos disques de VM). Si les Moniteurs (MONs) devaient suivre l'emplacement de r√©plication de chaque petit objet, ils seraient instantan√©ment surcharg√©s.

Ceph r√©sout ce probl√®me en deux √©tapes:

### Les *Placement Groups* (PGs) üì¶

Au lieu de g√©rer des millions d'objets, Ceph les regroupe dans des bo√Ætes logiques, appel√©s **Placement Groups (PGs).** √Ä titre de comparatif: imaginez si vous deviez suivre chaque vis d'une quincaillerie... mieux vaut les regrouper en paquet de quelques vis pour vous simplifier la vie.

Le PG devient l'unit√© de base pour la r√©plication, l'√©quilibrage et la r√©cup√©ration. Quand un disque tombe, Ceph d√©place les PGs qui s'y trouvaient, pas des millions de petits objets un par un.

**Comment √ßa marche ?** Ceph prend l'identifiant de l'objet, le "hashe" (transformation math√©matique), et d√©termine ainsi √† quel PG il appartient.

### L'algorithme CRUSH

Maintenant que nous avons des PGs (nos "bo√Ætes"), comment le syst√®me d√©cide-t-il sur quels disques (OSDs) les placer ?

C'est le r√¥le de **CRUSH** (Controlled Replication Under Scalable Hashing).

CRUSH est un algorithme qui **calcule dynamiquement** o√π un PG doit √™tre √©crit et lu. Il n'a **pas besoin de consulter une table de m√©tadonn√©es** (ce qui √©vite le goulot d'√©tranglement).

CRUSH utilise simplement deux choses :

    1. L'identifiant du Placement Group.
    2. La CRUSH map (la carte de votre infrastructure).

### Le processus de placement complet

Le flux complet est donc:

`Objet` ‚Üí `PG` ‚Üí `OSDs`

    1. Votre VM √©crit une donn√©e. Ceph identifie l'**objet** correspondant (ex: un bloc de 4 Mo).
    2. Un hash de l'ID de l'objet d√©termine son **Placement Group (PG)**
    3. L'algorithme **CRUSH** prend l'ID de ce PG et la CRUSH map pour calculer sur quels **OSDs** ce PG (et ses 3 copies) doit √™tre plac√©.

## La r√©plication pour plus de r√©silience

### Comment fonctionne la r√©plication

Quand vous √©crivez un fichier de 100 MB sur Ceph :

1.  **D√©coupage :** Le fichier se divise en petits objets de 4 MB.
2.  **Distribution intelligente :** Chaque objet est assign√© √† un PG. Ce PG est ensuite √©crit sur 3 disques diff√©rents (sur des serveurs diff√©rents gr√¢ce √† CRUSH).
3.  **Confirmation :** L'√©criture n'est valid√©e que quand les 3 copies existent.

**R√©sultat :** Vous pouvez perdre 2 disques (ou 2 serveurs entiers, selon la r√®gle CRUSH) sans rien perdre. La 3√®me copie subsiste.

### L'√©quilibre des copies ‚öñÔ∏è

Vous choisissez le nombre de r√©pliques (`size`) selon vos besoins :

  - **R√©plication √ó 2 (`size=2`) :**
      - Espace utilis√© : 50% de surcharge.
      - R√©silience : Tol√®re 1 disque/n≈ìud en panne.
      - Usage : D√©veloppement, donn√©es non critiques.
  - **R√©plication √ó 3 (`size=3`) :**
      - Espace utilis√© : 66% de surcharge.
      - R√©silience : Tol√®re 2 disques/n≈ìuds en panne.
      - Usage : **Production, donn√©es critiques. C'est la valeur par d√©faut recommand√©e.**
  - **R√©plication √ó 4 (`size=4`) :**
      - Espace utilis√© : 75% de surcharge.
      - R√©silience : Tol√®re 3 disques/n≈ìuds en panne.
      - Usage : Donn√©es ultra-critiques uniquement (rare).

:::caution[La r√©plication n'est pas une sauvegarde]
Attention : Si vous supprimez un fichier, il dispara√Æt des 3 copies simultan√©ment. Ceph prot√®ge contre les pannes **mat√©rielles**, PAS contre les erreurs **humaines** ou les **malwares**.

Vous devez **TOUJOURS** avoir une strat√©gie de sauvegarde externe, ind√©pendante de Ceph.
:::

### L'alternative √† la r√©plication : L'Erasure Coding (EC) 

La r√©plication 3x est simple et rapide, mais elle est co√ªteuse : 1 To de donn√©es utilisables consomme 3 To d'espace brut (66% de surcharge).

L'**Erasure Coding (EC)** fonctionne comme un **RAID 5 ou 6 distribu√©**.

  - **Principe :** Au lieu de copier la donn√©e 3 fois, Ceph la d√©coupe en "morceaux de donn√©es" (Data Chunks) et y ajoute des "morceaux de parit√©" (Parity Chunks).
  - **Exemple (k=4, m=2) :**
      - `k=4` : La donn√©e est coup√©e en 4 morceaux.
      - `m=2` : Ceph calcule 2 morceaux de parit√©.
      - Ces 6 morceaux sont √©crits sur 6 OSDs diff√©rents.
      - **R√©silience :** Le cluster peut perdre **2** OSDs (peu importe lesquels) et peut toujours reconstruire la donn√©e originale.
  - **Co√ªt en espace :** Pour 1 To de donn√©es, vous n'utilisez que 1.5 To d'espace brut (33% de surcharge, contre 66% pour la r√©plication).
  - **Inconv√©nient :** Plus lent en √©criture et en r√©cup√©ration (plus de calculs de parit√©).
  - **Usage id√©al :** Parfait pour un pool de **backups** ou d'archivage (via CephFS), o√π l'√©conomie d'espace est plus importante que la performance pure.

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/CEPH_W.svg'),
            dark: useBaseUrl('/img/Virtu/CEPH_D.svg'),
        }}
    />
</div>

## CEPH + Proxmox = Synergie ‚ôæÔ∏è

Proxmox int√®gre Ceph nativement, ce qui cr√©e une synergie puissante :

**1. Hyperconvergence**<br/>
Vos serveurs Proxmox sont √† la fois serveurs de calcul (ex√©cutent les VMs) **ET** serveurs de stockage (stockent les disques des VMs). Plus besoin de SAN s√©par√©. Les ressources mat√©rielles sont utilis√©es efficacement.

**2. Haute disponibilit√©**<br/>
Si un n≈ìud Proxmox tombe, les VMs qui y tournaient red√©marrent automatiquement sur un autre n≈ìud car leurs disques existent sur le cluster Ceph, qui est toujours accessible.

**3. Migration transparente**<br/>
Vous pouvez d√©placer une VM d'un n≈ìud √† un autre **SANS interruption de service**. Le disque est accessible depuis n'importe quel n≈ìud gr√¢ce √† Ceph.

**4. Scalabilit√© simple**<br/>
Besoin de plus de stockage ou de performance ? Ajoutez un disque (OSD) ou un serveur au cluster. Ceph r√©√©quilibre automatiquement.

### R√©seaux distincts üåê

Pour que tout cela fonctionne sans latence, une configuration r√©seau d√©di√©e est cruciale. Ceph utilise deux r√©seaux :

1.  **R√©seau Public (Public Network) :**

      - **Utilisation :** Trafic d'administration, communication des hyperviseurs avec les MONs, et trafic des VMs.
      - **Exemple :** Le r√©seau de gestion de Proxmox suffit amplement.

2.  **R√©seau Cluster (Cluster Network) :**

      - **Utilisation :** **Exclusivement** pour le trafic interne de Ceph. C'est l√† que transitent les r√©plications, le ¬´ healing ¬ª et les ¬´ heartbeats ¬ª des OSDs.
      - **Pourquoi ?** Ce trafic est massif et constant. Le s√©parer garantit que la r√©plication d'un disque ne va pas ralentir vos VMs.
      - **Requis :** Id√©alement 10 Gbps ou plus. (Vous avez bien lu...)
      - **Exemple :** Un r√©seau d√©di√©, sans passerelle.

### Types de stockage

Ceph propose plusieurs types de stockage. Proxmox en utilise deux principalement :

**1. RBD (RADOS Block Device)**

  * RBD pr√©sente un "p√©riph√©rique bloc" √† la VM, exactement comme un disque dur virtuel.
  * **Tr√®s performant.**
  * Id√©al pour les disques de VMs et conteneurs.
  * C'est celui que vous utiliserez dans 99% des cas.

**2. CephFS (Ceph File System)**

  * CephFS est un syst√®me de fichiers distribu√© compatible POSIX.
  * Utile pour du stockage partag√© (ISOs, backups).
  * Accessible par plusieurs VMs/conteneurs en m√™me temps.

## Performances et limitations 

### Ce qui ralentit Ceph

La r√©plication a un co√ªt. Pour √©crire 1 MB :

  - Vous devez √©crire 3 MB (3 copies).
  - Le r√©seau doit supporter ce trafic 3√ó.
  - Les disques doivent supporter cette charge.

C'est pourquoi la performance r√©elle est limit√©e par le composant le plus faible.

### La hi√©rarchie des performances

  - **NVMe PCIe 4.0 :** 500 000 IOPS, latence \< 0.1ms
  - **SSD SATA :** 80 000 IOPS, latence \~0.5ms
  - **HDD 7200rpm :** 200 IOPS, latence \~5ms
  - **R√©seau 10Gbps :** 1 250 MB/s
  - **R√©seau 1Gbps :** 125 MB/s

:::tip
Une cha√Æne n'est forte que de son maillon le plus faible.
Si votre r√©seau est 1Gbps et vos disques sont des SSD, la performance sera limit√©e par le r√©seau (125 MB/s au lieu de 550 MB/s possibles du SSD).
:::

## Point cl√©s √† retenir ‚úÖ

  - Ceph distribue les donn√©es sur plusieurs disques. Si un disque tombe, les donn√©es existent ailleurs.
  - La r√©plication se fait automatiquement. Via CRUSH, sans intervention manuelle.
  - CRUSH d√©cide o√π placer les donn√©es via un algorithme. Pas de serveur central d√©cisionnaire.
  - Rien n'est unique et critique. Monitors, Managers, disques - tout a des sauvegardes.
  - Le quorum des Monitors est essentiel. Vous avez besoin d'au minimum 3 n≈ìuds.
  - La performance d√©pend du r√©seau **ET** des disques. Pas juste l'un ou l'autre.
  - **La r√©plication n'est pas une sauvegarde.** Faites des backups externes.
  - Ceph est complexe, mais Proxmox le simplifie. L'interface web automatise les complexit√©s.

## Pi√®ges √† √©viter üö´

<span class="red-text">**Pi√®ge 1 : Confondre r√©plication et sauvegarde**<br/></span>
La r√©plication prot√®ge contre les pannes **mat√©rielles**. La sauvegarde prot√®ge contre les erreurs **humaines** et les **attaques**. Vous avez besoin des deux.

<span class="red-text">**Pi√®ge 2 : N√©gliger le r√©seau**<br/></span>
Ceph est tr√®s gourmand en bande passante. Un r√©seau 1 Gbps tue les performances, m√™me avec des SSD. **Le 10 Gbps est un pr√©requis** pour le r√©seau "cluster".

<span class="red-text">**Pi√®ge 3 : Moins de 3 n≈ìuds**<br/></span>
Ceph a besoin d'un quorum. Moins de 3 n≈ìuds = pas de quorum = cluster non fonctionnel en production.

<span class="red-text">**Pi√®ge 4 : Sous-estimer la RAM**<br/></span>
Chaque OSD consomme de la RAM (1-4 GB selon la charge). Avec 3 OSD par n≈ìud, c'est 6-12 GB juste pour Ceph. Planifiez g√©n√©reusement.

<span class="red-text">**Pi√®ge 5 : Utiliser du mat√©riel h√©t√©rog√®ne**<br/></span>
Un cluster Ceph fonctionne mieux quand tous les n≈ìuds sont identiques. M√©langer vieux serveurs et neufs cr√©e des d√©s√©quilibres de performance (le cluster ira √† la vitesse du plus lent).