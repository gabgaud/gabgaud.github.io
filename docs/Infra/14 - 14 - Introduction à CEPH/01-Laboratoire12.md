import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Laboratoire 12

* * *

## D√©ploiement d'un cluster hyperconverg√© Proxmox avec Ceph

## Pr√©alables ‚úÖ

- 1 Passerelle de type PfSense (vous pouvez utiliser un mod√®le)
- 3 hyperviseurs Proxmox avec les caract√©ristiques suivantes:
    - Virtualisation mat√©rielle activ√©e
    - 4 coeurs
    - 16 Go de m√©moire vive
    - 1 disque dur pour le syst√®me d'exploitation (Proxmox)
    - 2 disques durs de 500Go pour vos *OSDs*
    - 1 carte r√©seau reli√©e √† votre passerelle
    - 1 carte r√©seau interne sans passerelle (pour Ceph)

## Objectifs üéØ

- Installer et initialiser un cluster Ceph (Quincy) sur Proxmox.
- Cr√©er des *OSDs* √† partir de vos disques virtuels.
- Cr√©er un pool de stockage RBD pours les disques des *VMs*.
- Cr√©er un pool de stockage CephFS pour les ISOs et les sauvegardes.
- Valider le fonctionnement de l'hyperconvergence.

## Sch√©ma du laboratoire

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/Labo12_W.svg'),
            dark: useBaseUrl('/img/Virtu/Labo12_D.svg'),
        }}
    />
</div>

## √âtapes de r√©alisation üî¢

### 1. Mise en place de votre passerelle

Dans `LabInfo`, clonez le mod√®le `_PASSERELLE` dans votre r√©pertoire pour vous cr√©er un r√©seau derri√®re un NAT. Cela √©vitera les diff√©rentes probl√©matiques li√©es au fait que plusieurs √©tudiants ne renomment pas convenablement leur hyperviseur lors de l'installation.

### 2. D√©ploiement de vos hyperviseurs Proxmox

Comme je suis tr√®s gentil, je vous ai pr√©par√© un mod√®le Proxmox contenant d√©j√† plusieurs `ISOs` √† l'int√©rieur üôÇ. Vous pouvez utiliser ce mod√®le, mais il sera important de faire quelques manipulations suite au clonage. Clonez donc le mod√®le `_ProxmoxVE_ModeleGG` qui se trouve dans `LabInfo` ‚Üí `Mod√®les` ‚Üí `420-5V6-5S6`. Lors du clonage, **<mark>n'oubliez pas de modifier le mat√©riel de la machine (disques, cartes r√©seaux, etc.)</mark>** afin qu'il corresponde aux demandes dans la section pr√©alables.

#### 2.1 Script pour rendre chaque hyperviseur unique

Une fois que votre machine Proxmox aura bien d√©marr√©e, vous trouverez un script nomm√© `new_node.sh` dans votre r√©pertoire de travail. Ce script automatise les √©tapes que vous avez √† faire pour rendre votre machine unique aux yeux des autres serveurs Proxmox que vous clonerez √©galement. Ex√©cutez donc le script en suivant les √©tapes. 

**R√©p√©tez ces √©tapes jusqu'√† ce que vous ayez trois noeud parfaitement fonctionnels**

:::tip
Les *ISOs* de Ubuntu et Windows sont d√©j√† pr√©sents dans le mod√®le Proxmox. De rien üòâ.
:::

### 3. Cr√©ation du r√©seau d√©di√© pour Ceph

Dans chacun de vos hyperviseurs Proxmox, vous devrez cr√©er un nouveau *bridge* Linux. Ce *bridge* devra √™tre associ√© √† votre seconde carte r√©seau (voir sch√©ma ci-dessus). D√©finissez une adresse IP pour chaque hyperviseur manuellement.

Exemple:

- noeud 1 : 10.0.0.10
- noeud 2 : 10.0.0.20
- noeud 3 : 10.0.0.30

### 4. Clustering

Cr√©ez un cluster sur l'un des noeuds, et joignez-y les deux autres noeuds.

### 5. Mise en place du cluster Ceph

#### 5.1 Activation des d√©p√¥ts pour Ceph

Pour installer Ceph, il nous faut activer certains *repositories* dans Proxmox. **Pour chacun des noeuds**, dirigez-vous dans *Updates*, puis *Repositories*. De l√†, s√©lectionnez `Add`, puis cliquez sur `Ceph Quincy No-Subscription`. Finaliser l'ajout en cliquant de nouveau sur `Add`.

Assurez-vous que tout est en ordre en entrant la commande suivante dans le *shell* de chaque noeud:

```bash
apt update
```

#### 5.2 Installation de Ceph

Dans l'interface web de votre cluster Proxmox, cliquez sur un premieur et dirigez-vous dans **Ceph**. Imm√©diatement, Proxmox vous mentionnera que Ceph n'est pas install√© et vous proposera d'en faire l'installation. Cliquez donc sur *Install Ceph*.

Dans la fen√™tre d'installation, vous devrez choisir la version √† installer. Pour les environnements de tests ou de laboratoires, il est recommand√© d'installer la version `quincy`. C'est donc ce que nous utiliserons. S√©lectionnez donc `quincy (17.2)` et le *repository* `No-Subscription`.

Lancez l'installation.

Une fois les paquets install√©s, vous aurez acc√®s √† l'onglet *Configuration* o√π vous pourrez d√©finir les r√©seaux √† utiliser pour Ceph. Nous avons distinguer ces deux types de r√©seau dans la th√©orie, configurez-les donc conform√©ment √† ce dont nous avons discut√©.

**R√©p√©tez l'√©tape 5.2 pour chaque noeud**.

#### 5.3 Cr√©ation d'un pool Ceph

Un pool, c'est simplement un rassemblement d'*OSDs* auquel on applique une r√®gle de r√©plication. C'est donc lorsqu'on cr√©e un *pool* ceph qu'on peut d√©terminer si nous d√©sirons 2,3 ou m√™me 4 r√©plications distinctes.

Il est obligatoire de cr√©er au moins un *pool* pour utiliser Ceph. Dans un de vos noeuds, dirigez-vous dans le menu `Ceph`, puis s√©lectionnez `Pools`. Enfin, cliquez sur `Create`.

Nommez votre *pool* vm_disks tout simplement. Une taille 3 (3 copies de chaque *PGs*) est la norme. Enfin, laissez l'*autoscaler* de Proxmox d√©terminer la taille des *PGs*. Proxmox est assez efficace pour √ßa.

D√®s la cr√©ation de votre *pool*, vous le verrez apparaitre dans chaque noeud...sauf que le pool est vide actuellement...

#### 5.4 Cr√©ation des OSDs

**Souvenez-vous:**

**1 disque = 1 OSD**

Nous avons ajout√© 2 disques de 500Go dans chacun de nos noeuds Proxmox. Nous avons donc 6 *OSDs* √† cr√©er.<br/> **<u>Dans chacun des noeuds:</u>**

Dirigez-vous dans le menu `Ceph`, puis dans `OSD`. Dans cette fen√™tre, cliquez sur `Create: OSD`. Cr√©ez un OSD pour `/dev/sdb` et `/dev/sdc`.

### 6. Cr√©ation d'une machine virtuelle

Dans le noeud de votre choix, cr√©ez une machine virtuelle (linux, windows, peu importe) en utilisant votre pool Ceph comme stockage.<br/>

R√©pondez aux questions suivantes:

- O√π est stock√© le disque de votre machine virtuelle ?
- Combien de *downtime* il y a lors d'une migration √† chaud de votre machine virtuelle de test ?
- Pouvez-vous stocker des sauvegardes sur votre pool Ceph ?