import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Projet final - Phase 1

# Mise en place d'une infrastructure VMware compl√®te

:::danger[Travail individuel]
Le projet doit √™tre r√©alis√© individuellement. L'entraide est permise, cependant:
  - Vos captures d'√©cran doivent √™tre uniques et prises par vous.
  - Votre documentation et vos explications doivent √™tre votre composition (pas celle de l'IA ü•≤ ni celle d'un coll√®gue)
  - Le plagiat (IA, coll√®gue ou autre) entrainera syst√©matiquement une note de z√©ro.
:::

* * *

Il ne nous reste plus que quelques √©l√©ments √† voir avec les produits VMware dans le cadre de ce cours. Tout ce qui nous reste √† voir concerne et/ou tourne autour du *clustering*. Comme vous connaissez, en bonne partie, les avantages, les inconv√©nients et les raisons pour lesquelles nous mettons en place ces *clusters* (gr√¢ce √† la portion Proxmox du cours), ces derni√®res notions vous seront transmises sous la forme d'un projet synth√®se.

Un cluster vSphere, comme un cluster Proxmox, est un regroupement logique de plusieurs h√¥tes. Dans vSphere, les noeuds ESXi ainsi rassembl√©s g√©r√©s sous la forme d'une seule ressource par vCenter. Trois piliers caract√©risent le clustering VMware:

1. **vMotion (Migration √† chaud)**
    - D√©place une VM d'un h√¥te √† un autre sans interruption de service.
    - La VM reste allum√©e et accessible pendant le d√©placement.
    - Parfait pour la maintenance et/ou l'√©quilibrage de charge.

2. **vSphere HA (*High Availability*)**
    - Red√©marre automatiquement les VMs si un h√¥te tombe en panne.
    - Protection contre les d√©faillances mat√©rielles.
    - Temps d'arr√™t r√©duit au minimum.

3. **vSphere DRS (*Distributed Ressource Scheduler*)**
    - √âquilibre automatiquement la charge de travail
    - Utilise *vMotion* pour d√©placer les *VMs* selon les besoins
    - Optimise l'utilisation des ressources du *cluster*.

## Phase 1

La premi√®re √©tape du projet consiste √† mettre en place l'infrastructure virtuelle avec laquelle nous mettrons en place les diff√©rents services en lien avec le clustering. ‚ö†Ô∏è**Vous devez faire une nouvelle infrastruscture.**‚ö†Ô∏è

- La nouvelle infrastructure n'est pas install√©e de la m√™me fa√ßon.
- Nous utiliserons des vSwitchs standards et distribu√©s.
- Cela vous permettra √©galement de r√©viser certaines notions.


## Sch√©ma de la phase 1

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/Projet-Phase1_W.svg'),
            dark: useBaseUrl('/img/Virtu/Projet-Phase1_D.svg'),
        }}
    />
</div>

### 1. Passerelle et Serveur DNS

Nous allons devoir se cr√©er une passerelle et un serveur DNS pour r√©pondre √† nos besoins. Pour la passerelle, vous pouvez utiliser le mod√®le `MODELE_pfSense (Sans DHCP)` disponible √† la racine du dossier Mod√®les dans LabInfo. Configurez une premi√®re carte sur `Acc√®s Internet` et l'autre sur l'un de vos r√©seaux priv√©s. Prenez note qu'en ce qui me concerne, j'utiliserai le r√©seau priv√© 192.168.21.0/24 pour mes d√©monstrations. Vous pouvez suivre mon plan d'adressage ou utiliser le v√¥tre.

:::danger[Choix du nom de domaine]
Choisissez-vous un nom de domaine personnalis√©. Je vous recommande *prenom.lan*.

**NE PRENEZ PAS CEMTI.CA**<br/>
**NE PRENEZ PAS LABINFO.CEMTI.CA**<br/>
**NE PRENEZ PAS QUELQUECHOSE.CEMTI.CA**<br/>
**NE PRENEZ PAS LABINFO.QUELQUECHOSE**<br/>

Vous aurez des conflits avec les noms de domaine du C√©gep.
:::

Pour le serveur DNS, je vous laisse le soin de choisir le syst√®me d'exploitation de votre choix. N√©anmoins, assurez-vous de cr√©er une zone de recherche direct et une autre invers√©e dans lesquelles vous mettrez en place les enregistrements suivants:

**Zone de recherche direct:**
|Nom|Type|Destination|Description|
|---|----|-----------|-----------|
|@|NS|ns1.domaine.lan.|Enregistrement NS pour le serveur de noms|
|@|A|192.168.21.5|Enregistrement A au nom de la zone pointant vers l'IP de mon serveur DNS|
|ns1|A|192.168.21.5|Traduction du nom pour ns1|
|esx1|A|192.168.21.11|Enregistrement A pour ESXi-1|
|esx2|A|192.168.21.12|Enregistrement A pour ESXi-2|
|esx3|A|192.168.21.13|Enregistrement A pour ESXi-3|
|vcenter|A|192.168.21.21|Enregistrement A pour vCenter|

**Zone de recherche inverse:**
|Nom|Type|Destination|Description|
|---|----|-----------|-----------|
|@|NS|ns1.domaine.lan.|Enregistrement NS pour le serveur de noms|
|5|PTR|ns1.domaine.lan.|Enregistrement PTR pour ns1|
|11|PTR|esx1.domaine.lan|Enregistrement PTR pour ESXi-1|
|12|PTR|esx2.domaine.lan|Enregistrement PTR pour ESXi-2|
|13|PTR|esx3.domaine.lan|Enregistrement PTR pour ESXi-3|
|21|PTR|vcenter.domaine.lan|Enregistrement PTR pour vCenter|

### 2. D√©ploiement des ESXi

J'ai cr√©√© un mod√®le d'hyperviseur ESXi pour vous permettre de gagner du temps. Vous pouvez √©videmment l'utiliser, mais vous devrez **<span class="red-text">obligatoirement l'adapter. Sans quoi, vous aurez diff√©rentes problmatiques.</span>** Si vous pr√©f√©rez, vous pouvez aussi installer individuellement les trois hyperviseurs, c'est √©galement une possibilit√© viable. **<span class="red-text">Si vous clonez mon mod√®le, ne faites qu'un hyperviseur √† la fois!</span>**

#### 2.1 Clone et importation

    - Dirigez-vous dans le dossier `Mod√®les` ‚Üí `420-5V6` et cloner le mod√®le **MODELE_ESXi_8.0.3_Golden** vers une machine virtuelle.
    - Personnaliser le mat√©riel de la machine virtuelle:
        - Ajustez la premi√®re carte r√©seau pour brancher celle-ci sur le r√©seau priv√© que d√©ssert votre passerelle.
        - Ajoutez une deuxi√®me interface r√©seau que vous relierez sur un autre de vos r√©seaux priv√©s. ( iSCSI )
        - Ajoutez une troisi√®me interface r√©seau que vous relierez sur un autre de vos r√©seaux priv√©s (Diff√©rent des interfaces pr√©c√©dentes). Cette troisi√®me carte n'est pas repr√©sent√©e dans le sch√©ma pour l'instant. Nous nous en occuperons lors de la prochaine phase du projet.
        - Ajoutez un second disque dur de 40Go en vous assurant de le <u>relier au contr√¥leur SATA.</u>
        - Ajoutez un troisi√®me disque dur de 250Go en vous assurant de le <u>relier au contr√¥leur SATA.</u>
        - Supprimez le contr√¥leur SCSI qui aura √©t√© ajout√© automatiquement.
        - <mark>**Sur le noeud h√©bergeant vCenter seulement:**</mark>
            - Augmentez la m√©moire vive √† 24Go
        - Terminez.
    - D√©marrez la machine virtuelle que vous venez de d√©ployer.

#### 2.2 Configuration de l'hyperviseur clon√©

    - Dans la console (√©cran jaune et gris), ajustez les param√®tres IP et DNS conform√©ment √† ce que vous avez inscrit dans votre serveur DNS.
    - Un peu comme avec Active Directory et Windows, il faut nous assurer de l'unicit√© de chacun des hyperviseurs. Comme nous clonons une machine, il faut donc faire quelques modifications sur l'hyperviseur pour s'assurer qu'il poss√®de un identifiant unique:
        - Toujours dans la console, dirigez-vous dans `Troubleshooting options` et activez `ESXI Shell`
          ![ESXIShell](../Assets/Projet/ESXIShell.png)
        - Retournez dans le menu principal de la console et appuyez sur les touches <kbd>alt</kbd> + <kbd>f1</kbd>. Un shell devrait s'ouvrir vous permettant de vous identifier √† l'aide de votre compte root.
          ![LoginESXShell](../Assets/Projet/LoginESXShell.png)
        - √âvidemment, identifiez-vous.
        - Une fois identifi√© dans le shell, nous devrons effectuer quelques modifications importantes:
            - √âditez le fichier `/etc/vmware/esx.conf` et supprimez la ligne de texte commen√ßant par `/system/uuid`. Cela forcera ESXi a cr√©er un nouvel identifiant syst√®me. Ne cherchez pas Nano, il n'existe pas sous vmware. Vous devez utiliser **vi**, le papa de **vim** üòà. Assurez-vous de bien enregistrer vos modifications.
            - Supprimez les certificats li√©s √† l'identification de la machine. Il y a deux fichiers √† supprimer : `/etc/vmware/ssl/rui.crt` et `/etc/vmware/ssl/rui.key`.
            - Finalement, il nous faut recr√©er le VMkernel Port 0. Cette interface virtuelle poss√®de, elle aussi, une adresse MAC. Comme ESX a √©t√© clon√©, l'adresse MAC doit √™tre reg√©n√©r√©, sans quoi vous aurez beaucoup d'instabilit√© sur votre r√©seau:
            ```bash
            #Suppression du VMkernel Port 0
            esxcfg-vmknic -d Management\ Network

            #Cr√©ation d'un nouveau VMKernel Port 0
            esxcfg-vmknic -a -i w.x.y.z -n 255.255.255.0 Management\ Network
            ```
        - **Red√©marrez ESXI**

#### 2.3 Ajustement du r√©seau de gestion

    - Votre hyperviseur est presque pr√™t, il ne reste qu'√† d√©sactiver certains √©l√©ments de s√©curit√© sur le vSwitch0:

        - Dirigez-vous dans l'interface web de votre nouvel hyperviseur et √©ditez les param√®tres de s√©curit√© du vSwitch0 comme suit:

          ![vSwitchSec](../Assets/Projet/vSwitchSec.png)
          
**<mark>R√©p√©tez les √©tapes 2.1, 2.2, et 2.3 pour chaque hyperviseur clon√©!</mark>**

### 3. D√©ploiement du client

J'ai cr√©√© un mod√®le de client que vous devez utiliser <span class="red-text">**obligatoirement**</span>. La raison est bien simple, vous devrez utiliser un script qui se trouve √† l'int√©rieur du mod√®le √† utiliser.

Dirigez-vous donc dans le dossier `Mod√®les` ‚Üí `420-5V6` et cloner le mod√®le **Client5V6_ScriptvCenter**. Ajustez sa carte r√©seau pour que celle-ci soit reli√©e au r√©seau interne que d√©ssert votre passerelle.

Suite au d√©marrage de votre client, configurez lui une adresse IP statique. Ensuite, d√©marrez un navigateur web au sein de votre client. Pour chaque noeud ESXi, r√©alisez les √©tapes suivantes:

- Ouvrez un navigateur web et tentez d'acc√©der √† l'h√¥te ESXi **<u>√† partir du nom de domaine.</u>**
- Ouvrez une session et confirmez que l'hyperviseur fonctionne bien.

### 4. Configuration d'un vSwitch Standard pour iSCSI

Nous devons √† pr√©sent configurer un r√©seau pour notre stockage iSCSI. Dans le laboratoire pr√©c√©dent, nous avions utilis√© un *vSwitch* distribu√©. Or, pour utiliser un tel *vSwitch*, nous devons avoir acc√®s √† *vCenter*, que nous n'avons pas encore install√©. Pour faire changement, nous utiliserons des *vSwitch* standards cette fois.

<mark>Pour chaque serveur ESXi, suivez les √©tapes suivantes:</mark><br/><br/>

**Cr√©ation du *vSwitch Standard:***
- Menu `Mise en r√©seau` ‚Üí `Commutateurs virtuels`
- Cliquez sur `Ajouter un commutateur virtuel standard`
    - Nom: vSwitch1
    - MTU: 9000
    - Liaison montante 1: vmnic1
    - S√©curit√©:
        - **Mode Promiscuit√©:** Accepter
        - **Modifications de l'adresse MAC:** Accepter
        - **Transmissions forg√©es:** Accepter
- Ajouter

**Cr√©ation d'un groupe de ports:**
- Menu `Mise en r√©seau` ‚Üí `Groupe de ports`
- Cliquez sur `Ajouter un groupe de ports`
    - Nom: iSCSI
    - VLAN: 0
    - Commutateur virtuel: vSwitch1
- Ajouter

**Cr√©ation d'un VMkernel port:**
- Menu `Mise en r√©seau` ‚Üí `Ajouter une NIC VMKernel`
    - Groupe de ports: iSCSI
    - MTU: 9000
    - Version IP: IPv4
    - Param√®tres IPV4: Statique
      - IP: *Votre IP iSCSI*
      - Masque: 255.255.255.0
    - Pile TCP/IP: Par d√©faut
    - Services: Aucun

### 5. Activation de l'adapteur logiciel iSCSI

**<mark>Pour chaque serveur ESXi</mark>**, dirigez-vous dans le menu `stockage` ‚Üí `adapteurs` et cliquez sur `iSCSI Logiciel`. Activez iSCSI et attendez quelques secondes que tous les champs disponibles s'affichent dans la fen√™tre.

Lorsque vous le pourrez, faites une liaison avec votre port `vmk1` que vous avez pr√©alablement cr√©√© et ajoutez imm√©diatement votre cible dynamique (l'ip de TrueNAS).

### 6. Mise en place d'un stockage r√©seau iSCSI

J'ai cr√©√© un mod√®le de VM **TrueNAS** pour que vous puissiez vous concentrer sur la mise en place du stockage iSCSI plut√¥t que sur l'installation. Lorsque vous aurez import√© le mod√®le, n'oubliez pas de configurer un adapteur r√©seau sur votre r√©seau de gestion et l'autre sur votre r√©seau iSCSI. De plus, vous devrez ajouter deux disques durs pour configurer votre pool ZFS. Comme *vCenter* sera install√© directement sur notre stockage iSCSI, je vous recommande de mettre 2 x 500Go en mode *striping* pour totaliser 1To d'espace disque sur votre iSCSI.

R√©f√©rez-vous aux laboratoires pr√©c√©dents pour mettre en place votre stockage iSCSI sur le r√©seau. Lorsque ce sera fait, <mark>**cr√©ez une nouvelle banque de donn√©es sur l'un des hyperviseurs**</mark>. Celle-ci nous sera utile pour y installer vCenter.

:::tip
Une fois que votre banque de donn√©es VMware aura √©t√© cr√©√©e √† partir de l'un des hyperviseurs, elle devrait √©galement apparaitre dans les autres hyperviseurs. Relancer une analyse des stockages au besoin.
:::

### 7. D√©ploiement de vCenter

Dans le client Windows que vous utilisez. Dirigez-vous dans `C:\VCSA\` et √©ditez le fichier *parameters.json* pour faire correspondre vos informations:

```json title='C:\VCSA\parameters.json' showLineNumbers
{
  "__version": "2.13.0",
  "new_vcsa": {
    "esxi": {
      "hostname": "esx3.gabriel.lan",       #H√¥te ESXi qui accueillera vCenter
      "username": "root",                   #Username ESXi
      "password": "P@ssw0rd",               #Mot de passe ESXi
      "deployment_network": "VM Network",   #Le r√©seau utilis√© par la VM vCenter
      "datastore": "iSCSI_DATA"             #La banque de donn√©es √† utiliser pour l'installation
    },
    "appliance": {
      "name": "vCenter",
      "deployment_option": "tiny",
      "thin_disk_mode": true
    },
    "network": {
      "ip_family": "ipv4",                  #Configurations IP pour vCenter
      "mode": "static",
      "ip": "192.168.21.21",
      "dns_servers": ["192.168.21.5"],
      "prefix": "24",
      "gateway": "192.168.21.1"
    },
    "os": {
      "ntp_servers": "0.ca.pool.ntp.org",   #Activation du protocole SSH pour Root
      "password": "P@ssw0rd",
      "ssh_enable": true
    },
    "sso": {
      "password": "P@ssw0rd",               #Votre domaine SSO ainsi que le mot de passe de l'administrateur
      "domain_name": "gabriel.lan"
    }
  },  
    "ceip": {                               #R√©ponse n√©gative au programme d'am√©lioration du produit
    "settings": {
      "ceip_enabled": false
    }
  }
}
```

:::caution
Les commentaires ne sont pas accept√©s dans le format JSON! Ne copiez donc pas mes commentaires b√™tement, √ßa ne fonctionnera pas.
:::

Une fois que votre fichier JSON est bien modifi√©. Lancez PowerShell 7 dans Windows et entrez la commande suivante: `C:\VCSA_Deploy.ps1`. Si les configurations fournies dans votre fichier JSON sont bonnes, le d√©ploiement devrait prendre de 20 √† 30 minutes. Autrement, un message d'erreur vous sera fourni, vous explicant ce qui a caus√© l'√©chec du d√©ploiement. Corrigez l'erreur, supprimez le dossier `C:\deploy-log` et tentez le coup √† nouveau.

### 8. Configuration initiale de vCenter

Acc√©dez √† l'interface de votre instance vCenter et ouvrez une session:

- Cr√©ez votre *Datacenter*
- Ajoutez vos noeuds ESXi au *datacenter*

**<span class="red-text">Fin de la phase 1</span>**

:::tip
√Ä ce stade, si tout fonctionne bien, je vous recommande de prendre des *snapshots* dans LabInfo. Identifiez-les pertinemment. En cas de retour en arri√®re, vous ne pourrez pas revenir en arri√®re sur un seul hyperviseur, vous devrez revenir en arri√®re pour les trois ainsi que TrueNAS

Vous pouvez utiliser la date et une br√®ve description. Ex: 20251201-Phase1
:::