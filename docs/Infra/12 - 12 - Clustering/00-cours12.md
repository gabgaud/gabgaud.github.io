import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Cours 12

# Clustering Proxmox

* * *

## Cluster Proxmox, pourquoi ? ğŸ¤”

En fait, pour comprendre pourquoi il est essentiel de crÃ©er un cluster Proxmox en entreprise, il suffit d'imaginer ce qui pourrait se produire si nous n'en avions pas. Imaginons, par exemple, un serveur Proxmox hÃ©bergeant 10 machines virtuelles. Sans cluster on risquerait:

- **Une panne matÃ©rielle** : Carte mÃ¨re grillÃ©e â†’ Plusieurs jours d'arrÃªt
- **Maintenance** : Mise Ã  jour du serveur â†’ Interruption de service
- **Saturation** : Plus de ressources â†’ Impossible d'ajouter des *VMs*

**<span class="fonttaller green-text">La solution : le cluster</span>**<br/>

Le *clustering*, c'est la crÃ©ation d'une Ã©quipe de serveurs qui travaillent ensemble en vue d'atteindre des objectifs communs. Cette faÃ§on de fonctionner comporte de nombreux avantages:

1. **Haute disponibilitÃ©:** Une *VM* redÃ©marre ailleurs si son serveur hÃ´te plante.
2. **Migration Ã  chaud:** DÃ©placer une *VM* sans l'arrÃªter
3. **Maintenance sans interruption:** RedÃ©marrer un serveur sans impact
4. **Gestion centralisÃ©e:** Un seul point d'administration
5. **Ã‰volutivitÃ©:** Ajouter des serveurs facilement

```
Cluster Proxmox
â”œâ”€â”€ Serveur 1 : VM 1, VM 4, VM 7
â”œâ”€â”€ Serveur 2 : VM 2, VM 5, VM 8
â””â”€â”€ Serveur 3 : VM 3, VM 6, VM 9

Si Serveur 1 tombe â†’ VMs redÃ©marrent automatiquement sur Serveur 2 ou 3 !
```

### Quand considÃ©rer le *clustering* ?

Vous DEVEZ considÃ©rer un *cluster* si:

- Vos applications ne peuvent pas s'arrÃªter (sites web, BDD critiques)
- Vous gÃ©rez plus de 5-10 VMs importantes
- Vous avez besoin de faire des maintenances sans interruption
- Votre entreprise dÃ©pend de ces VMs

## Qu'est-ce qu'un *cluster* ? ğŸš’

RÃ¨gle gÃ©nÃ©ral: J'aime bien les analogies. Ici, je vais vous comparer le *clustering* Ã  une Ã©quipe de pompiers. Imaginez si la caserne de votre ville ne contenait qu'un seul pompier:

- Il peut Ã©teindre qu'un seul feu Ã  la fois.
- Advenant qu'il soit malade, il n'y a personne pour intervenir.
- S'il est dÃ©jÃ  en intervention, il ne peut pas rÃ©pondre pour un autre urgence.
- Sa force physique est limitÃ©.

Maintenant, avec trois pompiers, on change la dynamique:

- Si un pompier est indisponible, un autre prend le relais.
- Ils peuvent gÃ©rer plus d'un incendie Ã  la fois.
- Ils se coordonnent par radio.
- Ils peuvent se partager de l'Ã©quipement.

**Le *clustering* c'est exactement Ã§a, mais dans le domaine des technologies de l'information:**

1. Les serveurs Proxmox communiquent entre eux via ce qu'on appelle un *heartbeat* toutes les secondes ğŸ’“. En gros les serveurs veillent les uns sur les autres pour s'assurer que tout le monde va bien.
2. Les serveurs Proxmox partagent leurs informations Ã  l'aide d'une base de donnÃ©es distribuÃ©e nommÃ©e Corosync ( [corosync comporte plusieurs services](https://fr.wikipedia.org/wiki/Corosync_Cluster_Engine) ). Cette base de donnÃ©e contient la liste des *VMs* ainsi que leur emplacement, l'Ã©tat de santÃ© de chaque noeud, la configuration du cluster, les ressources disponibles, etc.
3. Les serveurs prennent Ã©galement des dÃ©cisions collectivement. Par exemple, si le noeud #2 croit que le noeud #1 est en panne, un Â« vote Â» aura lieu entre les noeuds restant pour dÃ©terminer si oui, ou non, le noeud #1 est en panne. Si c'est le cas, d'autres actions s'initieront.

## Les composants d'un cluster Proxmox

### Les noeuds ğŸª¢
Nous en avons dÃ©jÃ  parlÃ©, il s'agit des membres (serveurs) qui composent le groupe (*cluster*). Chaque serveur Proxmox dans un cluster est donc appelÃ© un noeud (*node*).

### Corosync
Il s'agit d'un systÃ¨me de communication *open source* comprenant des fonctionnalitÃ©s supplÃ©mentaires pour la mise en oeuvre de la haute-disponibilitÃ©. Il gÃ¨re les *heartbeats* ainsi que la base de donnÃ©e rÃ©pliquÃ©e, entre autre choses.

### Quorum ğŸ—³ï¸
Le quorum correspond Ã  la majoritÃ© des serveurs. Lorsque des dÃ©cisions critiques sont prises, comme redÃ©marrez des *VMs* depuis un autre noeud, le quorum doit s'exprimer favorablement.

### Stockage partagÃ© 
Un stockage accessible par tous les noeuds assure qu'une *VM* peut Ãªtre dÃ©marrÃ© sur n'importe quel serveur.

* * *

|Un cluster n'est **PAS** âŒ|Un cluster **EST** âœ…|
|---------------------------|----------------------|
|Une sauvegarde. Un cluster ne remplace pas les *backups*| Une solution de haute disponibilitÃ©.|
|Une protection contre les erreurs humaines. Supprimer une *VM* la supprime partout| Une facilitÃ© de gestion centralisÃ©e.|
|Gratuit en ressources. Le *clustering* consomme des ressources physiques pour se coordonner.| Une protection contre les pannes matÃ©rielles.|
|Simple Ã  maintenir. C'est plus complexe qu'un serveur unique.| Une capacitÃ© de migration Ã  chaud.|

## Comment Ã§a fonctionne âš™ï¸

### Architecture technique

Un cluster Proxmox repose sur 3 technologies clÃ©s:

1. **Corosync: La communication**

    ```
    Corosync = Le systÃ¨me nerveux du cluster

    NÅ“ud 1 â†’ "Heartbeat : Je suis vivant" â†’ NÅ“ud 2, NÅ“ud 3
    NÅ“ud 2 â†’ "Heartbeat : Je suis vivant" â†’ NÅ“ud 1, NÅ“ud 3
    NÅ“ud 3 â†’ "Heartbeat : Je suis vivant" â†’ NÅ“ud 1, NÅ“ud 2

    FrÃ©quence : Toutes les secondes
    Si pas de rÃ©ponse aprÃ¨s 10 secondes â†’ NÅ“ud considÃ©rÃ© mort
    ```

2. **Corosync Configuration Database (corosync.conf)**
    ```
    Base de donnÃ©es distribuÃ©e qui stocke :
    â”œâ”€â”€ Configuration du cluster
    â”œâ”€â”€ Liste des nÅ“uds
    â”œâ”€â”€ VMs et leur emplacement
    â”œâ”€â”€ Ressources HA
    â””â”€â”€ ParamÃ¨tres rÃ©seau

    SynchronisÃ©e automatiquement entre tous les nÅ“uds
    ```

3. **Proxmox Cluster File System (pmxcfs)**
    ```
    SystÃ¨me de fichiers spÃ©cial montÃ© sur /etc/pve/
    â”œâ”€â”€ Accessible en lecture/Ã©criture sur tous les nÅ“uds
    â”œâ”€â”€ Modifications rÃ©pliquÃ©es automatiquement
    â””â”€â”€ Contient toute la configuration du cluster

    Exemple : CrÃ©er une VM sur nÅ“ud 1 â†’ Visible immÃ©diatement sur nÅ“ud 2 et 3
    ```

### Flux de communication ğŸ“¢

#### Exemple 1 : CrÃ©ation d'une VM
```
Vous cliquez sur "CrÃ©er VM" dans l'interface web
                        â†“
La requÃªte arrive au nÅ“ud oÃ¹ vous Ãªtes connectÃ©
                        â†“
La configuration est Ã©crite dans /etc/pve/qemu-server/100.conf
                        â†“
pmxcfs rÃ©plique automatiquement ce fichier vers les autres nÅ“uds
                        â†“
Tous les nÅ“uds voient la nouvelle VM instantanÃ©ment
                        â†“
La VM dÃ©marre sur le nÅ“ud que vous avez choisi
```

#### Exemple 2: Quand un noeud tombe en panne
```
NÅ“ud 1 ne rÃ©pond plus aux heartbeats
                        â†“
AprÃ¨s 10 secondes, NÅ“ud 2 et 3 le dÃ©clarent "mort"
                        â†“
Les VMs HA configurÃ©es sur NÅ“ud 1 sont identifiÃ©es
                        â†“
Vote du quorum : "Doit-on redÃ©marrer ces VMs ?"
                        â†“
Si quorum OK â†’ RedÃ©marrage automatique sur NÅ“ud 2 ou 3
                        â†“
Services restaurÃ©s (downtime : ~2-3 minutes)`
```

### Les rÃ©seaux ğŸŒ

Un cluster Proxmox utilise typiquement plusieurs rÃ©seaux:
- Le rÃ©seau du cluster lui-mÃªme (Corosync): Communication vitale (Ex: VLAN10 - 10.0.10.0/24)
- RÃ©seau de stockage : AccÃ¨s aux donnÃ©es partagÃ©es (Ex: VLAN20 - 10.0.20.0/24)
- RÃ©seau de gestion : Interface web et SSH (Ex: VLAN 2 - 10.0.2.0/24)
- RÃ©seau des VMs: Trafic des machines (Ex: VLAN30 - 10.0.30.0/24)

:::info
Il s'agit ni plus ni moins de considÃ©rer le *cluster* et son environnment intrinsÃ¨que dans votre segmentation lorsque vous mettez le tout en rÃ©seau.
:::

### Synchronisation (NTP) â±ï¸

Tout comme pour *Active Directory*, la synchronisation des horloges est cruciale pour la communication entre les noeuds et la prise de dÃ©cision. **<span class="yellow-text">On ne peut pas tolÃ©rer plus de 10 secondes de dÃ©calage!</span>** Autrement, il risque d'y avoir un dÃ©calage dans les diffÃ©rentes donnÃ©es sur les serveurs et cela risque de provoquer une corruption de ces mÃªmes donnÃ©es. On appelle ce phÃ©nomÃ¨ne le *split-brain* et nous devons l'Ã©viter Ã  tout prix.


## Le quorum: le coeur du cluster ğŸ§¡

Le **quorum**, c'est le nombre **minimum** de noeuds nÃ©cessaires pour que le cluster prenne des dÃ©cisions. Pourquoi est-ce que c'est vital ? Pour Ã©viter le *split-brain*. Imaginons un cluster composÃ© de seulement 2 noeuds (**dÃ©jÃ , 2 noeuds ce n'est pas recommandÃ©**).

Un problÃ¨me de rÃ©seau survient et les deux noeuds deviennent incapables de communiquer:

```
    Noeud 1                       |           Noeud 2
        â†“                         |              â†“
"Je ne vois plus Noeud 2"         |  "Je ne vois plus Noeud 1"
        â†“                         |              â†“
"Noeud 1 est probablement down"   |  "Noeud 2 est probablement down"
        â†“                         |              â†“
"Je redÃ©marre ses VMs"            |  "Je redÃ©marre ses VMs"
        â†“                         |              â†“
---------------------------------------------------------------------
DÃ‰SASTRE : VMs en double sur les noeuds, corruption des donnÃ©es!
```

**<span class="fonttaller green-text">La solution : le quorum (au moins 3 noeuds)</span>**<br/>

Reprenons le mÃªme problÃ¨me de rÃ©seau, mais en y ajoutant un noeud:

```
    Noeud 1 isolÃ©                      |      Noeud 2 & 3 communiquent
        â†“                              |              â†“
"Je ne vois plus Noeud 2 & 3"          |     "Je ne vois plus Noeud 1"
        â†“                              |              â†“
"Noeud 2 & 3 sont probablement down"   |     "Noeud 1 est probablement down"
        â†“                              |              â†“
"1 seul vote = pas de quorum"          |     "2 votes = quorum atteint"
        â†“                              |              â†“
"Ne peut agir, attend"                 |     "RedÃ©marrent les VMs de Noeud1 "
```

>*Oui mais Gabriel, est-ce que Ã§a veut dire que je ne peux pas avoir un cluster de deux noeuds en aucune circonstance ?*
>
>*-Les Ã©tudiants*

Bonne question! Vous pouvez effectivement avoir un cluster de deux noeuds sans problÃ¨me. NÃ©anmoins, il vous faudra ajouter ce que l'on nomme un **QDevice**.

### Le QDevice: Le juge de paix â˜®ï¸

Un **QDevice** est tout simplement une petite machine (vm, raspberry, conteneur) qui ne servira qu'Ã  voter lorsque le cluster cherchera Ã  obtenir le **quorum**. **<span class="red-text">Ã‰videmment, n'installez PAS un QDevice en tant que VM sur l'un des noeuds du cluster!</span>** Ce n'est pas une bonne pratique puisque si une panne matÃ©rielle devait arriver sur ce noeud, deux votes du quorum serait automatiquement perdu.

#### Installation d'un QDevice

Sur la machine Linux qui sera QDevice:

```bash
sudo apt install corosync-qnetd         #Installation du package

sudo systemctl enable corosync-qnetd    #DÃ©marrage automatique du service
sudo systemctl start corosync-qnetd     #DÃ©marrer le service dÃ¨s maintenant
```

Puis sur un dÃ¨s noeuds Proxmox:

```bash
pvecm qdevice setup 192.168.1.100       # IP du QDevice
```

### Ã‰tat du quorum

L'Ã©tat du quorum est visible directement dans l'interface web du *cluster*:

![QuorumGUI](../Assets/12/QuorumGUI.png)

Ou via l'invite de commande:

```bash
pvecm status
```