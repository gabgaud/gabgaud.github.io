import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Projet final - Phase 2

:::danger[Travail individuel]
Le projet doit √™tre r√©alis√© individuellement. L'entraide est permise, cependant:
  - Vos captures d'√©cran doivent √™tre uniques et prises par vous.
  - Votre documentation et vos explications doivent √™tre votre composition (pas celle de l'IA ü•≤ ni celle d'un coll√®gue)
  - Le plagiat (IA, coll√®gue ou autre) entrainera syst√©matiquement une note de z√©ro.
:::

* * *

Il est essentiel que vous ayez d√ªment compl√©t√© la Phase 1 du projet avant de vous lancer dans cette portion du projet. Prendre le risque de sauter certaines √©tapes pourrait vous obliger √† tout recommencer.

* * *

## Phase 2

Dans la phase 2, nous optimiseront l'infrastructure virtuelle et d√©ploieront certaines technologies en lien avec le *clustering*.

### 9. Migration vers vSwitch Distribu√©

Comme vous l'avez peut-√™tre d√©j√† remarqu√© dans la phase 1 de ce projet, nous n'utilisons que des *vSwitch Standards* pour le moment. En classe, nous avons vu que l'utilisation de *vSwitch Standards* n'est pas id√©al puisque celles-ci n√©cessitent:

- Une configuration s√©par√©e.
- Des r√©p√©titions dans le travail de gestion et d'administration. üòÆ‚Äçüí®
- Une attention particuli√®re pour √©viter les erreurs (tout comme ce projet). üëÄ

Nous allons donc migrer notre configuration actuelle pour utiliser des *vSwitch Distribu√©s* qui nous permettent une meilleure efficacit√© et un risque d'erreur amoindri. Voici comment nous allons proc√©der. Voici l'architecture cible apr√®s les manipulations:


#### 9.10 Cr√©ation d'un premier vDS

Dans **vCenter**, s√©lectionnez votre *datacenter* puis cliquez sur `Nouveau Distributed Switch`:

- **Nom:** vDS Production
- **Version:** 8.0.3
- **Nombre de liaison montante:** 2
- **Network I/O Control:** Activ√©
- **Default PortGroup:** Aucun pour l'instant

#### 9.11 Cr√©ation d'un second VDS pour vMotion et vSAN

Dans **vCenter**, s√©lectionnez votre *datacenter* puis cliquez sur `Nouveau Distributed Switch`:

- **Nom:** vDS Services
- **Version:** 8.0.3
- **Nombre de liaison montante:** 2
- **Network I/O Control:** Activ√©
- **Default PortGroup:** Aucun pour l'instant
- **MTU:** 9000 (Changeable dans les param√®tres par la suite)

#### 9.20 Cr√©ation des groupes de ports pour vDS Production

Nous aurons besoin de deux groupes de ports distincts sur notre **vDS Production**: Un premier groupe pour l'administration et un second pour les *VMs*. Faites un clic √† l'aide du bouton de droite de la souris sur votre nouveau vDS et s√©lectionnez `Nouveau groupe de ports distribu√©s`:

- **Nom:** DPG-Management
- **Liaison de port:** Statique
- **Nombre de ports:** 8
- **Pool de ressources:** Aucun
- **VLAN:** Aucun
- **S√©curit√©:**
    - **Mode promiscuit√©:** Accepter
    - **Modifications de l'adresse MAC:** Accepter
    - **Transmissions forg√©es:** Accepter
- **Formation du traffic:** Par d√©faut
- **Association et basculement:** Par d√©faut
- **Surveillance:** Par d√©faut
- **Divers:** Par d√©faut

**<span class="red-text"><u>R√©p√©tez les m√™mes √©tapes pour vous cr√©er un groupe de ports nomm√© DPG-VM-Network sur le m√™me vDS</u></span>**

#### 9.21 Cr√©ation des groupes de ports pour vDS Services

Pour les groupes de ports du **vDS Services**, proc√©dez de la m√™me fa√ßon en prenant soin de cr√©er un groupe de ports distribu√© pour vMotion et un autre pour vSAN. <mark>**Ne mettez surtout pas de VLANs en place**</mark>, vos commutateurs priv√©s sur LabInfo ne les supportent pas.

Assurez-vous de cr√©er les groupes de ports sur les bons commutateurs virtuels.

#### 9.3 Ajouter les h√¥tes aux vDS

Nous devons maintenant permettre aux hyperviseurs ESXi d'utiliser les vDS que nous venons de cr√©er. Pour cela, il faut ajouter des h√¥tes aux commutateurs virtuels. **Pour chacun des vDS, faites les √©tapes suivantes:**

- Clic √† l'aide du bouton de droite de la souris sur le vDS, puis `Ajoutez et g√©rez des h√¥tes`
- **S√©lectionner la t√¢che:** Ajouter des h√¥tes
- **S√©lectionner des h√¥tes:** Cocher tous les hyperviseurs
- **G√©rer les adapteurs physiques:** <span class="red-text">Ne rien cocher</span>
- **G√©rer les adapteurs VMKernel:** <span class="red-text">Ne rien cocher</span>
- **Migrer la mise en r√©seau des VMs:** <span class="red-text">Ne rien cocher</span>

#### 9.4 Migration de vmnic0 vers vDS ‚ö†Ô∏è

:::danger[Risque de perdre la connexion!]
Cette √©tape est √† la fois d√©licate et cruciale. Assurez-vous d'avoir du temps et de la concentration avant de proc√©der. Nous nous appr√™tons √† faire passer le r√©seau de gestion des hyperviseurs d'un **vSwitch Standard** √† un **vSwitch Distribu√©.** Vous perdrez donc la connexion avec vos hyperviseurs pendant un court instant. Une erreur de configuration √† cette √©tape vous obligera √† revenir en arri√®re. Respirez par le nez, dites *namaste* et allez-y calmement üòä.
:::

**<span class="fonttaller">Pour les noeuds ne contenant <span class="red-text">PAS</span> vCenter:</span>**

Faites un clic √† l'aide du bouton de droite de la souris sur `vDS Production` et s√©lectionnez `Ajouter ou g√©rer des h√¥tes`:

- **S√©lectionnez la t√¢che:** G√©rez la mise en r√©seau de l'h√¥te
- **S√©lectionnez des h√¥tes:** Choisissez un seul noeud pour l'instant
- **G√©rer les adapteurs physiques:** Attribuez le vmnic0 √† l'uplink 1
- **G√©rer les adapteurs VMKernel:** Attribuez vmk0 √† votre groupe de ports `DPG-Management`
- **Migrer la mise en r√©seau VM:** Pas de modification ici
- **Cliquez sur `terminer`.**

<u>Refaites les m√™mes √©tapes pour l'autre noeud ne contenant PAS vCenter</u>

<br/><br/>
**<span class="fonttaller">Pour le noeud contenant vCenter:</span>**

Observez le sch√©ma ci-dessous. Remarquez comment, actuellement, notre machine **vCenter** d√©pend compl√®tement de notre *vSwitch0*. La probl√©matique que nous rencontreront est directement li√© √† cette d√©pendance. L'objectif est de remplacer notre *vSwitch0* (un vSS) par notre vDS-Prodction (un VDS). Or, **vCenter** poss√®de des protections contre les coupures de connexion. Si nous tentons de nous y prendre comme nous l'avons fait avec les deux autres noeuds, **vCenter** nous bloquera.

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ReplacevSwitch0_W.svg'),
            dark: useBaseUrl('/img/Virtu/ReplacevSwitch0_D.svg'),
        }}
    />
</div>
<br/>
**<span class="fonttaller">Plan de match</span>**

1. Relier la vmnic2 au r√©seau de votre passerelle (lien rose)
2. Relier la vmnic2 au vDS
3. Assigner vmk0 au vDS
4. Migrer le r√©seau de **vCenter**
5. Relier la vmnic0 au vDS
6. Retirer la vmnic2 du vDS

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ReplacevSwitch0_W.gif'),
            dark: useBaseUrl('/img/Virtu/ReplacevSwitch0_D.gif'),
        }}
    />
</div>

:::caution[Gabriel o√π est l'√©tape 6 dans le sch√©ma ?]
Elle n'apparait pas. Elle consiste simplement √† retirer les liens 1 & 2.
:::

**√âtape par √©tape:**

1. Sur l'ESXi h√¥te, reli√© l'adapteur vmnic2 (ou celui disponible) au commutateur d√©sservi par PfSense dans LabInfo.
2. Reliez votre vDS Production √† cette vmnic. Pour ce faire suivez les sous-√©tapes:
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 1: vmnic2 (ou celle qui vous concerne)
    - G√©rez les adapteurs VMKernel:
        - Assignez `DPG-Management` √† vmk0
    - Migrer la mise en r√©seau des VMs:
        - Assignez l'adapteur r√©seau de vCenter dans `DPG-Management`
    - Terminer
3. V√©rifiez que **vCenter** est toujours accessible et que vos changements ont bien √©t√© pris en compte.
4. Ajoutez vmnic0 au vDS (retirer de vSwitch0)
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 2: vmnic0
    - G√©rez les adapteurs VMKernel: rien √† faire
    - Migrer la mise en r√©seau des VMs: rien √† faire
    - Terminer
5. Retirez vmnic2 du vDS (utilis√© temporairement)
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 1: vide
        - Uplink 2: vmnic0
    - G√©rez les adapteurs VMKernel: rien √† faire
    - Migrer la mise en r√©seau des VMs: rien √† faire
    - Terminer

#### 9.5 Suppression des VSS0

√Ä ce stade, tous vos noeuds sont d√©sormais reli√©s au vSwitch Distrivu√© vDS Production que nous avons cr√©√© plus t√¥t. Nous n'avons plus besoin des vSwitch Standards 0 qui sont au sein des diff√©rents hyperviseurs.

Passez donc dans chacun des hyperviseurs et supprimez ces commutateurs virtuels qui sont devenus inutiles.

### 10. Cr√©ation du Cluster

Dans **vCenter**, cr√©ez le *vSphere Cluster* en effectuant un clic droit sur votre *datacenter* et en s√©lectionnant `Nouveau Cluster`:

|Param√®tre|Valeur|Notes|
|:---------:|:------:|:-----:|
|**Nom**|Cluster-*vosinitiales* EX: Cluster-GG|Nom du cluster|
|**vSphere DRS**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**vSphere HA**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**vSAN**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**Image**| Importer une image √† partir d'un h√¥te existant | Tous les noeuds devront correspondre √† cette image syst√®me.|

#### 10.1 Ajouter les noeuds au sein du cluster

Une fois le *cluster* cr√©√©, vous le verrez appara√Ætre dans votre inventaire. Clic droit ‚Üí `Ajouter des h√¥tes` et s√©lectionnez tous vos noeuds ESXi. **Inutile d'importer √† nouveau une image pour vLCM (*Life Cycle Manager*), nous l'avons d√©j√† fait lors de la cr√©ation du *cluster***.

:::danger[Gabriel! Mes hyperviseurs tombent en maintenance üò±]
C'est normal! C'est le *Cluster Quickstart* qui d√©marre automatiquement. En gros, c'est une aide √† l'administrateur...que nous n'utiliseront pas ü•≤
:::

Pour d√©sactiver le *Cluster Quickstart*, s√©lectionnez votre *cluster* dans l'inventaire puis dans la section `Configurer` ‚Üí `D√©marrage Rapide`, cliquez sur `Ignorer le d√©marrage rapide`.

Au besoin, faites un clic droit sur vos hyperviseurs et sortez les du mode maintenance. üöß

### 11. Configurer vMotion

Dans les √©tapes pr√©c√©dentes, nous avons pr√©par√© un vDS pour nos services vMotion & vSAN. Cependant, nous avons pas encore configur√© les interfaces **Vmkernel Ports** pour ces services. Il faut donc s'affairer √† les cr√©er maintenant. D'abord, s√©lectionnez un sous-r√©seau pour **vMotion**. Si vous d√©sirez suivre mon propre plan d'adressage, vous pouvez √©galement: 

|H√¥te|IP vMotion|Subnet|VLAN|
|:----:|:----------:|:------:|:----:|
|ESXi-01|10.20.0.11|255.255.255.0|20|
|ESXi-02|10.20.0.12|255.255.255.0|20|
|ESXi-03|10.20.0.13|255.255.255.0|20|

#### 11.1 Configurer les VMKernel pour chaque noeud

Pour chaque noeud ESXi dans l'inventaire:

- Menu `Configurer` ‚Üí `Adapteurs VMKernel`
- Ajouter une mise en r√©seau:
    - **S√©lectionner un type de connexion:** Adapteur r√©seau VMKernel
    - **S√©lectionner un p√©riph√©rique cible:** S√©lectionner un r√©seau existant ‚Üí `DPG-Vmotion`
    - **Propri√©t√©s du port:** Par d√©faut ‚Üí Services: vMotion
    - **Param√®tres IPv4:** Statique
        - Adresse: *La v√¥tre*
        - Masque: /24
        - Passerelle et DNS: Ne rien changer, pas utile pour nous

**R√©p√©ter les m√™mes √©tapes pour les tous les noeuds**

#### 11.2 Raccorder le vDS Services

Notre vDS n'a aucune liaison montante pour le moment. Autrement dit, aucune interface physique ne d√©ssert ce commutateur virtuel. Nous allons donc associer notre vmnic2, dont nous nous sommes servi temporairement un peu plus t√¥t. 

Dans **vCenter**, clic droit sur `vDS Services` ‚Üí `Ajouter et g√©rer des h√¥tes`:

- **S√©lectionner la t√¢che:** G√©rer la mise en r√©seau de l'h√¥te
- **S√©lectionner des h√¥tes:** S√©lectionner tout
- **G√©rer les adapteurs physiques:** vmnic2 : uplink1
- **G√©rer les adapteurs VMkernel:** vmk2 : `DPG-vMotion`
- **Migrer la mise en r√©seau VM:** Rien √† faire
- **Terminer**

#### 11.3 Tests avec vMotion

Bon, √ßa y est. **vMotion** est en place. Il ne nous reste qu'√† effectuer quelques tests pour s'assurer que √ßa fonctionne bien. Cr√©ez-vous une machine virtuelle toute simple sur votre *cluster*. Id√©alement, √©vitez de cr√©er cette machine virtuelle sur le m√™me noeud que **vCenter**.

:::tip[DRS]
Si le **DRS** √©tait d√©j√† activ√© sur notre *cluster*, nous n'aurions pas besoin de s√©lectionner le noeud. Le module ferait une analyse de charge de travail sur tous les noeuds et d√©terminerait, pour nous, le meilleur emplacement pour notre *vm*. On y reviendra.
:::

Si vous d√©sirez suivre mon propre test, sachez que j'utiliserai une *VM Ubuntu Server* sur ESX1.

:::caution[Aucun DHCP]
Il n'y a aucun serveur DHCP sur notre r√©seau... Ne l'oubliez pas lors de la cr√©ation de votre *vm*
:::

Le vrai test √† effectuer avec **vMotion**, ce n'est pas seulement de constater que la machine peut √™tre transf√©r√©e sans √™tre arr√™t√©e, mais c'est de constater qu'il n'y a aucune interruption.

Pour visualiser ce concept, je vais d√©marrer une commande `ping` vers Google dans ma *vm*. Sous Linux, le `ping` est perp√©tuel, je devrai donc l'arr√™ter moi-m√™me. √Ä cela, je vais ajouter un intervalle de 2 secondes pour ralentir l√©g√®rement l'envoie. Puis, du c√¥t√© de **vCenter**, je vais d√©clencher le transfert avec **vMotion** et je pourrai observer qu'il n'y aucun impact sur l'envoie des pings vers Google, malgr√© ce transfert.

La commande √† utiliser:
```bash
ping -i 2 google.com
```

:::tip[Duplicata de Ping]
Il se peut que vous receviez des r√©ponses en duplicata lors de vos tests d'envoie ICMP. Votre *vm* se trouve derri√®re un environnement r√©seau anormalement lourd. Vous pouvez tout simplement ignorer ces messages `(DUP!)`.
:::

### 12. Configurer la haute-disponibilit√© l'√©quilibrage de charge

La haute-disponibilit√© permet de red√©marrer automatiquement les machines virtuelles sur un autre noeud lorsqu'un incident se produit. Cela dit, **attention:** ce n'est pas du vMotion. Quant √† l'√©quilibrage de charge (DRS), son travail est de surveiller continuellement l'utilisation de CPU/RAM de chaque noeud et de d√©placer automatiquement les *VMs* via **vMotion** au besoin. Le tout dans l'objectif d'optimiser l'utilisation des ressources.

**DRS** poss√®de diff√©rents modes de fonctionnement ainsi que des seuils de migrations:

<span class="green-text">**Modes**:</span>

- **Manuel:** DRS recommande des actions, vous d√©cidez
- **Partiellement automatique:** DRS planifie des actions, mais vous demande une confirmation
- **Automatique:** DRS prend les d√©cisions et migre automatiquement

<span class="green-text">**Seuils de migration**:</span>

- **Conservateur:** Migre les *VMs* seulement lorsqu'il y a un d√©s√©quilibre important
- **Mod√©r√©:** Migre les *VMs* lorsqu'il y a un d√©s√©quilibre standard
- **Agressif:** Migre les machines d√®s le moindre d√©s√©quilibre

#### 12.1 Activation et configuration de vSphere HA

Dans l'interface web de **vCenter**, s√©lectionnez votre *cluster* puis dirigez-vous dans le menu `Configurer` ‚Üí `Disponibilit√© vSphere`. Cliquez sur `Modifier...`

Commencez par activer **vSphere HA**

![vSphereHA](../Assets/Projet/vSphereHA.png)

<span class="green-text fonttaller">**Onglet Pannes et r√©ponses:**</span><br/><br/>

|Param√®tre|Valeur|Explication|
|---------|------|-----------|
|R√©ponse en cas de panne de l'h√¥te|Red√©marrer les machines virtuelles|vSphere HA surveille les noeuds via des *heartbeats* toutes les secondes. Si un noeud ne r√©pond plus pendant 15 secondes, **vSphere HA** consid√®re le noeud plant√© et prend les mesures n√©cessaires. Dans notre cas, les mesures correspondent au red√©marrage des *VMs* sur d'autres noeuds encore op√©rationnels.|
|R√©ponse √† l'isolation d'h√¥te|Mettre hors tension et red√©marrer les machines virtuelles|Que fait **vSphere HA** si un noeud perd sa connexion r√©seau mais fonctionne encore ? <br/><br/>Dans notre cas, les *VMs* seront √©teintes proprement puis elles seront d√©marr√©es sur d'autres noeuds.|
|Banque de donn√©es avec PDL (*Permanent Device Lost*)|Mettre hors tension et red√©marrer les machines virtuelles|Que fait **vSphere HA** si une banque de donn√©es dispara√Æt d√©finitivement ?<br/><br/>Avec notre configuration, **vSphere HA** tentera de d√©marrer les *VMs* depuis une autre banque de donn√©es si possible. √âvidemment, s'il n'existe pas de copie de la *VM* en question sur une autre banque de donn√©e, le red√©marrage sera impossible (logique: pas de donn√©es = pas de vm)|
|Banque de donn√©es avec APD (*All Path Down*)|Mettre hors tension et red√©marrer les machines virtuelles|Que fait **vSphere HA** si tous les chemins vers une banque de donn√©es sont coup√©s ?<br/><br/>Le stockage ne r√©pond plus, mais n'a pas signal√© sa d√©faillance. **vSphere HA** attendra 140 secondes (par d√©faut) pour voir si le stockage revient. Apr√®s ce d√©lai, si le stockage est toujours absent, le syst√®me entreprend les mesures prescrites.|
|Surveillance de *VM*|D√©sactiv√©|**vSphere HA** peut s'assurer que les *VMs* elles-m√™mes r√©pondent bien en leur envoyant des *heartbeats* r√©guli√®rement. Lorsqu'une *VM* ne r√©pond plus pendant un certain temps, elle est red√©marr√©e.|

<span class="green-text fonttaller">**Onglet Contr√¥le d'admission:**</span><br/><br/>

Le contr√¥le d'admission permet de d√©finir les politiques de r√©servation de ressources pour red√©marrer les *VMs* en cas de panne. En gros, on s'assure que tous les noeuds aient une r√©server de ressource au cas o√π un noeud flancherait. Ainsi, les *VMs* qui doivent √™tre d√©plac√©es peuvent effectivement l'√™tre.

|Param√®tre|Valeur|Explication|
|---------|------|-----------|
|Panne d'h√¥te tol√©r√©es par le cluster|1|Combien de pannes simultan√©es voulez-vous que le *cluster* soit en mesure de survivre ?<br/><br/>Vous ne pouvez pas d√©finir n'importe quoi comme politique ici. Avec 3 noeuds (notre situation), on ne peut g√©n√©ralement pas aller plus haut que FTT=1 (*Failed To Tolerate*). C'est purement math√©matiques: supposons qu'un noeud tombe en panne, les deux autres noeuds doivent poss√©der suffisamment de ressources disponibles pour accueillir les *VMs* qui seront transf√©r√©es.|
|Capacit√© de basculement|Strat√©gie d'emplacement (vm sous tension)|Il existe plusieurs fa√ßons de configurer le basculement en cas de panne:<br/><br/>**1. Strat√©gie d'emplacement (vm sous tension):**<br/> Avec cette strat√©gie **vSphere HA** calcul automatiquement les ressources n√©cessaires au basculement et s'assure que les noeuds aient les ressources disponibles en cas de panne de l'un des noeuds. Strat√©gie simple et directe.<br/><br/>**2. Pourcentage de ressources du cluster:**<br/> Cette strat√©gie vous permet de sp√©cifiez vous-m√™me le poucentage de ressources √† r√©server sur les noeuds. Cette strat√©gie est flexible mais n√©cessite des calculs manuels.<br/><br/>**3. H√¥tes de basculement d√©di√©s**<br/> Cette strat√©gie vous permet de sp√©cifiez un noeud qui est en *stand-by*. Ce noeud reste vide et sert uniquement au *failover*. C'est √©videmment une strat√©gie tr√®s couteuse √† mettre en place.|
|D√©gradation des performances tol√©r√©e par les *VMs*|100%|Est-ce que **vSphere HA** autorise une surcharge temporaire apr√®s une panne ?<br/><br/>√âvidemment, lorsque des *VMs* sont d√©plac√©es suite √† une panne, il n'est pas rare que les performances soient d√©grad√©es. Les noeuds qui prennent le relais sont souvent surcharg√©s. Souvent, mieux vaut une *VM* moins performante qu'une *VM* √©teinte. La valeur 100% d√©sactive cette surveillance de la d√©gradation.|

<span class="green-text fonttaller">**Onglet Banque de donn√©es de signal de pulsation:**</span><br/><br/>

**vSphere HA** ne se contente pas seulement de faire des *heartbeats* sur le r√©seau pour v√©rifier si un noeud est en panne, il utilise √©galement un syst√®me de *heartbeats* avec les banque de donn√©es. <u>Tous les noeuds doivent √©crire sur une banque de donn√©e partag√© √† chaque *x* secondes pour confirmer qu'ils sont bien en vie.</u>

**Sc√©nario:**
Imaginons qu'un noeud ESXi a un probl√®me au niveau de son r√©seau de gestion (c√¢ble d√©branch√© ou *switch* √©teinte). Cependant le noeud fonctionne toujours bien. Les autres noeuds se demanderont alors:

>*ESXi-1 ne r√©pond plus! Est-il vraiment en panne ou simplement isol√© ?*
>

Les autres noeuds v√©rifieront alors si ESXi-1 peut encore √©crire sur le stockage partag√©, gr√¢ce aux *heartbeats* sur la banque de donn√©e. Si c'est le cas: le noeud est vivant! Sinon: panne r√©el.

|Param√®tre|Valeur|Explication|
|---------|------|-----------|
|Banque de donn√©es pour *heartbeats*|S√©lectionner automatiquement les banques de donn√©es accessibles depuis les h√¥tes|Quelles banques de donn√©es **vSphere HA** doit utiliser pour son syst√®me de *heartbeats*<br/><br/>**S√©lectionner automatiquement (recommand√©):** **vSphere HA** d√©termine automatiquement les banques de donn√©es √† utiliser. Nous n'avons qu'un seul stockage partag√©; celui fait avec TrueNAS.<br/><br/>Les autres options vous permettent de sp√©cifier vous-m√™mes une partie ou la totalit√© des banques de donn√©es √† utiliser.|

La mise en place de **vSphere HA** peut prendre quelques minutes. Durant cette p√©riode, il se peut que vous soyez t√©moin de certains messages d'erreur. Prenez votre mal en patience. Tant que les configurations ne sont pas termin√©es, <span class="red-text">**ne touchez √† rien!**</span>

Au final, vous devriez termin√© avec des avertissements. Il s'agit d'√©l√©ments mineurs. Voici les avertissements acceptables:

1. **Cet h√¥te n'a pas de redondance de r√©seau de gestion:** C'est normal et vrai. Normalement, il nous faudrait une redondance pour le r√©seau de gestion, mais il ne s'agit que d'un laboratoire. Nous pouvons la jouer risqu√© üòú

2. **Le nombre de banque de donn√©es de signal de pulsation de vSphere HA pour cet h√¥te est 1, ce qui est moins que le minimum exig√© : 2** C'est normal et aussi vrai. Nous n'avons que notre stockage iSCSI via TrueNAS en place. Normalement, il nous faudrait une redondance ici aussi.

#### 12.2 Activation et configuration de DRS

Dans l'interface web de **vCenter**, s√©lectionnez votre *cluster* puis dirigez-vous dans le menu `Configurer` ‚Üí `vSphere DRS`. Cliquez sur `Modifier...`

Commencez par activer **vSphere HA**

![ActiverDRS](../Assets/Projet/vSphereDRS.png)

<span class="green-text fonttaller">**Onglet Automatisation:**</span><br/><br/>

|Param√®tre|Valeur|Explication|
|---------|------|-----------|
|Niveau d'automatisation|Enti√®rement automatis√©|Il s'agit du mode de fonctionnement de DRS:<br/><br/>**Enti√®rement automatis√©:**<br/>DRS prend les d√©cisions et migre automatiquement.<br/><br/>**Partiellement automatis√©:**<br/> DRS planifie des actions, mais vous demande une confirmation.<br/><br/>**Manuel:** <br/>DRS recommande des actions, vous d√©cidez|
|Seuil de migration|Medium|Fr√©quence d'intervention de DRS:<br/><br/>**Conservateur (gauche):**<br/> Migre les *VMs* seulement lorsqu'il y a un d√©s√©quilibre important.<br/><br/> **Mod√©r√© (millieu):**<br/> Migre les *VMs* lorsqu'il y a un d√©s√©quilibre standard.<br/><br/>**Agressif (droite):**<br/> Migre les machines d√®s le moindre d√©s√©quilibre.|
|Predictive DRS|‚ùå D√©sactiv√©|Utilise l'intelligence pr√©dictive pour anticiper certains besoins en ressources. Cette option n√©cessite **vRealize Operations Manager**|
|Automatisation de machine virtuelle|‚úÖ Activ√©|Cette option permet de d√©finir des r√®gles d'automatisation sp√©cifiques par *VM*. Cela peut √™tre utile si certaines *VMs* n√©cessitent un traitement diff√©rent du reste du cluster.|
|Limite de temps de latence des p√©riph√©riques de VM|‚ùå D√©sactiv√©|Il s'agit du temps d'attente acceptable qu'une *VM* peut attendre avant de pouvoir acc√©der √† un p√©riph√©rique comme un stockage par exemple. Un temps de latence long est souvent un indicateur qu'une *VM* √©prouve des difficult√©s. DRS peut donc se servir de cette valeur pour prendre des d√©cisions de r√©√©quilibrage.|
|Automatisation DRS de la VM de relais|‚ùå D√©sactiv√©|Il s'agit de la configuration des options pour les *VMs* ayant des p√©riph√©riques *pass-through*. Ces *VMs* sont normalement plus difficiles √† configurer.|

<span class="green-text fonttaller">**Onglet options suppl√©mentaires:**</span><br/><br/>

|Param√®tre|Valeur|Explication|
|---------|------|-----------|
|Distribution des *VMs*|‚ùå D√©sactiv√©|Cette option force DRS √† r√©partir les VMs de mani√®re plus uniforme entre tous les h√¥tes du cluster, plut√¥t que de se concentrer uniquement sur l'√©quilibrage des ressources (CPU/RAM). Le message concernant la d√©gradation du DRS signifie que si vous cochez cette option l'√©quilibrage des ressources pourrait √™tre moins optimal.|
|Surcharge du CPU|4v:1p|Permet de sur-allouer les ressources CPU des h√¥tes ESXi. C'est-√†-dire allouer plus de vCPUs aux VMs que de c≈ìurs physiques disponibles.<br/><br/>R√©alit√© des VMs :<br/>-La plupart des VMs n'utilisent pas 100% de leurs vCPUs en permanence<br/>-Un serveur web avec 4 vCPUs peut utiliser en moyenne 20-30% seulement<br/>-La surallocation permet de consolider plus de *VMs*|
|Partages √©volutifs|‚ùå D√©sactiv√©| Lorsqu'il y a des pools de ressources, l'acc√®s aux ressources est prioris√© en fonction du nombre de *shares* que d√©tiennent les *VMs*. Nous n'utilisons pas de pools ressources, laissez donc cette option d√©sactiv√©.|

<span class="green-text fonttaller">**Onglet gestion de l'alimentation:**</span><br/>

DRS peut √©teindre les noeuds qui sont tr√®s peu utilis√©s. Cela permet √† une entreprise de faire des √©conomies d'√©nergie importante. Dans notre contexte (LabInfo), ce n'est pas vraiment pertinent √† mettre en place, mais sachez que c'est une configuration possible.

<span class="green-text fonttaller">**Onglet options avanc√©es:**</span><br/>

Options suppl√©mentaires que vous pouvez ajouter manuellement.

Une fois les options remplies, cliquez sur `Terminer`

**<span class="red-text">Fin de la phase 2</span>**

:::tip
√Ä ce stade, si tout fonctionne bien, je vous recommande de prendre des *snapshots* dans LabInfo. Identifiez-les pertinemment. En cas de retour en arri√®re, vous ne pourrez pas revenir en arri√®re sur un seul hyperviseur, vous devrez revenir en arri√®re pour les trois ainsi que TrueNAS.

Vous pouvez utiliser la date et une br√®ve description. Ex: 20251203-Phase2
:::