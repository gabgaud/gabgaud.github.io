import useBaseUrl from '@docusaurus/useBaseUrl';
import ThemedImage from '@theme/ThemedImage';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Projet final - Phase 2

:::danger[Travail individuel]
Le projet doit √™tre r√©alis√© individuellement. L'entraide est permise, cependant:
  - Vos captures d'√©cran doivent √™tre uniques et prises par vous.
  - Votre documentation et vos explications doivent √™tre votre composition (pas celle de l'IA ü•≤ ni celle d'un coll√®gue)
  - Le plagiat (IA, coll√®gue ou autre) entrainera syst√©matiquement une note de z√©ro.
:::

* * *

Il est essentiel que vous ayez d√ªment compl√©t√© la Phase 1 du projet avant de vous lancer dans cette portion du projet. Prendre le risque de sauter certaines √©tapes pourrait vous obliger √† tout recommencer.

* * *

## Phase 2

Dans la phase 2, nous optimiseront l'infrastructure virtuelle et d√©ploieront certaines technologies en lien avec le *clustering*.

### 9. Migration vers vSwitch Distribu√©

Comme vous l'avez peut-√™tre d√©j√† remarqu√© dans la phase 1 de ce projet, nous n'utilisons que des *vSwitch Standards* pour le moment. En classe, nous avons vu que l'utilisation de *vSwitch Standards* n'est pas id√©al puisque celles-ci n√©cessitent:

- Une configuration s√©par√©e.
- Des r√©p√©titions dans le travail de gestion et d'administration. üòÆ‚Äçüí®
- Une attention particuli√®re pour √©viter les erreurs (tout comme ce projet). üëÄ

Nous allons donc migrer notre configuration actuelle pour utiliser des *vSwitch Distribu√©s* qui nous permettent une meilleure efficacit√© et un risque d'erreur amoindri. Voici comment nous allons proc√©der. Voici l'architecture cible apr√®s les manipulations:


#### 9.10 Cr√©ation d'un premier vDS

Dans **vCenter**, s√©lectionnez votre *datacenter* puis cliquez sur `Nouveau Distributed Switch`:

- **Nom:** vDS Production
- **Version:** 8.0.3
- **Nombre de liaison montante:** 2
- **Network I/O Control:** Activ√©
- **Default PortGroup:** Aucun pour l'instant

#### 9.11 Cr√©ation d'un second VDS pour vMotion et vSAN

Dans **vCenter**, s√©lectionnez votre *datacenter* puis cliquez sur `Nouveau Distributed Switch`:

- **Nom:** vDS Services
- **Version:** 8.0.3
- **Nombre de liaison montante:** 2
- **Network I/O Control:** Activ√©
- **Default PortGroup:** Aucun pour l'instant
- **MTU:** 9000 (Changeable dans les param√®tres par la suite)

#### 9.20 Cr√©ation des groupes de ports pour vDS Production

Nous aurons besoin de deux groupes de ports distincts sur notre **vDS Production**: Un premier groupe pour l'administration et un second pour les *VMs*. Faites un clic √† l'aide du bouton de droite de la souris sur votre nouveau vDS et s√©lectionnez `Nouveau groupe de ports distribu√©s`:

- **Nom:** DPG-Management
- **Liaison de port:** Statique
- **Nombre de ports:** 8
- **Pool de ressources:** Aucun
- **VLAN:** Aucun
- **S√©curit√©:**
    - **Mode promiscuit√©:** Accepter
    - **Modifications de l'adresse MAC:** Accepter
    - **Transmissions forg√©es:** Accepter
- **Formation du traffic:** Par d√©faut
- **Association et basculement:** Par d√©faut
- **Surveillance:** Par d√©faut
- **Divers:** Par d√©faut

**<span class="red-text"><u>R√©p√©tez les m√™mes √©tapes pour vous cr√©er un groupe de ports nomm√© DPG-VM-Network sur le m√™me vDS</u></span>**

#### 9.21 Cr√©ation des groupes de ports pour vDS Services

Pour les groupes de ports du **vDS Services**, proc√©dez de la m√™me fa√ßon en prenant soin de cr√©er un groupe de ports distribu√© pour vMotion et un autre pour vSAN. <mark>**Ne mettez surtout pas de VLANs en place**</mark>, vos commutateurs priv√©s sur LabInfo ne les supportent pas.

Assurez-vous de cr√©er les groupes de ports sur les bons commutateurs virtuels.

#### 9.3 Ajouter les h√¥tes aux vDS

Nous devons maintenant permettre aux hyperviseurs ESXi d'utiliser les vDS que nous venons de cr√©er. Pour cela, il faut ajouter des h√¥tes aux commutateurs virtuels. **Pour chacun des vDS, faites les √©tapes suivantes:**

- Clic √† l'aide du bouton de droite de la souris sur le vDS, puis `Ajoutez et g√©rez des h√¥tes`
- **S√©lectionner la t√¢che:** Ajouter des h√¥tes
- **S√©lectionner des h√¥tes:** Cocher tous les hyperviseurs
- **G√©rer les adapteurs physiques:** <span class="red-text">Ne rien cocher</span>
- **G√©rer les adapteurs VMKernel:** <span class="red-text">Ne rien cocher</span>
- **Migrer la mise en r√©seau des VMs:** <span class="red-text">Ne rien cocher</span>

#### 9.4 Migration de vmnic0 vers vDS ‚ö†Ô∏è

:::danger[Risque de perdre la connexion!]
Cette √©tape est √† la fois d√©licate et cruciale. Assurez-vous d'avoir du temps et de la concentration avant de proc√©der. Nous nous appr√™tons √† faire passer le r√©seau de gestion des hyperviseurs d'un **vSwitch Standard** √† un **vSwitch Distribu√©.** Vous perdrez donc la connexion avec vos hyperviseurs pendant un court instant. Une erreur de configuration √† cette √©tape vous obligera √† revenir en arri√®re. Respirez par le nez, dites *namaste* et allez-y calmement üòä.
:::

**<span class="fonttaller">Pour les noeuds ne contenant <span class="red-text">PAS</span> vCenter:</span>**

Faites un clic √† l'aide du bouton de droite de la souris sur `vDS Production` et s√©lectionnez `Ajouter ou g√©rer des h√¥tes`:

- **S√©lectionnez la t√¢che:** G√©rez la mise en r√©seau de l'h√¥te
- **S√©lectionnez des h√¥tes:** Choisissez un seul noeud pour l'instant
- **G√©rer les adapteurs physiques:** Attribuez le vmnic0 √† l'uplink 1
- **G√©rer les adapteurs VMKernel:** Attribuez vmk0 √† votre groupe de ports `DPG-Management`
- **Migrer la mise en r√©seau VM:** Pas de modification ici
- **Cliquez sur `terminer`.**

<u>Refaites les m√™mes √©tapes pour l'autre noeud ne contenant PAS vCenter</u>

<br/><br/>
**<span class="fonttaller">Pour le noeud contenant vCenter:</span>**

Observez le sch√©ma ci-dessous. Remarquez comment, actuellement, notre machine **vCenter** d√©pend compl√®tement de notre *vSwitch0*. La probl√©matique que nous rencontreront est directement li√© √† cette d√©pendance. L'objectif est de remplacer notre *vSwitch0* (un vSS) par notre vDS-Prodction (un VDS). Or, **vCenter** poss√®de des protections contre les coupures de connexion. Si nous tentons de nous y prendre comme nous l'avons fait avec les deux autres noeuds, **vCenter** nous bloquera.

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ReplacevSwitch0_W.svg'),
            dark: useBaseUrl('/img/Virtu/ReplacevSwitch0_D.svg'),
        }}
    />
</div>
<br/>
**<span class="fonttaller">Plan de match</span>**

1. Relier la vmnic2 au r√©seau de votre passerelle (lien rose)
2. Relier la vmnic2 au vDS
3. Assigner vmk0 au vDS
4. Migrer le r√©seau de **vCenter**
5. Relier la vmnic0 au vDS
6. Retirer la vmnic2 du vDS

<div style={{textAlign: 'center'}}>
    <ThemedImage
        alt="Sch√©ma"
        sources={{
            light: useBaseUrl('/img/Virtu/ReplacevSwitch0_W.gif'),
            dark: useBaseUrl('/img/Virtu/ReplacevSwitch0_D.gif'),
        }}
    />
</div>

:::caution[Gabriel o√π est l'√©tape 6 dans le sch√©ma ?]
Elle n'apparait pas. Elle consiste simplement √† retirer les liens 1 & 2.
:::

**√âtape par √©tape:**

1. Sur l'ESXi h√¥te, reli√© l'adapteur vmnic2 (ou celui disponible) au commutateur d√©sservi par PfSense dans LabInfo.
2. Reliez votre vDS Production √† cette vmnic. Pour ce faire suivez les sous-√©tapes:
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 1: vmnic2 (ou celle qui vous concerne)
    - G√©rez les adapteurs VMKernel:
        - Assignez `DPG-Management` √† vmk0
    - Migrer la mise en r√©seau des VMs:
        - Assignez l'adapteur r√©seau de vCenter dans `DPG-Management`
    - Terminer
3. V√©rifiez que **vCenter** est toujours accessible et que vos changements ont bien √©t√© pris en compte.
4. Ajoutez vmnic0 au vDS (retirer de vSwitch0)
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 2: vmnic0
    - G√©rez les adapteurs VMKernel: rien √† faire
    - Migrer la mise en r√©seau des VMs: rien √† faire
    - Terminer
5. Retirez vmnic2 du vDS (utilis√© temporairement)
    - Dans **vCenter**, clic droit sur vDS Production ‚Üí Ajoutez et g√©rez des h√¥tes.
    - G√©rez la mise en r√©seau de l'h√¥te
        - S√©lectionnez l'h√¥te avec **vCenter** seulement.
    - G√©rez les adapteurs physiques:
        - Uplink 1: vide
        - Uplink 2: vmnic0
    - G√©rez les adapteurs VMKernel: rien √† faire
    - Migrer la mise en r√©seau des VMs: rien √† faire
    - Terminer

#### 9.5 Suppression des VSS0

√Ä ce stade, tous vos noeuds sont d√©sormais reli√©s au vSwitch Distrivu√© vDS Production que nous avons cr√©√© plus t√¥t. Nous n'avons plus besoin des vSwitch Standards 0 qui sont au sein des diff√©rents hyperviseurs.

Passez donc dans chacun des hyperviseurs et supprimez ces commutateurs virtuels qui sont devenus inutiles.

### 10. Cr√©ation du Cluster

Dans **vCenter**, cr√©ez le *vSphere Cluster* en effectuant un clic droit sur votre *datacenter* et en s√©lectionnant `Nouveau Cluster`:

|Param√®tre|Valeur|Notes|
|:---------:|:------:|:-----:|
|**Nom**|Cluster-*vosinitiales* EX: Cluster-GG|Nom du cluster|
|**vSphere DRS**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**vSphere HA**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**vSAN**| ‚ùå D√©sactiv√© | On activera manuellement plus tard|
|**Image**| Importer une image √† partir d'un h√¥te existant | Tous les noeuds devront correspondre √† cette image syst√®me.|

#### 10.1 Ajouter les noeuds au sein du cluster

Une fois le *cluster* cr√©√©, vous le verrez appara√Ætre dans votre inventaire. Clic droit ‚Üí `Ajouter des h√¥tes` et s√©lectionnez tous vos noeuds ESXi. **Inutile d'importer √† nouveau une image pour vLCM (*Life Cycle Manager*), nous l'avons d√©j√† fait lors de la cr√©ation du *cluster***.

:::danger[Gabriel! Mes hyperviseurs tombent en maintenance üò±]
C'est normal! C'est le *Cluster Quickstart* qui d√©marre automatiquement. En gros, c'est une aide √† l'administrateur...que nous n'utiliseront pas ü•≤
:::

Pour d√©sactiver le *Cluster Quickstart*, s√©lectionnez votre *cluster* dans l'inventaire puis dans la section `Configurer` ‚Üí `D√©marrage Rapide`, cliquez sur `Ignorer le d√©marrage rapide`.

Au besoin, faites un clic droit sur vos hyperviseurs et sortez les du mode maintenance. üöß

### 11. Configurer vMotion

Dans les √©tapes pr√©c√©dentes, nous avons pr√©par√© un vDS pour nos services vMotion & vSAN. Cependant, nous avons pas encore configur√© les interfaces **Vmkernel Ports** pour ces services. Il faut donc s'affairer √† les cr√©er maintenant. D'abord, s√©lectionnez un sous-r√©seau pour **vMotion**. Si vous d√©sirez suivre mon propre plan d'adressage, vous pouvez √©galement: 

|H√¥te|IP vMotion|Subnet|VLAN|
|:----:|:----------:|:------:|:----:|
|ESXi-01|10.20.0.11|255.255.255.0|20|
|ESXi-02|10.20.0.12|255.255.255.0|20|
|ESXi-03|10.20.0.13|255.255.255.0|20|

#### 11.1 Configurer les VMKernel pour chaque noeud

Pour chaque noeud ESXi dans l'inventaire:

- Menu `Configurer` ‚Üí `Adapteurs VMKernel`
- Ajouter une mise en r√©seau:
    - **S√©lectionner un type de connexion:** Adapteur r√©seau VMKernel
    - **S√©lectionner un p√©riph√©rique cible:** S√©lectionner un r√©seau existant ‚Üí `DPG-Vmotion`
    - **Propri√©t√©s du port:** Par d√©faut ‚Üí Services: vMotion
    - **Param√®tres IPv4:** Statique
        - Adresse: *La v√¥tre*
        - Masque: /24
        - Passerelle et DNS: Ne rien changer, pas utile pour nous

**R√©p√©ter les m√™mes √©tapes pour les tous les noeuds**

#### 11.2 Raccorder le vDS Services

Notre vDS n'a aucune liaison montante pour le moment. Autrement dit, aucune interface physique ne d√©ssert ce commutateur virtuel. Nous allons donc associer notre vmnic2, dont nous nous sommes servi temporairement un peu plus t√¥t. 

Dans **vCenter**, clic droit sur `vDS Services` ‚Üí `Ajouter et g√©rer des h√¥tes`:

- **S√©lectionner la t√¢che:** G√©rer la mise en r√©seau de l'h√¥te
- **S√©lectionner des h√¥tes:** S√©lectionner tout
- **G√©rer les adapteurs physiques:** vmnic2 : uplink1
- **G√©rer les adapteurs VMkernel:** vmk2 : `DPG-vMotion`
- **Migrer la mise en r√©seau VM:** Rien √† faire
- **Terminer**

#### 11.3 Tests avec vMotion

Bon, √ßa y est. **vMotion** est en place. Il ne nous reste qu'√† effectuer quelques tests pour s'assurer que √ßa fonctionne bien. Cr√©ez-vous une machine virtuelle toute simple sur votre *cluster*. Id√©alement, √©vitez de cr√©er cette machine virtuelle sur le m√™me noeud que **vCenter**.

:::tip[DRS]
Si le **DRS** √©tait d√©j√† activ√© sur notre *cluster*, nous n'aurions pas besoin de s√©lectionner le noeud. Le module ferait une analyse de charge de travail sur tous les noeuds et d√©terminerait, pour nous, le meilleur emplacement pour notre *vm*. On y reviendra.
:::

Si vous d√©sirez suivre mon propre test, sachez que j'utiliserai une *VM Ubuntu Server* sur ESX1.

:::caution[Aucun DHCP]
Il n'y a aucun serveur DHCP sur notre r√©seau... Ne l'oubliez pas lors de la cr√©ation de votre *vm*
:::

Le vrai test √† effectuer avec **vMotion**, ce n'est pas seulement de constater que la machine peut √™tre transf√©r√©e sans √™tre arr√™t√©e, mais c'est de constater qu'il n'y a aucune interruption.

Pour visualiser ce concept, je vais d√©marrer une commande `ping` vers Google dans ma *vm*. Sous Linux, le `ping` est perp√©tuel, je devrai donc l'arr√™ter moi-m√™me. √Ä cela, je vais ajouter un intervalle de 2 secondes pour ralentir l√©g√®rement l'envoie. Puis, du c√¥t√© de **vCenter**, je vais d√©clencher le transfert avec **vMotion** et je pourrai observer qu'il n'y aucun impact sur l'envoie des pings vers Google, malgr√© ce transfert.

La commande √† utiliser:
```bash
ping -i 2 google.com
```

:::tip[Duplicata de Ping]
Il se peut que vous receviez des r√©ponses en duplicata lors de vos tests d'envoie ICMP. Votre *vm* se trouve derri√®re un environnement r√©seau anormalement lourd. Vous pouvez tout simplement ignorer ces messages `(DUP!)`.
:::

### 12. Configurer la haute-disponibilit√© l'√©quilibrage de charge

La haute-disponibilit√© permet de red√©marrer automatiquement les machines virtuelles sur un autre noeud lorsqu'un incident se produit. Cela dit, **attention:** ce n'est pas du vMotion. Quant √† l'√©quilibrage de charge (DRS), son travail est de surveiller continuellement l'utilisation de CPU/RAM de chaque noeud et de d√©placer automatiquement les *VMs* via **vMotion** au besoin. Le tout dans l'objectif d'optimiser l'utilisation des ressources.

**DRS** poss√®de diff√©rents modes de fonctionnement ainsi que des seuils de migrations:

<span class="green-text">**Modes**:</span>

- **Manuel:** DRS recommande des actions, vous d√©cidez
- **Partiellement automatique:** DRS planifie des actions, mais vous demande une confirmation
- **Automatique:** DRS prend les d√©cisions et migre automatiquement

<span class="green-text">**Seuils de migration**:</span>

- **Conservateur:** Migre les *VMs* seulement lorsqu'il y a un d√©s√©quilibre important
- **Mod√©r√©:** Migre les *VMs* lorsqu'il y a un d√©s√©quilibre standard
- **Agressif:** Migre les machines d√®s le moindre d√©s√©quilibre 